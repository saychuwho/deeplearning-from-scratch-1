{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7장 합성곱 신경망(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 합성곱 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1 완전연결 계층의 문제점\n",
    "\n",
    "완전연결 계층(Affine 계층)의 문제점은 데이터의 형상이 무시된다는 점이다. \n",
    "\n",
    "CNN의 Convolutional layer는 형상을 유지하기 때문에, 형상을 가진 데이터를 제대로 이해할 가능성이 있다.\n",
    "\n",
    "CNN의 convolutional layer의 입출력은 feature map이라고도 한다. 입력은 input feature map, 출력은 output feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2 합성곱 연산\n",
    "\n",
    "필터는 때때로는 커널로 불린다. \n",
    "\n",
    "합성곱 연산은 다음과 같은 흐름을 가진다. \n",
    "\n",
    "<img src=\"./img/fig7-4.jpg\" width=\"40%\">\n",
    "\n",
    "결과에 편향을 더하면 출력 데이터가 나오는 흐름이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.3 패딩\n",
    "\n",
    "합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정값으로 채우는 행위를 패딩이라고 한다. 패딩은 1, 2, 3 등 다양한 값으로 적용할 수 있다.\n",
    "\n",
    "패딩은 보통 출력 데이터의 크기를 조정하기 위해 적용한다.\n",
    "\n",
    "<img src=\"./img/fig7_6.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.4 스트라이드\n",
    "\n",
    "필터(커널)을 적용하는 위치의 간격을 스트라이드라고 한다. 위 합성곱 연산은 스트라이드가 1인 예시를 다루었지만, 다음 그림은 스트라이드가 2인 경우를 나타낸다. \n",
    "\n",
    "<img src=\"./img/fig7_7.jpg\" width=\"40%\">\n",
    "\n",
    "패딩, 스트라이드에 따른 출력 크기는 다음과 같이 계산할 수 있다. \n",
    "\n",
    "입력 크기를 $(H,W)$, 필터 크기를 $(FH,FW)$, 출력 크기를 $(OH, OW)$, 패딩을 $P$, 스트라이드를 $S$라고 하면, 출력 크기는 다음과 같이 계산할 수 있다.\n",
    "\n",
    "$OH={{H+2P-FH}\\over{S}}+1$\n",
    "\n",
    "$OW={{W+2P-FW}\\over{S}}+1$\n",
    "\n",
    "다만, 출력 크기는 모두 정수로 나누어떨어져야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.5 3차원 데이터의 합성곱 연산\n",
    "\n",
    "3차원 데이터의 합성곱 연산은 각 채널 별로 합성곱 연산을 수행한 결과를 다 더해서 출력 데이터를 만들어낸다. \n",
    "\n",
    "<img src=\"./img/fig7_9.jpg\" width=\"40%\">\n",
    "\n",
    "3차원 합성곱 연산에서 주의할 점은 입력 데이터의 채널 수와 필터의 채널 수가 같아야 한다는 점이다. 또한, 모든 채널의 필터 크기가 동일해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.6 블록으로 생각하기\n",
    "\n",
    "채널 수 $C$, 높이 $H$, 너비 $W$인 데이터의 형상은 $(C, H, W)$로 쓴다.\n",
    "\n",
    "아래 사진에서 출력 데이터는 한 장의 feature map이다. \n",
    "\n",
    "<img src=\"./img/fig7_10.jpg\" width=\"40%\">\n",
    "\n",
    "여러 장의 feature map은 여러개의 필터(커널)을 이용하면 만들 수 있다. \n",
    "\n",
    "<img src=\"./img/fig7_11.jpg\" width=\"40%\">\n",
    "\n",
    "위 흐름으로 만든 feature map을 다음 계층으로 넘기는 것이 CNN의 처리 흐름이다.\n",
    "\n",
    "합성곱 연산에서 편향이 사용되니, 편향은 다음과 같이 적용하면 된다. 편향은 채널 하나 당 값 하나씩으로 구성된다. \n",
    "\n",
    "<img src=\"./img/fig7_12.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.7 배치 처리\n",
    "\n",
    "합성곱 연산에서도 배치 처리를 할 수 있다. 배치 처리의 흐름은 다음과 같이 만들 수 있다. 각 데이터의 선두에는 배치용 차원이 추가되어 있다.\n",
    "\n",
    "<img src=\"./img/fig7_13.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 풀링 계층\n",
    "\n",
    "풀링은 세로, 가로 방향의 공간을 줄이는 연산이다. 아래 사진은 maxpooling을 스트라이드 2로 처리하는 과정이다. 이미지 인식 분야에서는 주로 maxpooling을 사용한다.\n",
    "\n",
    "<img src=\"./img/fig7_14.jpg\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.1 풀링 계층의 특징\n",
    "\n",
    "풀링 계층은 다음과 같은 특징을 가지고 있다. \n",
    "\n",
    "1. 학습해야 할 매개변수가 없다\n",
    "2. 채널 수가 변하지 않는다\n",
    "3. 입력의 변화에 영향을 적게 받는다(강건하다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 합성곱/풀링 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2col 함수를 분석해보자\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    \"\"\" \n",
    "    np.pad : padding을 넣는다.\n",
    "        input_data : padding을 넣을 데이터 \n",
    "        중간에 list : 각 차원별로 어떤 크기의 padding을 넣을 것인가. 앞에 두개는 batch랑 channel이니 넣지 말고, 뒷 부분에 pad만큼의 padding을 넣는다.\n",
    "        'constant' : padding 되는 값이 상수라는 것을 의미. \n",
    "    \"\"\"\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            \n",
    "            \"\"\" \n",
    "            img[:,:,y:y_max:stride, x:x_max:stride]의 의미. \n",
    "                filter(kernel)의 (y,x)에 해당하는 원소와 곱해질 input_data의 원소를 찾는 과정이다. \n",
    "                kernel과 input_data는 행렬곱이 아니라 행렬의 원소별 곱셈이 된다. \n",
    "            \"\"\"\n",
    "\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    \"\"\" \n",
    "    col.transpose(0,4,5,1,2,3)\n",
    "        원래 col이 (0,1,2,3,4,5)의 차원을 가지고 있다면, \n",
    "        첫번째 위치는 그대로 두고, \n",
    "        두번째 위치에 원래 4번째를 놔두고\n",
    "        세번째 위치에 원래 5번째를 놔두고... 반복한다.\n",
    "        이 경우에는, 원래 col은 (batch_size=N, channel_size=C, filter_h, filter_w, out_h, out_w)를 가지고 있었지만, \n",
    "        col은 (batch_size=N, out_h, out_w, channel_size=C, filter_h, filter_w)로 차원이 달라지게 된다.  \n",
    "        이거를 하는 이유는 아마 뒤에 필요없는 차원을 보내버리고, 앞에 실제로 데이터가 있는 차원을 앞으로 보내기 위함인거 같다.\n",
    "    \"\"\"\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    \"\"\"  \n",
    "    여기서 H + 2*pad + stride - 1을 해주는 이유는, padding의 영향을 받은 image를 표현하기 위해서이다. \n",
    "    \"\"\"\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers.py에 정의되어 있는 Convolution\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    \"\"\" \n",
    "    col2im을 사용해야 한다는 점 이외에는 Affine 계층의 backward와 동일하다. \n",
    "    당연하지 않을까. 중간에 행렬 곱으로 가중치 곱하는 걸 구해놨으니, 그걸 역연산 한다 생각하면 dW, db는 똑같지 않을까.\n",
    "    업데이트 할 때는 SGD 이용해서 업데이트 하면 될 거 같고.\n",
    "     \n",
    "    \"\"\"\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 풀링 계층 구현하기\n",
    "\n",
    "# 풀링을 적용할 때도 im2col을 이용하지만, 각 채널별로 적용해준다. \n",
    "# 각 행별로 최댓값을 구하고 적절한 형상으로 성형하기만 하면 된다.\n",
    "\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "\n",
    "        \"\"\"  \n",
    "        이 부분 전개에 대한 이해가 더 필요할 거 같다.\n",
    "        여기서 im2col로 형태를 바꾸면, (N * out_h * out_w, -1)로 나오는데, \n",
    "        여기서 out_h = (H + 2*pad - filter_h) // stride + 1, out_w = (W + 2*pad - filter_w)//stride + 1 가 된다.\n",
    "        filter_h는 pool_h, filter_w는 pool_w이다. \n",
    "        뒤의 코드를 보니, 스트라이드 값을 pool_h, pool_w랑 동일하게 준다. 그러면 maxpooling을 적용해야 하는 부분들이 가로로 모여서 반환된다.\n",
    "        반환된 데이터를 reshape해서 세로로 세우면, 각 row 별 최댓값을 구하면 된다.\n",
    "        \"\"\"\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        \"\"\"\n",
    "        원래 이미지로 바꾸는 과정이다.  \n",
    "        \"\"\"\n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 CNN 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from functions import *\n",
    "from layers import Relu, Affine, SoftmaxWithLoss\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1,28,28),\n",
    "                 conv_param={'filter_num':30, 'filter_size':5,\n",
    "                             'pad':0,'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num*(conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b3'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affilne1'].dW\n",
    "        grads['b2'] = self.layers['Affilne1'].db\n",
    "        grads['W3'] = self.layers['Affilne2'].dW\n",
    "        grads['b3'] = self.layers['Affilne2'].db\n",
    "\n",
    "        return grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2997709838145632\n",
      "=== epoch:1, train acc:0.123, test acc:0.114 ===\n",
      "train loss:2.2960917726798247\n",
      "train loss:2.2933912333249786\n",
      "train loss:2.288697483900441\n",
      "train loss:2.282109817171497\n",
      "train loss:2.2634216464358317\n",
      "train loss:2.2581457840739296\n",
      "train loss:2.2372354888860446\n",
      "train loss:2.2218764938477302\n",
      "train loss:2.190099159256585\n",
      "train loss:2.1669337680027096\n",
      "train loss:2.1365678065975047\n",
      "train loss:2.079087819965746\n",
      "train loss:2.03860964413956\n",
      "train loss:2.0042751055031096\n",
      "train loss:1.8885632630851148\n",
      "train loss:1.8669072382250262\n",
      "train loss:1.7747194320993964\n",
      "train loss:1.7703931335631526\n",
      "train loss:1.7198046301569048\n",
      "train loss:1.5398469182623784\n",
      "train loss:1.5493236193880338\n",
      "train loss:1.3312754905290207\n",
      "train loss:1.3701917589757153\n",
      "train loss:1.275938402897511\n",
      "train loss:1.1885115848562773\n",
      "train loss:1.0759208213617042\n",
      "train loss:1.018632602324246\n",
      "train loss:0.9080305831965912\n",
      "train loss:1.0056067412339933\n",
      "train loss:0.8597075725068517\n",
      "train loss:0.8927922706557222\n",
      "train loss:0.8737324564316125\n",
      "train loss:0.9118312616000808\n",
      "train loss:0.8072400362318131\n",
      "train loss:0.7722033308189005\n",
      "train loss:0.8464859034947536\n",
      "train loss:0.7547334960068713\n",
      "train loss:0.8389698101039833\n",
      "train loss:0.696986936245866\n",
      "train loss:0.8342733706594394\n",
      "train loss:0.5706902791928413\n",
      "train loss:0.8034390701765887\n",
      "train loss:0.5520442820169306\n",
      "train loss:0.6001810947504255\n",
      "train loss:0.47998806292025464\n",
      "train loss:0.560074237443908\n",
      "train loss:0.4804071395413686\n",
      "train loss:0.6268836305725521\n",
      "train loss:0.6974319225579945\n",
      "train loss:0.6163325642259649\n",
      "train loss:0.45771559764198616\n",
      "train loss:0.4688783122206895\n",
      "train loss:0.7511296920254005\n",
      "train loss:0.5197263164739997\n",
      "train loss:0.7011384767925903\n",
      "train loss:0.4812268427404354\n",
      "train loss:0.6287976063379004\n",
      "train loss:0.5506097541792169\n",
      "train loss:0.5481634724465496\n",
      "train loss:0.3432950330012035\n",
      "train loss:0.5680623168210164\n",
      "train loss:0.5268948650326296\n",
      "train loss:0.4675773221750039\n",
      "train loss:0.499960349610648\n",
      "train loss:0.6240259286716764\n",
      "train loss:0.4799975936691335\n",
      "train loss:0.505896730688071\n",
      "train loss:0.4833273761061705\n",
      "train loss:0.34891346324164024\n",
      "train loss:0.4638909194933418\n",
      "train loss:0.46395315089789063\n",
      "train loss:0.436691026233106\n",
      "train loss:0.5278946268504857\n",
      "train loss:0.6715779309921182\n",
      "train loss:0.7319498658915325\n",
      "train loss:0.34494886738280683\n",
      "train loss:0.4151871815986183\n",
      "train loss:0.5609218523000196\n",
      "train loss:0.4557582500994746\n",
      "train loss:0.6502837347530704\n",
      "train loss:0.5369520429171546\n",
      "train loss:0.3499502448454159\n",
      "train loss:0.41628236085838854\n",
      "train loss:0.64216306767188\n",
      "train loss:0.4714690592994706\n",
      "train loss:0.4522787048399481\n",
      "train loss:0.383235732354503\n",
      "train loss:0.279330925623454\n",
      "train loss:0.3338377466328009\n",
      "train loss:0.47097323238334704\n",
      "train loss:0.5429552921531188\n",
      "train loss:0.3480786244447323\n",
      "train loss:0.3723722973850549\n",
      "train loss:0.398708578770726\n",
      "train loss:0.5706160265846222\n",
      "train loss:0.41518182069786386\n",
      "train loss:0.42364842223517996\n",
      "train loss:0.41347537845424065\n",
      "train loss:0.5007913082333828\n",
      "train loss:0.3257327895637993\n",
      "train loss:0.4661424051513443\n",
      "train loss:0.2567369462388832\n",
      "train loss:0.38099630607985213\n",
      "train loss:0.4916976589508163\n",
      "train loss:0.6206957748184745\n",
      "train loss:0.39428823478314734\n",
      "train loss:0.4573675956380046\n",
      "train loss:0.39813442563003976\n",
      "train loss:0.2686209160048824\n",
      "train loss:0.41963904737350094\n",
      "train loss:0.3317569417235638\n",
      "train loss:0.41047592187703963\n",
      "train loss:0.34564810117174327\n",
      "train loss:0.4068621124332523\n",
      "train loss:0.2974599156444131\n",
      "train loss:0.443910093853294\n",
      "train loss:0.2935543375812405\n",
      "train loss:0.3845498841302967\n",
      "train loss:0.3794109196299116\n",
      "train loss:0.41072863009609323\n",
      "train loss:0.21748648352780303\n",
      "train loss:0.3290319635681478\n",
      "train loss:0.4372906192833483\n",
      "train loss:0.6261705240675026\n",
      "train loss:0.41465594193893224\n",
      "train loss:0.4597051161793425\n",
      "train loss:0.3068463107332363\n",
      "train loss:0.38455480880705134\n",
      "train loss:0.3877848895820861\n",
      "train loss:0.44322535552061554\n",
      "train loss:0.38911615708884667\n",
      "train loss:0.28256102020588064\n",
      "train loss:0.36035116953592367\n",
      "train loss:0.32904901949901677\n",
      "train loss:0.22867958006340072\n",
      "train loss:0.3625277692345651\n",
      "train loss:0.23822123197572023\n",
      "train loss:0.4026431475281057\n",
      "train loss:0.39067246060418065\n",
      "train loss:0.4589822327038078\n",
      "train loss:0.28152790276356265\n",
      "train loss:0.4985039372011621\n",
      "train loss:0.49376843514598173\n",
      "train loss:0.34711539996783053\n",
      "train loss:0.29547898967375824\n",
      "train loss:0.35342559446260075\n",
      "train loss:0.45264003463149116\n",
      "train loss:0.2959997968979737\n",
      "train loss:0.2745205691744808\n",
      "train loss:0.4610071333382042\n",
      "train loss:0.3130019980630409\n",
      "train loss:0.46082098900650487\n",
      "train loss:0.4250705773172819\n",
      "train loss:0.25163000735805297\n",
      "train loss:0.5386400631657796\n",
      "train loss:0.40781365821994675\n",
      "train loss:0.2548590242569524\n",
      "train loss:0.3677007722417389\n",
      "train loss:0.31967010981860106\n",
      "train loss:0.5686101078956168\n",
      "train loss:0.289119377022945\n",
      "train loss:0.4525322803562996\n",
      "train loss:0.22953665926063388\n",
      "train loss:0.33485473894795986\n",
      "train loss:0.35906458362345\n",
      "train loss:0.27169651674798945\n",
      "train loss:0.334478523586785\n",
      "train loss:0.42731498547353225\n",
      "train loss:0.2754314392799983\n",
      "train loss:0.3590185054057188\n",
      "train loss:0.3676159276720491\n",
      "train loss:0.2811265020060159\n",
      "train loss:0.4392970205262731\n",
      "train loss:0.29014774298329277\n",
      "train loss:0.26654955956281134\n",
      "train loss:0.5378431071482855\n",
      "train loss:0.24961988440194727\n",
      "train loss:0.3692374602217196\n",
      "train loss:0.34735173256709934\n",
      "train loss:0.3493420809381471\n",
      "train loss:0.2624897742769597\n",
      "train loss:0.4266988935008441\n",
      "train loss:0.30857193120776655\n",
      "train loss:0.30629525581219563\n",
      "train loss:0.3218041507589142\n",
      "train loss:0.23070847329999253\n",
      "train loss:0.28194697071264135\n",
      "train loss:0.3115351338176373\n",
      "train loss:0.3995501114110263\n",
      "train loss:0.20234756749476493\n",
      "train loss:0.49621910543017633\n",
      "train loss:0.4880364611700278\n",
      "train loss:0.18293841024355864\n",
      "train loss:0.24952407860052808\n",
      "train loss:0.3106113658313291\n",
      "train loss:0.25586346368062995\n",
      "train loss:0.2670622610637234\n",
      "train loss:0.4528990925443934\n",
      "train loss:0.3213461955773671\n",
      "train loss:0.29230481804512265\n",
      "train loss:0.296841930913049\n",
      "train loss:0.2920234678332206\n",
      "train loss:0.4060425523703226\n",
      "train loss:0.4205043985132223\n",
      "train loss:0.2698075188114321\n",
      "train loss:0.1504473120952782\n",
      "train loss:0.374403891229085\n",
      "train loss:0.36744363727986057\n",
      "train loss:0.23687954778856873\n",
      "train loss:0.3194201578653819\n",
      "train loss:0.36774966738343495\n",
      "train loss:0.29642687221027647\n",
      "train loss:0.20966027083845476\n",
      "train loss:0.3725819985704261\n",
      "train loss:0.30006124822573255\n",
      "train loss:0.2922233520783488\n",
      "train loss:0.19657756769941653\n",
      "train loss:0.27988891510309627\n",
      "train loss:0.2276635371572092\n",
      "train loss:0.3213762376709418\n",
      "train loss:0.4143600154890315\n",
      "train loss:0.26159266363709555\n",
      "train loss:0.3029612247648959\n",
      "train loss:0.38617468106206365\n",
      "train loss:0.2601936509192021\n",
      "train loss:0.33949559954330005\n",
      "train loss:0.2707189481192759\n",
      "train loss:0.2434113799097432\n",
      "train loss:0.2643287928510277\n",
      "train loss:0.3467306253270916\n",
      "train loss:0.20203538866549223\n",
      "train loss:0.3345617655102038\n",
      "train loss:0.3226010357721298\n",
      "train loss:0.2733258463705113\n",
      "train loss:0.336869327553575\n",
      "train loss:0.39942262174061754\n",
      "train loss:0.27448451934367424\n",
      "train loss:0.2602591019903659\n",
      "train loss:0.2768385698990727\n",
      "train loss:0.23447289371659438\n",
      "train loss:0.28626653553551795\n",
      "train loss:0.23173526590218063\n",
      "train loss:0.3724018234567568\n",
      "train loss:0.33692214155152195\n",
      "train loss:0.3022030641776002\n",
      "train loss:0.40262603060708907\n",
      "train loss:0.3340202581952664\n",
      "train loss:0.30326211949407955\n",
      "train loss:0.21346167949585787\n",
      "train loss:0.29032341425212993\n",
      "train loss:0.30496896979942645\n",
      "train loss:0.307086312278611\n",
      "train loss:0.3864639975904264\n",
      "train loss:0.22322822618889618\n",
      "train loss:0.32596754158593766\n",
      "train loss:0.24599167687678844\n",
      "train loss:0.2885630724880096\n",
      "train loss:0.21589612625217114\n",
      "train loss:0.21277494680619125\n",
      "train loss:0.4921581936342001\n",
      "train loss:0.275610878061741\n",
      "train loss:0.32004460330011847\n",
      "train loss:0.2571576427380359\n",
      "train loss:0.3177508211684534\n",
      "train loss:0.3496596542025152\n",
      "train loss:0.2407962389194093\n",
      "train loss:0.36918136731401374\n",
      "train loss:0.30534708022775187\n",
      "train loss:0.34767947852012343\n",
      "train loss:0.3836312744399838\n",
      "train loss:0.255853221253514\n",
      "train loss:0.4077986467052442\n",
      "train loss:0.3265906766838508\n",
      "train loss:0.2953322623079927\n",
      "train loss:0.3061095105168839\n",
      "train loss:0.3157353858237568\n",
      "train loss:0.2948977227114118\n",
      "train loss:0.3856261402971789\n",
      "train loss:0.1957926498243136\n",
      "train loss:0.30499084561620277\n",
      "train loss:0.21135425148070286\n",
      "train loss:0.6179267971319662\n",
      "train loss:0.2517266593246906\n",
      "train loss:0.26741795619176556\n",
      "train loss:0.2339773923189984\n",
      "train loss:0.12184988044455888\n",
      "train loss:0.23308681937108675\n",
      "train loss:0.316194993614827\n",
      "train loss:0.3088247344417599\n",
      "train loss:0.25144687397877147\n",
      "train loss:0.2114208357187048\n",
      "train loss:0.3613238514710648\n",
      "train loss:0.2949710862018966\n",
      "train loss:0.16020616306535157\n",
      "train loss:0.27786134001330365\n",
      "train loss:0.2899715825992575\n",
      "train loss:0.19215920017374524\n",
      "train loss:0.19237127636151707\n",
      "train loss:0.3187669696684033\n",
      "train loss:0.19978045809350878\n",
      "train loss:0.2512400297707556\n",
      "train loss:0.20233831234113506\n",
      "train loss:0.2619481147227958\n",
      "train loss:0.21661486700888652\n",
      "train loss:0.33468177631102436\n",
      "train loss:0.22226270205865906\n",
      "train loss:0.1723068279566954\n",
      "train loss:0.3986400387059451\n",
      "train loss:0.2113568281194947\n",
      "train loss:0.28385414745646725\n",
      "train loss:0.3494057277579973\n",
      "train loss:0.3387080389666569\n",
      "train loss:0.16873288869261419\n",
      "train loss:0.24683576030101673\n",
      "train loss:0.35583456517316925\n",
      "train loss:0.33852693440534054\n",
      "train loss:0.16937871881842995\n",
      "train loss:0.30455995140432446\n",
      "train loss:0.42355727758273504\n",
      "train loss:0.306159504333941\n",
      "train loss:0.36852070895692257\n",
      "train loss:0.2978539735371015\n",
      "train loss:0.24349119570874375\n",
      "train loss:0.26856637424326835\n",
      "train loss:0.262587228106367\n",
      "train loss:0.3341438424118071\n",
      "train loss:0.13746232373288808\n",
      "train loss:0.2910658300317972\n",
      "train loss:0.17541922981828983\n",
      "train loss:0.24765866517030546\n",
      "train loss:0.2726840434670269\n",
      "train loss:0.18098609807568763\n",
      "train loss:0.23511394260334942\n",
      "train loss:0.1749999238185458\n",
      "train loss:0.2712882964282519\n",
      "train loss:0.3610476604303608\n",
      "train loss:0.1623118799749583\n",
      "train loss:0.17984002392869536\n",
      "train loss:0.30356842963006725\n",
      "train loss:0.17705775569900034\n",
      "train loss:0.1540095640053291\n",
      "train loss:0.4625638636287361\n",
      "train loss:0.42133352963338583\n",
      "train loss:0.1283361417642445\n",
      "train loss:0.4348393356864073\n",
      "train loss:0.24356077840839074\n",
      "train loss:0.2481647520931437\n",
      "train loss:0.2158843645442827\n",
      "train loss:0.2447622396600193\n",
      "train loss:0.24964837672520054\n",
      "train loss:0.22882992393552626\n",
      "train loss:0.1289160792650215\n",
      "train loss:0.21439370264144952\n",
      "train loss:0.2654192315153139\n",
      "train loss:0.35852996294921885\n",
      "train loss:0.30546832675398716\n",
      "train loss:0.16092253957900382\n",
      "train loss:0.41482924311323016\n",
      "train loss:0.2663872366083083\n",
      "train loss:0.28849564299057173\n",
      "train loss:0.1377876490006632\n",
      "train loss:0.3198176301015155\n",
      "train loss:0.25546879413900336\n",
      "train loss:0.21727850591414466\n",
      "train loss:0.15250504652323704\n",
      "train loss:0.16387454420051983\n",
      "train loss:0.20229099587321717\n",
      "train loss:0.1797782771588135\n",
      "train loss:0.21292881945305847\n",
      "train loss:0.286476307636423\n",
      "train loss:0.1254481042685924\n",
      "train loss:0.2331242820942953\n",
      "train loss:0.19448258583443903\n",
      "train loss:0.16251243953970307\n",
      "train loss:0.2466868553534372\n",
      "train loss:0.3819749124381984\n",
      "train loss:0.18911881450417628\n",
      "train loss:0.1609749620899053\n",
      "train loss:0.1757418719241335\n",
      "train loss:0.20176975555258692\n",
      "train loss:0.09134965611031447\n",
      "train loss:0.12544614561897233\n",
      "train loss:0.12716281898697568\n",
      "train loss:0.2346679316207057\n",
      "train loss:0.11029371033485708\n",
      "train loss:0.31951484875945224\n",
      "train loss:0.19237669386429054\n",
      "train loss:0.1626818039669653\n",
      "train loss:0.16982229072889043\n",
      "train loss:0.12197452181849687\n",
      "train loss:0.1573275775851342\n",
      "train loss:0.14553813174736863\n",
      "train loss:0.19344664807636966\n",
      "train loss:0.3344154509609909\n",
      "train loss:0.11603750314192166\n",
      "train loss:0.14790322819311444\n",
      "train loss:0.17832275143765702\n",
      "train loss:0.24520829585966294\n",
      "train loss:0.2171327773813812\n",
      "train loss:0.1356974238648896\n",
      "train loss:0.19747315928195583\n",
      "train loss:0.1568685811794776\n",
      "train loss:0.1939336253384737\n",
      "train loss:0.1615949839544738\n",
      "train loss:0.2826465697919549\n",
      "train loss:0.3151117653906821\n",
      "train loss:0.1133220904184299\n",
      "train loss:0.31794390980479315\n",
      "train loss:0.15247607299245575\n",
      "train loss:0.17361713614967897\n",
      "train loss:0.10711472789947214\n",
      "train loss:0.23336746953354953\n",
      "train loss:0.14077312836733516\n",
      "train loss:0.394666427944276\n",
      "train loss:0.222100127243406\n",
      "train loss:0.16310625344802038\n",
      "train loss:0.3570683175203381\n",
      "train loss:0.2176123852957069\n",
      "train loss:0.13723112436661045\n",
      "train loss:0.33714851642922816\n",
      "train loss:0.2913705942972031\n",
      "train loss:0.2717446992651151\n",
      "train loss:0.1787052406162546\n",
      "train loss:0.20267091940154314\n",
      "train loss:0.17712818885279483\n",
      "train loss:0.17403350773950974\n",
      "train loss:0.17124930696277985\n",
      "train loss:0.11984588456724538\n",
      "train loss:0.22185391697713147\n",
      "train loss:0.33190648357218244\n",
      "train loss:0.273642363193035\n",
      "train loss:0.42044094425513556\n",
      "train loss:0.15899044998212486\n",
      "train loss:0.24510094042128713\n",
      "train loss:0.15611776033367128\n",
      "train loss:0.16638716622960467\n",
      "train loss:0.20034892470038138\n",
      "train loss:0.21871631529279095\n",
      "train loss:0.13347339102645656\n",
      "train loss:0.15013493160520602\n",
      "train loss:0.2046528149947106\n",
      "train loss:0.21630984990245897\n",
      "train loss:0.14598186061786364\n",
      "train loss:0.33842030787175714\n",
      "train loss:0.32647116286841\n",
      "train loss:0.13340291094528997\n",
      "train loss:0.27201096133268615\n",
      "train loss:0.15316940509777952\n",
      "train loss:0.2144864475942455\n",
      "train loss:0.22289897647091653\n",
      "train loss:0.09893827826485196\n",
      "train loss:0.11269167698068108\n",
      "train loss:0.16108098324922732\n",
      "train loss:0.17128351062782973\n",
      "train loss:0.33852146264523214\n",
      "train loss:0.14249324198226332\n",
      "train loss:0.15587406881582383\n",
      "train loss:0.20878111758796017\n",
      "train loss:0.10277269033693955\n",
      "train loss:0.1524484112701173\n",
      "train loss:0.3137365386463471\n",
      "train loss:0.1792728864035229\n",
      "train loss:0.06045401835382481\n",
      "train loss:0.1977526404466533\n",
      "train loss:0.1895148503094385\n",
      "train loss:0.25698316572409946\n",
      "train loss:0.19410300203194986\n",
      "train loss:0.20466272484058376\n",
      "train loss:0.42903228855220193\n",
      "train loss:0.174470878176853\n",
      "train loss:0.1850259545870635\n",
      "train loss:0.1559674314109165\n",
      "train loss:0.16272619021481835\n",
      "train loss:0.19271315225858143\n",
      "train loss:0.14315926495876183\n",
      "train loss:0.14423299470024056\n",
      "train loss:0.21003932308354822\n",
      "train loss:0.1210516371321519\n",
      "train loss:0.1029801334724008\n",
      "train loss:0.14210857813354413\n",
      "train loss:0.07461310834915202\n",
      "train loss:0.1824646963996706\n",
      "train loss:0.12975651460937962\n",
      "train loss:0.1398764146009001\n",
      "train loss:0.2547194084858726\n",
      "train loss:0.12230918690698778\n",
      "train loss:0.11699243681716177\n",
      "train loss:0.16223819366765396\n",
      "train loss:0.19415291241556748\n",
      "train loss:0.10991988887292255\n",
      "train loss:0.1643600509118774\n",
      "train loss:0.12103646602785005\n",
      "train loss:0.1686210262372267\n",
      "train loss:0.2707130318500665\n",
      "train loss:0.5106226329763897\n",
      "train loss:0.1492896173659468\n",
      "train loss:0.13260907952173512\n",
      "train loss:0.15377274948652647\n",
      "train loss:0.18660398892534963\n",
      "train loss:0.2369186699564664\n",
      "train loss:0.212291734102673\n",
      "train loss:0.2915348033316462\n",
      "train loss:0.09172141248320295\n",
      "train loss:0.13591619284784084\n",
      "train loss:0.37666156648420585\n",
      "train loss:0.24900219761365247\n",
      "train loss:0.18940050185035362\n",
      "train loss:0.13075850932535812\n",
      "train loss:0.253948364657684\n",
      "train loss:0.21002006390320524\n",
      "train loss:0.22271006615387148\n",
      "train loss:0.17705046309773378\n",
      "train loss:0.11502798745624576\n",
      "train loss:0.13957819746736666\n",
      "train loss:0.16327884253366548\n",
      "train loss:0.18750093672882426\n",
      "train loss:0.2484220452094835\n",
      "train loss:0.14582018348197606\n",
      "train loss:0.18450017895573942\n",
      "train loss:0.11337880684246882\n",
      "train loss:0.16819524616662512\n",
      "train loss:0.2157852502850004\n",
      "train loss:0.13770158301055024\n",
      "train loss:0.3073759461757941\n",
      "train loss:0.23544555323674793\n",
      "train loss:0.1878537248906226\n",
      "train loss:0.10756745136974655\n",
      "train loss:0.24421950653905106\n",
      "train loss:0.14220742739101816\n",
      "train loss:0.3052128710412937\n",
      "train loss:0.15983804248344347\n",
      "train loss:0.1951309776664649\n",
      "train loss:0.13672401762827938\n",
      "train loss:0.24564485444986425\n",
      "train loss:0.2074281615402957\n",
      "train loss:0.0647935621981515\n",
      "train loss:0.2347755818354068\n",
      "train loss:0.11705499187696483\n",
      "train loss:0.19971916249384536\n",
      "train loss:0.19296474082891377\n",
      "train loss:0.13775997308541618\n",
      "train loss:0.2134437482636881\n",
      "train loss:0.16619856756767168\n",
      "train loss:0.142691483951775\n",
      "train loss:0.11936342290399503\n",
      "train loss:0.2412448776549362\n",
      "train loss:0.1929735990249978\n",
      "train loss:0.2185855687503266\n",
      "train loss:0.10988738515795332\n",
      "train loss:0.18194879629646044\n",
      "train loss:0.20776015030925635\n",
      "train loss:0.20639557103933612\n",
      "train loss:0.07407264716670796\n",
      "train loss:0.20073695060996466\n",
      "train loss:0.2204231661795017\n",
      "train loss:0.31649468724972063\n",
      "train loss:0.20278965883631395\n",
      "train loss:0.11736144492115043\n",
      "train loss:0.09716421355768028\n",
      "train loss:0.07256558098296288\n",
      "train loss:0.1271599260468049\n",
      "train loss:0.10388673284856167\n",
      "train loss:0.20894878799547612\n",
      "train loss:0.26246575900328684\n",
      "train loss:0.1118221669819164\n",
      "train loss:0.12045527225021832\n",
      "train loss:0.14963663834605623\n",
      "train loss:0.1283034938956319\n",
      "train loss:0.09521895570318215\n",
      "train loss:0.17231782611350305\n",
      "train loss:0.11773407591784765\n",
      "train loss:0.18875238147665246\n",
      "train loss:0.21878406509773385\n",
      "train loss:0.0682669543079665\n",
      "train loss:0.11731123933125388\n",
      "train loss:0.12261266708606806\n",
      "train loss:0.17381792804466575\n",
      "train loss:0.1374888971190969\n",
      "train loss:0.14986114186725133\n",
      "train loss:0.22745979249103995\n",
      "train loss:0.07511089159623933\n",
      "train loss:0.1377944556901736\n",
      "train loss:0.13033832903518122\n",
      "train loss:0.10887973128934497\n",
      "train loss:0.13783118507720343\n",
      "train loss:0.19514065373784897\n",
      "train loss:0.24451316714214916\n",
      "train loss:0.19389715200142577\n",
      "train loss:0.2603497073802674\n",
      "train loss:0.1920416391413002\n",
      "train loss:0.21150906894485058\n",
      "train loss:0.07418268534828587\n",
      "train loss:0.15363040896532923\n",
      "train loss:0.15239836873180235\n",
      "train loss:0.24689078609895496\n",
      "train loss:0.1577535413075582\n",
      "train loss:0.13115391658420775\n",
      "train loss:0.20820301593941332\n",
      "train loss:0.16793681384933806\n",
      "train loss:0.05751536599941438\n",
      "=== epoch:2, train acc:0.951, test acc:0.953 ===\n",
      "train loss:0.18769790184701596\n",
      "train loss:0.12667282653621753\n",
      "train loss:0.1523257065846928\n",
      "train loss:0.19235336879018072\n",
      "train loss:0.13453277140571107\n",
      "train loss:0.23467007370422327\n",
      "train loss:0.07239298939178009\n",
      "train loss:0.15092142188836774\n",
      "train loss:0.1349453705577976\n",
      "train loss:0.1086192751486709\n",
      "train loss:0.13830936639415078\n",
      "train loss:0.13403297824806606\n",
      "train loss:0.22601144002277407\n",
      "train loss:0.15037516170145737\n",
      "train loss:0.16056655380944812\n",
      "train loss:0.16421760399010182\n",
      "train loss:0.21828670334012099\n",
      "train loss:0.1298786976823064\n",
      "train loss:0.12691473266090594\n",
      "train loss:0.2084312839892999\n",
      "train loss:0.13780199559533832\n",
      "train loss:0.1033572413738645\n",
      "train loss:0.19942139970772682\n",
      "train loss:0.1930988412816122\n",
      "train loss:0.13686413688592888\n",
      "train loss:0.07113676485177381\n",
      "train loss:0.12158025315022183\n",
      "train loss:0.21439436453061284\n",
      "train loss:0.08901023087163117\n",
      "train loss:0.13280856572400346\n",
      "train loss:0.07381338644064517\n",
      "train loss:0.13739408776770046\n",
      "train loss:0.1363636332646181\n",
      "train loss:0.09043189345138301\n",
      "train loss:0.23009093855097895\n",
      "train loss:0.18727444327908324\n",
      "train loss:0.16167158890300454\n",
      "train loss:0.15593198548396783\n",
      "train loss:0.1413224266398625\n",
      "train loss:0.10600384957856585\n",
      "train loss:0.0896588836678054\n",
      "train loss:0.10060727745455018\n",
      "train loss:0.16015131441399816\n",
      "train loss:0.17766867773855538\n",
      "train loss:0.14582297654328122\n",
      "train loss:0.12067150498938113\n",
      "train loss:0.20713831978920397\n",
      "train loss:0.17373281469940569\n",
      "train loss:0.10171878893168014\n",
      "train loss:0.07521323022203749\n",
      "train loss:0.15261024519923624\n",
      "train loss:0.1434097465432851\n",
      "train loss:0.18125144990373754\n",
      "train loss:0.11429082425769964\n",
      "train loss:0.10487964875241225\n",
      "train loss:0.11534412363122706\n",
      "train loss:0.14588723727215613\n",
      "train loss:0.12495185446762064\n",
      "train loss:0.16572795162608248\n",
      "train loss:0.04861409844939346\n",
      "train loss:0.3086935554213587\n",
      "train loss:0.26891219676130773\n",
      "train loss:0.10783103067154466\n",
      "train loss:0.1554035789915418\n",
      "train loss:0.09636502707340464\n",
      "train loss:0.17183516940493104\n",
      "train loss:0.09266947243133894\n",
      "train loss:0.11246697006493055\n",
      "train loss:0.14097610550918105\n",
      "train loss:0.194348239983268\n",
      "train loss:0.09394428442239787\n",
      "train loss:0.09574644221901057\n",
      "train loss:0.09942307423017686\n",
      "train loss:0.09395889666077814\n",
      "train loss:0.12556623757507857\n",
      "train loss:0.058592272390963276\n",
      "train loss:0.13605186429229826\n",
      "train loss:0.21326615186530407\n",
      "train loss:0.09601927174590827\n",
      "train loss:0.2518022897317805\n",
      "train loss:0.22946870170211786\n",
      "train loss:0.08070013399864412\n",
      "train loss:0.0929496349484084\n",
      "train loss:0.19571153108906206\n",
      "train loss:0.16108605784393068\n",
      "train loss:0.26749502937908703\n",
      "train loss:0.12553875668193276\n",
      "train loss:0.07198874414412469\n",
      "train loss:0.0864881933554336\n",
      "train loss:0.1772490626412613\n",
      "train loss:0.0902355010511599\n",
      "train loss:0.28169988726481177\n",
      "train loss:0.14242223501598908\n",
      "train loss:0.1676312086444332\n",
      "train loss:0.07916900386845574\n",
      "train loss:0.12523462526813145\n",
      "train loss:0.13692446805026018\n",
      "train loss:0.10361261130604586\n",
      "train loss:0.14437712433861677\n",
      "train loss:0.08929766054858952\n",
      "train loss:0.11139029726830546\n",
      "train loss:0.11290805368790187\n",
      "train loss:0.21956375220859306\n",
      "train loss:0.13497035476232308\n",
      "train loss:0.162728769174101\n",
      "train loss:0.16187950037492385\n",
      "train loss:0.1103160574638598\n",
      "train loss:0.10813883920500618\n",
      "train loss:0.13905086978803113\n",
      "train loss:0.08593500063629149\n",
      "train loss:0.10126301053838306\n",
      "train loss:0.11368364964126945\n",
      "train loss:0.17340617936940783\n",
      "train loss:0.1471393124864456\n",
      "train loss:0.11152950768814804\n",
      "train loss:0.06767078419984247\n",
      "train loss:0.15827376431543738\n",
      "train loss:0.12645255233555766\n",
      "train loss:0.10756426725614984\n",
      "train loss:0.19040192727582805\n",
      "train loss:0.09520096722416162\n",
      "train loss:0.1038370621348364\n",
      "train loss:0.1255286001948747\n",
      "train loss:0.08901074589860651\n",
      "train loss:0.09422124649837839\n",
      "train loss:0.08552648727491045\n",
      "train loss:0.1283363307970928\n",
      "train loss:0.1373153713085746\n",
      "train loss:0.09631744673825539\n",
      "train loss:0.20565708899852034\n",
      "train loss:0.15621169766664403\n",
      "train loss:0.17894649815852703\n",
      "train loss:0.11942777120731646\n",
      "train loss:0.12507134800038328\n",
      "train loss:0.08455181886864224\n",
      "train loss:0.09649209100861986\n",
      "train loss:0.04562798342043223\n",
      "train loss:0.15359583831832344\n",
      "train loss:0.08091828424040369\n",
      "train loss:0.08177111374529054\n",
      "train loss:0.13316422110322912\n",
      "train loss:0.13839962528684663\n",
      "train loss:0.14824312706692475\n",
      "train loss:0.13729511028391309\n",
      "train loss:0.12750333076629\n",
      "train loss:0.05317015087574403\n",
      "train loss:0.0913039161813687\n",
      "train loss:0.06142052641801955\n",
      "train loss:0.08091673081475387\n",
      "train loss:0.12035827691170843\n",
      "train loss:0.12205065287518631\n",
      "train loss:0.15696451079604418\n",
      "train loss:0.18671352454939616\n",
      "train loss:0.07933230329464598\n",
      "train loss:0.07706574116537865\n",
      "train loss:0.06394385414642731\n",
      "train loss:0.12987583171888925\n",
      "train loss:0.08428214398770698\n",
      "train loss:0.1533528846789658\n",
      "train loss:0.11663336345715986\n",
      "train loss:0.19792714870088993\n",
      "train loss:0.0552774920249386\n",
      "train loss:0.07766062951676721\n",
      "train loss:0.07988254080317866\n",
      "train loss:0.1595955263484503\n",
      "train loss:0.15365900191601198\n",
      "train loss:0.1521682288813045\n",
      "train loss:0.1369728891521852\n",
      "train loss:0.1081947704546758\n",
      "train loss:0.07169030320058375\n",
      "train loss:0.07352989214267867\n",
      "train loss:0.2689911688328387\n",
      "train loss:0.11464486486163505\n",
      "train loss:0.048280793025867116\n",
      "train loss:0.0513870147950714\n",
      "train loss:0.1689555479361899\n",
      "train loss:0.05170792971448408\n",
      "train loss:0.14893561371675682\n",
      "train loss:0.07495149826510004\n",
      "train loss:0.12560559543406008\n",
      "train loss:0.08807072008994137\n",
      "train loss:0.15347220939312878\n",
      "train loss:0.1791026998400407\n",
      "train loss:0.12980881569554822\n",
      "train loss:0.07111553187067049\n",
      "train loss:0.09941823099027515\n",
      "train loss:0.11371718025020132\n",
      "train loss:0.09889364839968894\n",
      "train loss:0.13886848235579388\n",
      "train loss:0.25698725688270385\n",
      "train loss:0.06512027836149599\n",
      "train loss:0.06346761397563407\n",
      "train loss:0.04346779248743383\n",
      "train loss:0.05583676731039028\n",
      "train loss:0.1367650130130087\n",
      "train loss:0.07985258053622975\n",
      "train loss:0.10481823317798948\n",
      "train loss:0.1981195528619772\n",
      "train loss:0.08085006014488219\n",
      "train loss:0.11051375170091912\n",
      "train loss:0.12007162359516522\n",
      "train loss:0.12996520130357164\n",
      "train loss:0.1679074932553924\n",
      "train loss:0.12314053194090929\n",
      "train loss:0.14267381767684476\n",
      "train loss:0.0850566249667489\n",
      "train loss:0.15280093215339058\n",
      "train loss:0.10355641233245619\n",
      "train loss:0.1446522103509713\n",
      "train loss:0.0944210123806104\n",
      "train loss:0.14522604495375105\n",
      "train loss:0.04634431700162291\n",
      "train loss:0.1702451419929735\n",
      "train loss:0.24741974517433912\n",
      "train loss:0.2709635283086472\n",
      "train loss:0.07424922904328128\n",
      "train loss:0.08325776762695818\n",
      "train loss:0.08672197994347842\n",
      "train loss:0.08444529814791446\n",
      "train loss:0.12105362495836954\n",
      "train loss:0.1235129746051589\n",
      "train loss:0.06943662207512684\n",
      "train loss:0.10077186853468222\n",
      "train loss:0.14520780727799878\n",
      "train loss:0.057505142992212814\n",
      "train loss:0.07939277176848723\n",
      "train loss:0.16629932060063848\n",
      "train loss:0.14209235822659227\n",
      "train loss:0.10581044014677043\n",
      "train loss:0.11282574678006199\n",
      "train loss:0.05913837391181441\n",
      "train loss:0.03260079800207978\n",
      "train loss:0.08100380901877503\n",
      "train loss:0.1510576029539833\n",
      "train loss:0.06947626676316837\n",
      "train loss:0.0812372238429096\n",
      "train loss:0.071017138022612\n",
      "train loss:0.11666662676676406\n",
      "train loss:0.0894398487503746\n",
      "train loss:0.04811198419723654\n",
      "train loss:0.1075003206828749\n",
      "train loss:0.06194137865087966\n",
      "train loss:0.10544939255674539\n",
      "train loss:0.07394109493487017\n",
      "train loss:0.10448091453731938\n",
      "train loss:0.060598656268512636\n",
      "train loss:0.06263872385184312\n",
      "train loss:0.10478733176966688\n",
      "train loss:0.1666316672862371\n",
      "train loss:0.11123436382167898\n",
      "train loss:0.06641847238904405\n",
      "train loss:0.1409420185410126\n",
      "train loss:0.07422635298119531\n",
      "train loss:0.12305878351683265\n",
      "train loss:0.12855797655835371\n",
      "train loss:0.18646016706658486\n",
      "train loss:0.14005589799685172\n",
      "train loss:0.09514618410548263\n",
      "train loss:0.23050459173841373\n",
      "train loss:0.07552144958022733\n",
      "train loss:0.07373670831993989\n",
      "train loss:0.15698043042231827\n",
      "train loss:0.11187281908516612\n",
      "train loss:0.0891152149632123\n",
      "train loss:0.22087988532672806\n",
      "train loss:0.03197437352190002\n",
      "train loss:0.12181060703514124\n",
      "train loss:0.19232564456117035\n",
      "train loss:0.050240464031222745\n",
      "train loss:0.10270111944284382\n",
      "train loss:0.0701773157976705\n",
      "train loss:0.13507575409166928\n",
      "train loss:0.06749321044507753\n",
      "train loss:0.0704531056981947\n",
      "train loss:0.2381510919917413\n",
      "train loss:0.11431922221895033\n",
      "train loss:0.081135483125603\n",
      "train loss:0.09630118262175266\n",
      "train loss:0.13644323273475628\n",
      "train loss:0.07384780377585455\n",
      "train loss:0.20693030008241975\n",
      "train loss:0.09915419669456839\n",
      "train loss:0.1693328189121125\n",
      "train loss:0.05520039915865656\n",
      "train loss:0.06452167579616994\n",
      "train loss:0.06250531401989327\n",
      "train loss:0.18238023037156398\n",
      "train loss:0.04407303554770725\n",
      "train loss:0.08598683864971887\n",
      "train loss:0.3592365375136245\n",
      "train loss:0.06729878168476305\n",
      "train loss:0.10051798841549431\n",
      "train loss:0.10957400195232418\n",
      "train loss:0.11099754061128342\n",
      "train loss:0.1907835187080372\n",
      "train loss:0.12313931337242255\n",
      "train loss:0.05116813259876238\n",
      "train loss:0.05107668954429642\n",
      "train loss:0.07953760281105864\n",
      "train loss:0.08917284400303481\n",
      "train loss:0.062030523275548595\n",
      "train loss:0.09752816993726604\n",
      "train loss:0.0858090569825597\n",
      "train loss:0.18877912298009675\n",
      "train loss:0.0757131244327962\n",
      "train loss:0.036174688716291016\n",
      "train loss:0.04159159615995744\n",
      "train loss:0.06068471459006912\n",
      "train loss:0.15412004245301053\n",
      "train loss:0.0770596391823168\n",
      "train loss:0.09930984762398799\n",
      "train loss:0.11518438369867118\n",
      "train loss:0.06839360000127959\n",
      "train loss:0.07435183335034484\n",
      "train loss:0.08232397443228624\n",
      "train loss:0.08042562872362115\n",
      "train loss:0.04835189651909582\n",
      "train loss:0.06551596672286412\n",
      "train loss:0.05626122003968846\n",
      "train loss:0.06506359095684465\n",
      "train loss:0.04003518178291818\n",
      "train loss:0.0333584701528517\n",
      "train loss:0.07752077530438371\n",
      "train loss:0.048769644437084265\n",
      "train loss:0.05300252304378563\n",
      "train loss:0.05859955148514759\n",
      "train loss:0.10955241941554422\n",
      "train loss:0.05278860596509575\n",
      "train loss:0.047272501859319915\n",
      "train loss:0.03930610740101558\n",
      "train loss:0.10142800920530087\n",
      "train loss:0.057939410909133265\n",
      "train loss:0.10245288066194441\n",
      "train loss:0.1223358775735067\n",
      "train loss:0.08712598716863573\n",
      "train loss:0.049307498955793616\n",
      "train loss:0.035255924945626974\n",
      "train loss:0.08993024772892604\n",
      "train loss:0.10952227358832195\n",
      "train loss:0.07952085200442062\n",
      "train loss:0.05761223290031323\n",
      "train loss:0.10659408776738032\n",
      "train loss:0.08492028480076526\n",
      "train loss:0.09894402687712411\n",
      "train loss:0.12662916426787388\n",
      "train loss:0.09608107504752893\n",
      "train loss:0.08343537808098325\n",
      "train loss:0.03728529097298697\n",
      "train loss:0.06576864020972954\n",
      "train loss:0.03447248543404791\n",
      "train loss:0.0714929048031336\n",
      "train loss:0.03069814702481156\n",
      "train loss:0.08827213491387625\n",
      "train loss:0.09382112378384853\n",
      "train loss:0.07456842778111389\n",
      "train loss:0.19669985541519405\n",
      "train loss:0.07399173869826424\n",
      "train loss:0.06789915975907507\n",
      "train loss:0.1158694560584031\n",
      "train loss:0.029320747196921678\n",
      "train loss:0.03864949794562889\n",
      "train loss:0.09176434209942144\n",
      "train loss:0.05180383739686109\n",
      "train loss:0.029813054822575794\n",
      "train loss:0.08753257456228226\n",
      "train loss:0.029089130725593132\n",
      "train loss:0.14851639026001728\n",
      "train loss:0.03345210409783028\n",
      "train loss:0.04821806372093815\n",
      "train loss:0.03940753727315347\n",
      "train loss:0.07566084249335868\n",
      "train loss:0.04576659149374804\n",
      "train loss:0.040433876859601925\n",
      "train loss:0.059040781554229475\n",
      "train loss:0.09654389230430387\n",
      "train loss:0.12560756977455598\n",
      "train loss:0.07988440436086659\n",
      "train loss:0.24180463129008417\n",
      "train loss:0.12028316122104227\n",
      "train loss:0.1448101333771582\n",
      "train loss:0.04489335239461392\n",
      "train loss:0.09337456676634542\n",
      "train loss:0.10786131418256781\n",
      "train loss:0.1446578809741972\n",
      "train loss:0.05521025333285381\n",
      "train loss:0.06340483191226202\n",
      "train loss:0.16768690258999283\n",
      "train loss:0.15205703573639004\n",
      "train loss:0.07410130432065377\n",
      "train loss:0.21140004612035265\n",
      "train loss:0.10097017373550328\n",
      "train loss:0.18809725938355004\n",
      "train loss:0.08224450506649608\n",
      "train loss:0.10920214001193296\n",
      "train loss:0.09512435288106406\n",
      "train loss:0.08439102541440596\n",
      "train loss:0.0780695357664676\n",
      "train loss:0.08910980002993928\n",
      "train loss:0.04873317664121656\n",
      "train loss:0.14376180803110372\n",
      "train loss:0.10342290166302424\n",
      "train loss:0.08971319324374687\n",
      "train loss:0.05601157898524349\n",
      "train loss:0.07379537585096013\n",
      "train loss:0.0850407250286747\n",
      "train loss:0.09502230033571193\n",
      "train loss:0.19713560805605268\n",
      "train loss:0.12430194618075804\n",
      "train loss:0.09320711680304995\n",
      "train loss:0.07601928310140946\n",
      "train loss:0.1541850793281968\n",
      "train loss:0.1284484503415098\n",
      "train loss:0.08398044350763684\n",
      "train loss:0.0498120174312296\n",
      "train loss:0.11810550442854825\n",
      "train loss:0.116511664789895\n",
      "train loss:0.07589356836509113\n",
      "train loss:0.13793529275779018\n",
      "train loss:0.08234265896632008\n",
      "train loss:0.05717174866606599\n",
      "train loss:0.04152272481274643\n",
      "train loss:0.06114996528752946\n",
      "train loss:0.12626241574655273\n",
      "train loss:0.06339391260820003\n",
      "train loss:0.08001453063729866\n",
      "train loss:0.04461251565038823\n",
      "train loss:0.1847617253753663\n",
      "train loss:0.06868762680453813\n",
      "train loss:0.04650538880885931\n",
      "train loss:0.15707423725055022\n",
      "train loss:0.06715193129942226\n",
      "train loss:0.03405711061172352\n",
      "train loss:0.035467229923206875\n",
      "train loss:0.06346516618218791\n",
      "train loss:0.1416581906436343\n",
      "train loss:0.065958967734822\n",
      "train loss:0.04458196952903963\n",
      "train loss:0.09804775040034984\n",
      "train loss:0.20405589194089768\n",
      "train loss:0.15246113661773752\n",
      "train loss:0.14455584722390713\n",
      "train loss:0.05195076964998799\n",
      "train loss:0.03149602457903077\n",
      "train loss:0.06598183620078381\n",
      "train loss:0.1261451804085717\n",
      "train loss:0.05079436166972756\n",
      "train loss:0.0896502140742197\n",
      "train loss:0.058474921277521234\n",
      "train loss:0.048484161589083044\n",
      "train loss:0.1705764585613337\n",
      "train loss:0.0901699341605595\n",
      "train loss:0.07110998427173444\n",
      "train loss:0.08606817001098904\n",
      "train loss:0.018829065831599833\n",
      "train loss:0.04635073313701743\n",
      "train loss:0.09158999845816862\n",
      "train loss:0.13023815960837695\n",
      "train loss:0.05188721070779617\n",
      "train loss:0.06055057794883514\n",
      "train loss:0.030447305508035295\n",
      "train loss:0.06767293557927535\n",
      "train loss:0.03487514526456467\n",
      "train loss:0.0891007765023115\n",
      "train loss:0.06793271436138607\n",
      "train loss:0.12885349413071573\n",
      "train loss:0.050508642959788706\n",
      "train loss:0.09930464856523148\n",
      "train loss:0.0489812243108092\n",
      "train loss:0.06812375540605946\n",
      "train loss:0.02659666771828885\n",
      "train loss:0.11358942390592999\n",
      "train loss:0.1262486621938846\n",
      "train loss:0.0367041854290658\n",
      "train loss:0.09503474444686896\n",
      "train loss:0.10731308255840927\n",
      "train loss:0.1362069922301887\n",
      "train loss:0.07357891925183663\n",
      "train loss:0.033351677357379275\n",
      "train loss:0.05380699033542321\n",
      "train loss:0.08663187348348123\n",
      "train loss:0.06506429903804077\n",
      "train loss:0.04385469064719212\n",
      "train loss:0.04502322107969574\n",
      "train loss:0.06421715531042133\n",
      "train loss:0.09638991651141658\n",
      "train loss:0.08150921647606632\n",
      "train loss:0.036809692900614976\n",
      "train loss:0.09922023890801322\n",
      "train loss:0.07025218922581859\n",
      "train loss:0.10403917396094885\n",
      "train loss:0.1574341407358445\n",
      "train loss:0.037377072614920445\n",
      "train loss:0.039741732044525205\n",
      "train loss:0.052605071610348116\n",
      "train loss:0.06226933337301522\n",
      "train loss:0.10695438520134148\n",
      "train loss:0.17860633127688882\n",
      "train loss:0.04514443640179075\n",
      "train loss:0.032060457474455455\n",
      "train loss:0.10024010340549364\n",
      "train loss:0.03870706214361735\n",
      "train loss:0.12530775424509064\n",
      "train loss:0.17690387516989875\n",
      "train loss:0.06582203513459904\n",
      "train loss:0.031686418430256796\n",
      "train loss:0.05532444006183779\n",
      "train loss:0.059764830807530284\n",
      "train loss:0.060113520809270835\n",
      "train loss:0.08562076401813253\n",
      "train loss:0.1550430719769525\n",
      "train loss:0.07541399142855919\n",
      "train loss:0.09651705836186596\n",
      "train loss:0.03174284860203529\n",
      "train loss:0.07263027249024226\n",
      "train loss:0.04702114320372022\n",
      "train loss:0.03956203496035744\n",
      "train loss:0.06818874488283423\n",
      "train loss:0.08540155571734298\n",
      "train loss:0.18867691203657222\n",
      "train loss:0.18409232165958098\n",
      "train loss:0.04381921717815608\n",
      "train loss:0.0933288001119253\n",
      "train loss:0.17205662196077603\n",
      "train loss:0.07806235442665993\n",
      "train loss:0.08193760086185106\n",
      "train loss:0.07727511930765268\n",
      "train loss:0.11179096434159007\n",
      "train loss:0.14205783251790716\n",
      "train loss:0.0415797700526042\n",
      "train loss:0.09510836781054398\n",
      "train loss:0.04939810014040834\n",
      "train loss:0.04817393614524862\n",
      "train loss:0.0548966684761452\n",
      "train loss:0.05950451269374696\n",
      "train loss:0.053228974488417784\n",
      "train loss:0.03207273948530739\n",
      "train loss:0.019491813255939044\n",
      "train loss:0.06483220787283338\n",
      "train loss:0.06860692043466776\n",
      "train loss:0.1702058005417665\n",
      "train loss:0.1343752634677944\n",
      "train loss:0.05296892249261619\n",
      "train loss:0.27987718449676996\n",
      "train loss:0.088860441063279\n",
      "train loss:0.05818052205804427\n",
      "train loss:0.019204470979316334\n",
      "train loss:0.08562151644183492\n",
      "train loss:0.09361844588717799\n",
      "train loss:0.05982773304607413\n",
      "train loss:0.03299117125638062\n",
      "train loss:0.05223181032048075\n",
      "train loss:0.08937445082430258\n",
      "train loss:0.05491424170796727\n",
      "train loss:0.042998563709764584\n",
      "train loss:0.05604725727909913\n",
      "train loss:0.04167936246541255\n",
      "train loss:0.03704979220876631\n",
      "train loss:0.028610653131158643\n",
      "train loss:0.06479289594097638\n",
      "train loss:0.05936816486101905\n",
      "train loss:0.0490022229438702\n",
      "train loss:0.08285241702993743\n",
      "train loss:0.020460897175172257\n",
      "train loss:0.06786820822669133\n",
      "train loss:0.07470505916872637\n",
      "train loss:0.06539400003146896\n",
      "train loss:0.07387638766454853\n",
      "train loss:0.06579220283234531\n",
      "train loss:0.14600316851387096\n",
      "train loss:0.04388975604176944\n",
      "train loss:0.047576125483982004\n",
      "train loss:0.0692632278833934\n",
      "train loss:0.08118637938327025\n",
      "train loss:0.15490806908633542\n",
      "train loss:0.05704917304050888\n",
      "train loss:0.060156468824013105\n",
      "train loss:0.011993970938200016\n",
      "train loss:0.08947974610666991\n",
      "train loss:0.056326779939561034\n",
      "train loss:0.02099906074196668\n",
      "train loss:0.04800425571341666\n",
      "train loss:0.03180454724510299\n",
      "train loss:0.11811182605597788\n",
      "train loss:0.10138205958810864\n",
      "train loss:0.11913672887080409\n",
      "train loss:0.07782663766354705\n",
      "train loss:0.14022703505119385\n",
      "train loss:0.061106593934224955\n",
      "train loss:0.05599548372154477\n",
      "train loss:0.11528197823607306\n",
      "train loss:0.02054227295704219\n",
      "train loss:0.06450939531626099\n",
      "train loss:0.027349148602273415\n",
      "train loss:0.07770602962196965\n",
      "train loss:0.05653979387374675\n",
      "train loss:0.07039762194169978\n",
      "train loss:0.08148491605781868\n",
      "train loss:0.09528591391062399\n",
      "train loss:0.04441799703516308\n",
      "train loss:0.18649700729940022\n",
      "=== epoch:3, train acc:0.971, test acc:0.977 ===\n",
      "train loss:0.053843912889302466\n",
      "train loss:0.10878193621116417\n",
      "train loss:0.049603647483429894\n",
      "train loss:0.05380429742856649\n",
      "train loss:0.12668002964119426\n",
      "train loss:0.15261791090165197\n",
      "train loss:0.03591887946066169\n",
      "train loss:0.06664058292225959\n",
      "train loss:0.0334765086421916\n",
      "train loss:0.03595955987972976\n",
      "train loss:0.160954502336996\n",
      "train loss:0.04246765138808472\n",
      "train loss:0.01785149830317554\n",
      "train loss:0.08725995722670245\n",
      "train loss:0.04852663399338119\n",
      "train loss:0.053089588871920224\n",
      "train loss:0.08572209760950047\n",
      "train loss:0.043632950133261614\n",
      "train loss:0.04904797475364492\n",
      "train loss:0.023560831428173383\n",
      "train loss:0.07171321778246059\n",
      "train loss:0.08576943328553334\n",
      "train loss:0.023951876318919086\n",
      "train loss:0.07053415972084563\n",
      "train loss:0.05971650871568587\n",
      "train loss:0.051175020249512225\n",
      "train loss:0.12131714858219796\n",
      "train loss:0.09691824802981377\n",
      "train loss:0.03776620167058086\n",
      "train loss:0.06910571664478456\n",
      "train loss:0.04695812406302394\n",
      "train loss:0.07580544844475667\n",
      "train loss:0.04874944475557976\n",
      "train loss:0.16757926250397243\n",
      "train loss:0.08651782913596161\n",
      "train loss:0.1291248882591324\n",
      "train loss:0.10316720241315695\n",
      "train loss:0.05535917617775239\n",
      "train loss:0.02940127111534912\n",
      "train loss:0.0936689941355276\n",
      "train loss:0.0507519365528311\n",
      "train loss:0.0383897477443836\n",
      "train loss:0.05375191000781161\n",
      "train loss:0.05328290750504595\n",
      "train loss:0.014908603278822652\n",
      "train loss:0.05583802435585168\n",
      "train loss:0.06204186567372125\n",
      "train loss:0.07333467624945562\n",
      "train loss:0.04560083397605709\n",
      "train loss:0.041421898479169716\n",
      "train loss:0.03289109862163741\n",
      "train loss:0.011815192443637873\n",
      "train loss:0.04943318650635752\n",
      "train loss:0.04513423648804445\n",
      "train loss:0.07247032517124362\n",
      "train loss:0.06776746986011924\n",
      "train loss:0.03117480743309314\n",
      "train loss:0.027693989385467747\n",
      "train loss:0.04506103794352485\n",
      "train loss:0.03782447335089068\n",
      "train loss:0.05773280546789115\n",
      "train loss:0.06883651846675712\n",
      "train loss:0.03320166328538365\n",
      "train loss:0.101654589818816\n",
      "train loss:0.045076775612182915\n",
      "train loss:0.1258304037490801\n",
      "train loss:0.018797843140793608\n",
      "train loss:0.1124936113069786\n",
      "train loss:0.06728525520546529\n",
      "train loss:0.02369426654007658\n",
      "train loss:0.12495340363837715\n",
      "train loss:0.18338117108825167\n",
      "train loss:0.058700788252513354\n",
      "train loss:0.05889158189611738\n",
      "train loss:0.10882613754925993\n",
      "train loss:0.08622146614972888\n",
      "train loss:0.028899654247302613\n",
      "train loss:0.03035547334093157\n",
      "train loss:0.19796613023256054\n",
      "train loss:0.04925524812575275\n",
      "train loss:0.037578481094986585\n",
      "train loss:0.03219175963807282\n",
      "train loss:0.04665005122460639\n",
      "train loss:0.07154121350054597\n",
      "train loss:0.015220477759353061\n",
      "train loss:0.09901438340204222\n",
      "train loss:0.08580156358525666\n",
      "train loss:0.12546117634365983\n",
      "train loss:0.11528060921551746\n",
      "train loss:0.08045344721126423\n",
      "train loss:0.09751748550917753\n",
      "train loss:0.0778827106921608\n",
      "train loss:0.09775610707437667\n",
      "train loss:0.11916498709522035\n",
      "train loss:0.07308488420939155\n",
      "train loss:0.07632750738284252\n",
      "train loss:0.09353244971927065\n",
      "train loss:0.05557531063526067\n",
      "train loss:0.08205158457376113\n",
      "train loss:0.04719275851838546\n",
      "train loss:0.047072129602108416\n",
      "train loss:0.015979090171044024\n",
      "train loss:0.0650219132562064\n",
      "train loss:0.03524301141359631\n",
      "train loss:0.07826579168566171\n",
      "train loss:0.06099307374366283\n",
      "train loss:0.09545817005095053\n",
      "train loss:0.10911695287322004\n",
      "train loss:0.14080661472838324\n",
      "train loss:0.056949048534944834\n",
      "train loss:0.07956365830923225\n",
      "train loss:0.13115693542265136\n",
      "train loss:0.04308010851425384\n",
      "train loss:0.08108498632474506\n",
      "train loss:0.020103871434459276\n",
      "train loss:0.03856905366604088\n",
      "train loss:0.08742121588695902\n",
      "train loss:0.09138704064812021\n",
      "train loss:0.0535224338330724\n",
      "train loss:0.07133777875224964\n",
      "train loss:0.028086990106154775\n",
      "train loss:0.05667329035444588\n",
      "train loss:0.060938276305725195\n",
      "train loss:0.15310804048134827\n",
      "train loss:0.05821376836584429\n",
      "train loss:0.11405170709821316\n",
      "train loss:0.07641321329370046\n",
      "train loss:0.05014628223401285\n",
      "train loss:0.049861681461419616\n",
      "train loss:0.03412605270359964\n",
      "train loss:0.08008839879381423\n",
      "train loss:0.028480435064387122\n",
      "train loss:0.09319237874063352\n",
      "train loss:0.06715866714815749\n",
      "train loss:0.07009734079358537\n",
      "train loss:0.0826939848782701\n",
      "train loss:0.04028807489523147\n",
      "train loss:0.012752472444289915\n",
      "train loss:0.05524467202011524\n",
      "train loss:0.05833935452787031\n",
      "train loss:0.10121933472004452\n",
      "train loss:0.0569840291199432\n",
      "train loss:0.07069766469617572\n",
      "train loss:0.01760820296385738\n",
      "train loss:0.06036378162552117\n",
      "train loss:0.04896044803764877\n",
      "train loss:0.038457474624906146\n",
      "train loss:0.0921369927755895\n",
      "train loss:0.11169061846060972\n",
      "train loss:0.027552184103194377\n",
      "train loss:0.03119133021756527\n",
      "train loss:0.054520643851599805\n",
      "train loss:0.03263381991804533\n",
      "train loss:0.0478695373597883\n",
      "train loss:0.0456377515390219\n",
      "train loss:0.08607380711965443\n",
      "train loss:0.0811420264003656\n",
      "train loss:0.13014473958747966\n",
      "train loss:0.059478024984788914\n",
      "train loss:0.042969542947845614\n",
      "train loss:0.06699634039141053\n",
      "train loss:0.08306870822228461\n",
      "train loss:0.07312144484444287\n",
      "train loss:0.12283663852602374\n",
      "train loss:0.07711453876070172\n",
      "train loss:0.048798195281390074\n",
      "train loss:0.05988754890560055\n",
      "train loss:0.0357692367652941\n",
      "train loss:0.08893227049051285\n",
      "train loss:0.055728009695113406\n",
      "train loss:0.13615090153170537\n",
      "train loss:0.04040016062374058\n",
      "train loss:0.05932936346833301\n",
      "train loss:0.11672440625912966\n",
      "train loss:0.030006333618130702\n",
      "train loss:0.07414891297678138\n",
      "train loss:0.05046135209361303\n",
      "train loss:0.054944295616889544\n",
      "train loss:0.037377531543310555\n",
      "train loss:0.05602461601851982\n",
      "train loss:0.10039530792201248\n",
      "train loss:0.032786003184800384\n",
      "train loss:0.09234467712168577\n",
      "train loss:0.053424673884828265\n",
      "train loss:0.08444429212414228\n",
      "train loss:0.029254420311003733\n",
      "train loss:0.10629682446739995\n",
      "train loss:0.045985600374964596\n",
      "train loss:0.03581320215590049\n",
      "train loss:0.04190724429836515\n",
      "train loss:0.040548001204268054\n",
      "train loss:0.024721397364496123\n",
      "train loss:0.10512404118239269\n",
      "train loss:0.09607720893732971\n",
      "train loss:0.080278075944155\n",
      "train loss:0.07017362538287769\n",
      "train loss:0.13689125763338308\n",
      "train loss:0.05034666603959084\n",
      "train loss:0.015731477419394194\n",
      "train loss:0.0608011215059153\n",
      "train loss:0.07008225620361631\n",
      "train loss:0.11698159735638601\n",
      "train loss:0.025220050139906233\n",
      "train loss:0.10689046802750467\n",
      "train loss:0.03709906416892302\n",
      "train loss:0.020623534001621358\n",
      "train loss:0.029062796013746596\n",
      "train loss:0.11104193111829282\n",
      "train loss:0.019957781377927558\n",
      "train loss:0.02696417562666008\n",
      "train loss:0.06775624019337614\n",
      "train loss:0.06453266042744686\n",
      "train loss:0.15042596310718687\n",
      "train loss:0.05340581546100049\n",
      "train loss:0.01648138384976909\n",
      "train loss:0.05389962566551405\n",
      "train loss:0.08989414364383952\n",
      "train loss:0.09973083368782061\n",
      "train loss:0.026017607041344572\n",
      "train loss:0.056229342102588925\n",
      "train loss:0.04386040588412182\n",
      "train loss:0.07060533578369096\n",
      "train loss:0.03414799874034809\n",
      "train loss:0.03977547303755249\n",
      "train loss:0.05561248189579746\n",
      "train loss:0.02128949459765825\n",
      "train loss:0.07778248875512203\n",
      "train loss:0.0729174923465639\n",
      "train loss:0.038107855970796564\n",
      "train loss:0.07688018328188564\n",
      "train loss:0.04726919544710656\n",
      "train loss:0.029067544642410716\n",
      "train loss:0.03992929070973382\n",
      "train loss:0.1314979551637159\n",
      "train loss:0.04478435540411292\n",
      "train loss:0.04259796087819752\n",
      "train loss:0.02413742343852644\n",
      "train loss:0.1037798016536031\n",
      "train loss:0.08379585878224391\n",
      "train loss:0.04979115765568272\n",
      "train loss:0.028242891379902067\n",
      "train loss:0.04444968217240103\n",
      "train loss:0.07289565951823078\n",
      "train loss:0.10263215105140105\n",
      "train loss:0.0929429373370321\n",
      "train loss:0.0627694649167584\n",
      "train loss:0.03670718942790058\n",
      "train loss:0.06270880818418316\n",
      "train loss:0.08088572802240308\n",
      "train loss:0.07269818620509635\n",
      "train loss:0.017002026100352098\n",
      "train loss:0.11678357391527404\n",
      "train loss:0.03677345432839962\n",
      "train loss:0.0747639044665879\n",
      "train loss:0.028727861919002614\n",
      "train loss:0.2280281761722437\n",
      "train loss:0.1267507865571648\n",
      "train loss:0.06282981397058017\n",
      "train loss:0.09682983825729571\n",
      "train loss:0.02733563088764108\n",
      "train loss:0.1761250858856347\n",
      "train loss:0.10013397877227774\n",
      "train loss:0.025631186043012056\n",
      "train loss:0.06283151436988231\n",
      "train loss:0.025260354283716032\n",
      "train loss:0.024464557925649215\n",
      "train loss:0.0208269590568925\n",
      "train loss:0.09071841862793022\n",
      "train loss:0.15597220871034576\n",
      "train loss:0.04240670218116948\n",
      "train loss:0.03678082783971628\n",
      "train loss:0.04286842785994594\n",
      "train loss:0.05844211842354987\n",
      "train loss:0.08360105848802428\n",
      "train loss:0.10276630044442397\n",
      "train loss:0.027471374961643504\n",
      "train loss:0.20024180317632037\n",
      "train loss:0.10294088464826842\n",
      "train loss:0.0442355260051546\n",
      "train loss:0.1054196825749806\n",
      "train loss:0.07349991836433578\n",
      "train loss:0.1822624884750947\n",
      "train loss:0.04360026152528068\n",
      "train loss:0.04115642211561385\n",
      "train loss:0.04880436390032397\n",
      "train loss:0.03358791584439806\n",
      "train loss:0.03647460001145779\n",
      "train loss:0.09616976728300161\n",
      "train loss:0.03620673208603094\n",
      "train loss:0.0399658876836587\n",
      "train loss:0.059965166756370515\n",
      "train loss:0.05185693633420791\n",
      "train loss:0.014622198214356947\n",
      "train loss:0.04511188946389578\n",
      "train loss:0.059686371469758415\n",
      "train loss:0.13887273263096392\n",
      "train loss:0.03588304606009939\n",
      "train loss:0.10426372500169015\n",
      "train loss:0.044952441048702635\n",
      "train loss:0.056593086594792154\n",
      "train loss:0.11065839086991745\n",
      "train loss:0.06391593960307972\n",
      "train loss:0.05290851823477184\n",
      "train loss:0.03870698258566899\n",
      "train loss:0.05721183140182227\n",
      "train loss:0.104095133717431\n",
      "train loss:0.02384990972552528\n",
      "train loss:0.1917754112565952\n",
      "train loss:0.05620869322119426\n",
      "train loss:0.05112058683514111\n",
      "train loss:0.03555003067692568\n",
      "train loss:0.03123431734856406\n",
      "train loss:0.16435186145965752\n",
      "train loss:0.09121807091142134\n",
      "train loss:0.021692053199506556\n",
      "train loss:0.10469155324279704\n",
      "train loss:0.007246303472070381\n",
      "train loss:0.020995374431931307\n",
      "train loss:0.030739295753904532\n",
      "train loss:0.026053460969753365\n",
      "train loss:0.040768367683966016\n",
      "train loss:0.021439737026245062\n",
      "train loss:0.053761017855866955\n",
      "train loss:0.09015142057401103\n",
      "train loss:0.020029567467636625\n",
      "train loss:0.036971907566579074\n",
      "train loss:0.02747943703113095\n",
      "train loss:0.06575388418164944\n",
      "train loss:0.017269599554172715\n",
      "train loss:0.03140873618668011\n",
      "train loss:0.035068776322224805\n",
      "train loss:0.028785656258896525\n",
      "train loss:0.036932792844490366\n",
      "train loss:0.028241334226410068\n",
      "train loss:0.05534775275633042\n",
      "train loss:0.06908340927858195\n",
      "train loss:0.035419121212053\n",
      "train loss:0.08848000054320783\n",
      "train loss:0.10776545466164761\n",
      "train loss:0.03582023575697859\n",
      "train loss:0.036068977870809454\n",
      "train loss:0.02793212932911428\n",
      "train loss:0.020469113748218325\n",
      "train loss:0.05285895102569624\n",
      "train loss:0.0835069140281197\n",
      "train loss:0.08595302117216674\n",
      "train loss:0.027798207378639875\n",
      "train loss:0.03986746901083867\n",
      "train loss:0.04889391650382425\n",
      "train loss:0.0498893011694223\n",
      "train loss:0.03082462464756597\n",
      "train loss:0.036133558935964184\n",
      "train loss:0.06334766481199199\n",
      "train loss:0.04026435654796944\n",
      "train loss:0.05593717036023071\n",
      "train loss:0.01433416219103571\n",
      "train loss:0.02009208098021997\n",
      "train loss:0.12767895630868106\n",
      "train loss:0.015020860717590252\n",
      "train loss:0.06544692727651401\n",
      "train loss:0.033406098451959215\n",
      "train loss:0.10754168434680647\n",
      "train loss:0.021787164843021355\n",
      "train loss:0.05128841663997608\n",
      "train loss:0.020852131752181747\n",
      "train loss:0.016053220124377796\n",
      "train loss:0.07528006556127328\n",
      "train loss:0.04809252802888893\n",
      "train loss:0.02140009947546393\n",
      "train loss:0.041980403382492815\n",
      "train loss:0.08622832801194605\n",
      "train loss:0.05672262608788152\n",
      "train loss:0.05534917521040806\n",
      "train loss:0.02733840192105037\n",
      "train loss:0.01410539899544919\n",
      "train loss:0.035960973944251545\n",
      "train loss:0.044486722988178656\n",
      "train loss:0.02095454920868986\n",
      "train loss:0.033515010521633214\n",
      "train loss:0.05387781560500875\n",
      "train loss:0.012987003488270865\n",
      "train loss:0.04296908850177461\n",
      "train loss:0.03524611315610343\n",
      "train loss:0.054879892441701834\n",
      "train loss:0.01686153187706697\n",
      "train loss:0.039171436643265396\n",
      "train loss:0.11041485537418894\n",
      "train loss:0.08529564531560821\n",
      "train loss:0.021248542824446447\n",
      "train loss:0.041515929003387235\n",
      "train loss:0.061578456939260985\n",
      "train loss:0.1296603973881083\n",
      "train loss:0.06868729258515474\n",
      "train loss:0.06645670944610328\n",
      "train loss:0.08269424656016704\n",
      "train loss:0.0444868527694034\n",
      "train loss:0.08015316293026244\n",
      "train loss:0.08711512582402976\n",
      "train loss:0.0874370698751785\n",
      "train loss:0.035611449124738216\n",
      "train loss:0.010980103491039511\n",
      "train loss:0.02807297857603974\n",
      "train loss:0.04260487886218871\n",
      "train loss:0.09209618979204232\n",
      "train loss:0.06727467896122048\n",
      "train loss:0.02066799239643736\n",
      "train loss:0.05453691452943052\n",
      "train loss:0.025619700458292264\n",
      "train loss:0.028346641531841986\n",
      "train loss:0.1855706409903851\n",
      "train loss:0.09191120103634282\n",
      "train loss:0.040091289541387215\n",
      "train loss:0.0231349077333096\n",
      "train loss:0.052647583332744496\n",
      "train loss:0.03537962313786417\n",
      "train loss:0.05771121907198528\n",
      "train loss:0.04646683603048706\n",
      "train loss:0.011382222344779196\n",
      "train loss:0.08384238002207449\n",
      "train loss:0.06238034058753673\n",
      "train loss:0.025944868656508833\n",
      "train loss:0.04390988740180559\n",
      "train loss:0.04879359787181745\n",
      "train loss:0.09911322027648685\n",
      "train loss:0.036790778198018334\n",
      "train loss:0.07419823976108851\n",
      "train loss:0.07469277225350027\n",
      "train loss:0.008857693412569353\n",
      "train loss:0.04564375756413855\n",
      "train loss:0.020481191546084663\n",
      "train loss:0.05108182396277235\n",
      "train loss:0.012975910298463176\n",
      "train loss:0.021288182125528094\n",
      "train loss:0.02803378757211104\n",
      "train loss:0.09917425185665968\n",
      "train loss:0.0276613892868105\n",
      "train loss:0.0350045851725874\n",
      "train loss:0.03177437781127551\n",
      "train loss:0.01895181174075487\n",
      "train loss:0.0906691324545433\n",
      "train loss:0.01837265563889639\n",
      "train loss:0.03513174644769466\n",
      "train loss:0.10635621759682383\n",
      "train loss:0.14149913273236372\n",
      "train loss:0.037133064547230483\n",
      "train loss:0.09483160800865606\n",
      "train loss:0.15045036138631326\n",
      "train loss:0.0798741022780786\n",
      "train loss:0.14527576527819888\n",
      "train loss:0.12857260347027893\n",
      "train loss:0.05797779285637253\n",
      "train loss:0.015035255280715792\n",
      "train loss:0.06678693387343496\n",
      "train loss:0.029682236048064303\n",
      "train loss:0.10481391873665094\n",
      "train loss:0.025238920255637006\n",
      "train loss:0.10856080089809556\n",
      "train loss:0.0706573622621656\n",
      "train loss:0.07885231461037447\n",
      "train loss:0.0682148875221201\n",
      "train loss:0.02485852424144589\n",
      "train loss:0.028461960967694072\n",
      "train loss:0.053873066021884713\n",
      "train loss:0.03446235421470436\n",
      "train loss:0.12355485722250072\n",
      "train loss:0.06430667519568219\n",
      "train loss:0.04634899924458484\n",
      "train loss:0.04230361979649546\n",
      "train loss:0.05320936138462007\n",
      "train loss:0.04765430112461519\n",
      "train loss:0.04618647082501048\n",
      "train loss:0.02570255020225249\n",
      "train loss:0.05970570549836145\n",
      "train loss:0.12178365048396894\n",
      "train loss:0.028675541553861268\n",
      "train loss:0.033365614438213315\n",
      "train loss:0.011758199755415533\n",
      "train loss:0.03480716595772031\n",
      "train loss:0.03181916904149497\n",
      "train loss:0.026444938597954895\n",
      "train loss:0.021068998220902406\n",
      "train loss:0.03217743923146772\n",
      "train loss:0.07363228880215739\n",
      "train loss:0.029825804101301437\n",
      "train loss:0.05319143379892535\n",
      "train loss:0.04619807613176534\n",
      "train loss:0.04075948832758745\n",
      "train loss:0.034803571245665084\n",
      "train loss:0.06026391040057847\n",
      "train loss:0.02721587602671046\n",
      "train loss:0.04101073599824594\n",
      "train loss:0.02030506067617922\n",
      "train loss:0.02695838484595719\n",
      "train loss:0.065576413420371\n",
      "train loss:0.014843305574592396\n",
      "train loss:0.04389396810858292\n",
      "train loss:0.024620149058499994\n",
      "train loss:0.03725573177232436\n",
      "train loss:0.01975465021156584\n",
      "train loss:0.06939577911479906\n",
      "train loss:0.18181011029436006\n",
      "train loss:0.031000312501803393\n",
      "train loss:0.016745172923040487\n",
      "train loss:0.08496704974254082\n",
      "train loss:0.12120447659255879\n",
      "train loss:0.07296244144959572\n",
      "train loss:0.05381466368911446\n",
      "train loss:0.04369084713873396\n",
      "train loss:0.039143193533621584\n",
      "train loss:0.05842727324385418\n",
      "train loss:0.04773601031018395\n",
      "train loss:0.030463595312969067\n",
      "train loss:0.032781751827073186\n",
      "train loss:0.11360999140413457\n",
      "train loss:0.057661274417240775\n",
      "train loss:0.1604622280230691\n",
      "train loss:0.027077388223810533\n",
      "train loss:0.01775026617697877\n",
      "train loss:0.11762973453397631\n",
      "train loss:0.04735998328936403\n",
      "train loss:0.02673566781430157\n",
      "train loss:0.06021904052075544\n",
      "train loss:0.040266745624602585\n",
      "train loss:0.017752431110313507\n",
      "train loss:0.03699941764028803\n",
      "train loss:0.06844534204375895\n",
      "train loss:0.02954835941239979\n",
      "train loss:0.043466262312661305\n",
      "train loss:0.030094345828223807\n",
      "train loss:0.011372860521203955\n",
      "train loss:0.024241372780920326\n",
      "train loss:0.10291392250150501\n",
      "train loss:0.029136360753400477\n",
      "train loss:0.025786955056140908\n",
      "train loss:0.0522513387981618\n",
      "train loss:0.05275793278084735\n",
      "train loss:0.05722115151993759\n",
      "train loss:0.018825819881619195\n",
      "train loss:0.06087059607269603\n",
      "train loss:0.033192962142015774\n",
      "train loss:0.04408085882632161\n",
      "train loss:0.0662282121990894\n",
      "train loss:0.029354866054422\n",
      "train loss:0.057557112637842744\n",
      "train loss:0.08428028429987293\n",
      "train loss:0.04385202262591235\n",
      "train loss:0.08845399428737823\n",
      "train loss:0.0068441666274508695\n",
      "train loss:0.058595586006311866\n",
      "train loss:0.0680568304909961\n",
      "train loss:0.04067890511273755\n",
      "train loss:0.04720208077546792\n",
      "train loss:0.023037442250812723\n",
      "train loss:0.06777736019828255\n",
      "train loss:0.03393752271335162\n",
      "train loss:0.09125717874417714\n",
      "train loss:0.05423977126230274\n",
      "train loss:0.03837850240068305\n",
      "train loss:0.035346923056790505\n",
      "train loss:0.10255017321655525\n",
      "train loss:0.11182241564421519\n",
      "train loss:0.050319711073397534\n",
      "train loss:0.04349069710409585\n",
      "train loss:0.03176160961521653\n",
      "train loss:0.09830374898821749\n",
      "train loss:0.04631339974907544\n",
      "train loss:0.012928711924428399\n",
      "train loss:0.03451095959270503\n",
      "train loss:0.04430662513946621\n",
      "train loss:0.04599949328716409\n",
      "train loss:0.02364186122582109\n",
      "train loss:0.0345632018105916\n",
      "train loss:0.04434544690230807\n",
      "train loss:0.017988669625860088\n",
      "train loss:0.020929081103600297\n",
      "train loss:0.023615763246331002\n",
      "train loss:0.018792493057129284\n",
      "train loss:0.0318663055431871\n",
      "train loss:0.06062572191531683\n",
      "train loss:0.025878636725345894\n",
      "train loss:0.017693509527649223\n",
      "train loss:0.02915716828180621\n",
      "train loss:0.05859266659014919\n",
      "train loss:0.0519870033211152\n",
      "train loss:0.020142185883502407\n",
      "train loss:0.016909929794077228\n",
      "train loss:0.04743061172058338\n",
      "train loss:0.06838445946117787\n",
      "train loss:0.042782057517746656\n",
      "train loss:0.04784133752453868\n",
      "train loss:0.043640369606854586\n",
      "train loss:0.03875036468363748\n",
      "train loss:0.10355391383943602\n",
      "train loss:0.05967625017882575\n",
      "train loss:0.059719539139169164\n",
      "train loss:0.03325619880407668\n",
      "train loss:0.041186683412098475\n",
      "train loss:0.03708155973320784\n",
      "train loss:0.07305370594795961\n",
      "train loss:0.0233161682965877\n",
      "=== epoch:4, train acc:0.978, test acc:0.976 ===\n",
      "train loss:0.032184932669427464\n",
      "train loss:0.020486179971944553\n",
      "train loss:0.048218258265690525\n",
      "train loss:0.023273446491438577\n",
      "train loss:0.05198617029043022\n",
      "train loss:0.03876879456633752\n",
      "train loss:0.017643738535370202\n",
      "train loss:0.06141093410015569\n",
      "train loss:0.09498274835849642\n",
      "train loss:0.0625620613819616\n",
      "train loss:0.01268132856705668\n",
      "train loss:0.11303357128426333\n",
      "train loss:0.047404073437742944\n",
      "train loss:0.05107363424376339\n",
      "train loss:0.020608092042614098\n",
      "train loss:0.06998630590298344\n",
      "train loss:0.03750079213938955\n",
      "train loss:0.04079878539975215\n",
      "train loss:0.04832150449039349\n",
      "train loss:0.05333109752532752\n",
      "train loss:0.041720699211753025\n",
      "train loss:0.045887003906116955\n",
      "train loss:0.0702933229118904\n",
      "train loss:0.04128162689453499\n",
      "train loss:0.09835008741383516\n",
      "train loss:0.019323959240778326\n",
      "train loss:0.038141134071862724\n",
      "train loss:0.040066059054491315\n",
      "train loss:0.00831064633608557\n",
      "train loss:0.0852289339039602\n",
      "train loss:0.10519377905623303\n",
      "train loss:0.06439485097322344\n",
      "train loss:0.07209684277325254\n",
      "train loss:0.02695307850152004\n",
      "train loss:0.09856105908920755\n",
      "train loss:0.053241350088579456\n",
      "train loss:0.034707862336101604\n",
      "train loss:0.09997905503408071\n",
      "train loss:0.02353835405240057\n",
      "train loss:0.0720840123255865\n",
      "train loss:0.05320755985840419\n",
      "train loss:0.04998451625821617\n",
      "train loss:0.08321979395299303\n",
      "train loss:0.04371014978405225\n",
      "train loss:0.02375415949387375\n",
      "train loss:0.020478561176310944\n",
      "train loss:0.07424074779715162\n",
      "train loss:0.020983227265053993\n",
      "train loss:0.03404949003285004\n",
      "train loss:0.06604291728273254\n",
      "train loss:0.026972227290753718\n",
      "train loss:0.03874552476774619\n",
      "train loss:0.03707692581366072\n",
      "train loss:0.010994988446134146\n",
      "train loss:0.1583694455481578\n",
      "train loss:0.032432962093665155\n",
      "train loss:0.06626322965781306\n",
      "train loss:0.03595665104776068\n",
      "train loss:0.0546662488578932\n",
      "train loss:0.03699181895537296\n",
      "train loss:0.037107683666011276\n",
      "train loss:0.030183343962717584\n",
      "train loss:0.01992551330877712\n",
      "train loss:0.03710790847865664\n",
      "train loss:0.05273265540944443\n",
      "train loss:0.02758416698224558\n",
      "train loss:0.027830768216254652\n",
      "train loss:0.07215985865915785\n",
      "train loss:0.06514761547813498\n",
      "train loss:0.017421566224803987\n",
      "train loss:0.01630725230554746\n",
      "train loss:0.07597362916211009\n",
      "train loss:0.13891811175658839\n",
      "train loss:0.07812202244536973\n",
      "train loss:0.016244616798561946\n",
      "train loss:0.0680794019205288\n",
      "train loss:0.10171274841790533\n",
      "train loss:0.0751275706289491\n",
      "train loss:0.026924447445590825\n",
      "train loss:0.07831358418156023\n",
      "train loss:0.03823993763930597\n",
      "train loss:0.05992689863306927\n",
      "train loss:0.01761429185066676\n",
      "train loss:0.03825820442153229\n",
      "train loss:0.10176991965200262\n",
      "train loss:0.04082692136777179\n",
      "train loss:0.035994622960607126\n",
      "train loss:0.041240081544883435\n",
      "train loss:0.03717809941958778\n",
      "train loss:0.06383167091572163\n",
      "train loss:0.059761905347946974\n",
      "train loss:0.07399673345103625\n",
      "train loss:0.045658537930400464\n",
      "train loss:0.07659816578477886\n",
      "train loss:0.06699772918794053\n",
      "train loss:0.08013454339705199\n",
      "train loss:0.03886989438639656\n",
      "train loss:0.041197010522798026\n",
      "train loss:0.0545059977556547\n",
      "train loss:0.02983821281967867\n",
      "train loss:0.03602042551838965\n",
      "train loss:0.02554821525053727\n",
      "train loss:0.01241082954815624\n",
      "train loss:0.05434508782285794\n",
      "train loss:0.05325342261380665\n",
      "train loss:0.00939306567336647\n",
      "train loss:0.04154286493763208\n",
      "train loss:0.03186217438763483\n",
      "train loss:0.02281752708880613\n",
      "train loss:0.049284803322431084\n",
      "train loss:0.02265787235729453\n",
      "train loss:0.03004599442572918\n",
      "train loss:0.05247562752226585\n",
      "train loss:0.04185762857443998\n",
      "train loss:0.023800718936711998\n",
      "train loss:0.12442326025036785\n",
      "train loss:0.0265673634939853\n",
      "train loss:0.09197684929832961\n",
      "train loss:0.01830289320459709\n",
      "train loss:0.03248166041052504\n",
      "train loss:0.04653980363000433\n",
      "train loss:0.07794765776873126\n",
      "train loss:0.0624718016259914\n",
      "train loss:0.04557922809126736\n",
      "train loss:0.06984408872040497\n",
      "train loss:0.10668431500971774\n",
      "train loss:0.01721744409914653\n",
      "train loss:0.0073634192218369875\n",
      "train loss:0.07300345747157981\n",
      "train loss:0.005889884710780358\n",
      "train loss:0.018125172022901873\n",
      "train loss:0.05196952212250617\n",
      "train loss:0.011598902561396536\n",
      "train loss:0.011950986530895786\n",
      "train loss:0.07217867317284196\n",
      "train loss:0.06232361154603607\n",
      "train loss:0.048098005729284285\n",
      "train loss:0.009970147500539537\n",
      "train loss:0.11675457878219683\n",
      "train loss:0.023281692036685255\n",
      "train loss:0.023139604210814863\n",
      "train loss:0.01677397776073492\n",
      "train loss:0.03726589914806377\n",
      "train loss:0.07899777958281941\n",
      "train loss:0.022911725191757028\n",
      "train loss:0.008498625859660497\n",
      "train loss:0.022505257445585024\n",
      "train loss:0.029026177203831743\n",
      "train loss:0.015104621635434603\n",
      "train loss:0.01833425777159153\n",
      "train loss:0.05012154250728306\n",
      "train loss:0.05648743458267048\n",
      "train loss:0.032588648057600395\n",
      "train loss:0.03467051285818401\n",
      "train loss:0.02146868089703688\n",
      "train loss:0.013580589879407143\n",
      "train loss:0.03705143523819065\n",
      "train loss:0.0399875561152289\n",
      "train loss:0.013958927828236647\n",
      "train loss:0.03855599953019559\n",
      "train loss:0.025141504954628746\n",
      "train loss:0.06334422479050224\n",
      "train loss:0.012339518325823468\n",
      "train loss:0.030736812320437144\n",
      "train loss:0.034013925598068956\n",
      "train loss:0.07871405358426192\n",
      "train loss:0.010255413021068518\n",
      "train loss:0.0361750612323229\n",
      "train loss:0.05505377299255516\n",
      "train loss:0.052214279335783785\n",
      "train loss:0.04627718501917855\n",
      "train loss:0.0145493879724766\n",
      "train loss:0.022596250538441557\n",
      "train loss:0.04654229869827115\n",
      "train loss:0.01914297525357028\n",
      "train loss:0.035493321312525784\n",
      "train loss:0.025810576381765787\n",
      "train loss:0.039277124797554436\n",
      "train loss:0.009268806808027554\n",
      "train loss:0.0670727242708712\n",
      "train loss:0.024166200836934344\n",
      "train loss:0.054293620147208156\n",
      "train loss:0.046020782974675295\n",
      "train loss:0.09604367386105114\n",
      "train loss:0.028594073976300677\n",
      "train loss:0.21951129597851218\n",
      "train loss:0.027956408230140006\n",
      "train loss:0.05427008002769325\n",
      "train loss:0.030075144091420914\n",
      "train loss:0.07444988509401998\n",
      "train loss:0.05642664616535428\n",
      "train loss:0.029020033099238274\n",
      "train loss:0.12490184899232676\n",
      "train loss:0.06574488563657854\n",
      "train loss:0.01424205992804358\n",
      "train loss:0.086955912012345\n",
      "train loss:0.03357463940897866\n",
      "train loss:0.06639316414518116\n",
      "train loss:0.06888206268524855\n",
      "train loss:0.07855561776241021\n",
      "train loss:0.03583168725561045\n",
      "train loss:0.03621427839158528\n",
      "train loss:0.06489170808907153\n",
      "train loss:0.026738026881000035\n",
      "train loss:0.16971744865422267\n",
      "train loss:0.033771600214398796\n",
      "train loss:0.07033611312616492\n",
      "train loss:0.017538995596425137\n",
      "train loss:0.03081664308015879\n",
      "train loss:0.08126765235200185\n",
      "train loss:0.025471745340439753\n",
      "train loss:0.013078043853992966\n",
      "train loss:0.02106434215437044\n",
      "train loss:0.07506742068971528\n",
      "train loss:0.02129228248293418\n",
      "train loss:0.02021180367967181\n",
      "train loss:0.014771131420454484\n",
      "train loss:0.019935060210999424\n",
      "train loss:0.019958744497015152\n",
      "train loss:0.028208544014963227\n",
      "train loss:0.030144556712330583\n",
      "train loss:0.015794207275349734\n",
      "train loss:0.03997756919491886\n",
      "train loss:0.039406679506421204\n",
      "train loss:0.04766910719460345\n",
      "train loss:0.06333922619273881\n",
      "train loss:0.028658303013840647\n",
      "train loss:0.03628553046947934\n",
      "train loss:0.03482568746621732\n",
      "train loss:0.05601661633302366\n",
      "train loss:0.03920035809294096\n",
      "train loss:0.05439374720846331\n",
      "train loss:0.03995226774197181\n",
      "train loss:0.0154727359934802\n",
      "train loss:0.04836568707531166\n",
      "train loss:0.010911214896562224\n",
      "train loss:0.0126105711807446\n",
      "train loss:0.06065487106399029\n",
      "train loss:0.012108199552147238\n",
      "train loss:0.05920411567859846\n",
      "train loss:0.03562412216183315\n",
      "train loss:0.092120390714666\n",
      "train loss:0.04555764051086006\n",
      "train loss:0.03963434873436342\n",
      "train loss:0.06226757854103271\n",
      "train loss:0.018500024000643964\n",
      "train loss:0.016055059992708884\n",
      "train loss:0.12411024868649667\n",
      "train loss:0.045240409996131055\n",
      "train loss:0.04218756788054786\n",
      "train loss:0.01693151248713982\n",
      "train loss:0.049185967727554215\n",
      "train loss:0.027517123006801886\n",
      "train loss:0.036855474427016005\n",
      "train loss:0.0410054400283616\n",
      "train loss:0.02572090310797824\n",
      "train loss:0.02461153837929253\n",
      "train loss:0.018246355326492117\n",
      "train loss:0.026513249682667005\n",
      "train loss:0.05880239132134891\n",
      "train loss:0.026161123440839002\n",
      "train loss:0.025691295030823708\n",
      "train loss:0.03242489363491878\n",
      "train loss:0.017285440971112863\n",
      "train loss:0.04498952002585578\n",
      "train loss:0.014042201465464553\n",
      "train loss:0.12827737891481628\n",
      "train loss:0.045445169266582174\n",
      "train loss:0.08249150182494133\n",
      "train loss:0.06882887952800062\n",
      "train loss:0.029686706970052844\n",
      "train loss:0.03752033737512876\n",
      "train loss:0.021393483489920113\n",
      "train loss:0.07934114163552469\n",
      "train loss:0.06931507810001293\n",
      "train loss:0.09896161126020334\n",
      "train loss:0.07107482765215013\n",
      "train loss:0.011855768422406265\n",
      "train loss:0.02104953440920976\n",
      "train loss:0.014009769715301972\n",
      "train loss:0.011387932764016862\n",
      "train loss:0.046791309723251205\n",
      "train loss:0.07301798447629616\n",
      "train loss:0.02865450015382267\n",
      "train loss:0.04129497243653052\n",
      "train loss:0.04681739821017974\n",
      "train loss:0.025247313753245675\n",
      "train loss:0.021228452081356864\n",
      "train loss:0.07264608885236049\n",
      "train loss:0.03443838979246216\n",
      "train loss:0.03948630301955815\n",
      "train loss:0.02478517890840285\n",
      "train loss:0.08390045701525746\n",
      "train loss:0.016650122202674398\n",
      "train loss:0.03126186511217061\n",
      "train loss:0.02911444246851253\n",
      "train loss:0.16665478414062512\n",
      "train loss:0.017403729888620913\n",
      "train loss:0.03560789706166261\n",
      "train loss:0.014028312134293466\n",
      "train loss:0.018075775068481004\n",
      "train loss:0.03328426113660346\n",
      "train loss:0.07279253254912516\n",
      "train loss:0.0385468972636754\n",
      "train loss:0.031540556440777905\n",
      "train loss:0.0842398722150866\n",
      "train loss:0.030596128362481174\n",
      "train loss:0.047109319735148626\n",
      "train loss:0.07055277348713701\n",
      "train loss:0.03481999364268546\n",
      "train loss:0.030229772172308336\n",
      "train loss:0.10459334608384886\n",
      "train loss:0.08167018929673794\n",
      "train loss:0.026331354268161182\n",
      "train loss:0.04557128185402679\n",
      "train loss:0.037244844815587215\n",
      "train loss:0.024983805329697018\n",
      "train loss:0.04480819518786845\n",
      "train loss:0.022866840897498943\n",
      "train loss:0.0660221162466111\n",
      "train loss:0.04634678244592912\n",
      "train loss:0.014975685861178398\n",
      "train loss:0.042646134197497264\n",
      "train loss:0.02739284368820205\n",
      "train loss:0.08349856842008721\n",
      "train loss:0.018442600801354153\n",
      "train loss:0.057849519351700726\n",
      "train loss:0.013355634276463779\n",
      "train loss:0.030403418875220432\n",
      "train loss:0.02520593952838838\n",
      "train loss:0.01750568255202938\n",
      "train loss:0.053188832508922694\n",
      "train loss:0.007824299382482998\n",
      "train loss:0.054765399287708724\n",
      "train loss:0.04895518236736491\n",
      "train loss:0.019372343437168735\n",
      "train loss:0.07017596467097416\n",
      "train loss:0.01448484130494667\n",
      "train loss:0.008889956227840501\n",
      "train loss:0.05471003309590647\n",
      "train loss:0.02507260496469811\n",
      "train loss:0.018491998588722105\n",
      "train loss:0.1052361171041239\n",
      "train loss:0.02255573742312877\n",
      "train loss:0.0128580167766522\n",
      "train loss:0.02627247999378939\n",
      "train loss:0.04957123352702464\n",
      "train loss:0.02445594247893805\n",
      "train loss:0.033558289565488446\n",
      "train loss:0.012016967456953511\n",
      "train loss:0.014657557855894666\n",
      "train loss:0.03667700130973294\n",
      "train loss:0.018196452293868848\n",
      "train loss:0.025811461638361458\n",
      "train loss:0.14776992691833468\n",
      "train loss:0.03695930301268949\n",
      "train loss:0.04041584079285615\n",
      "train loss:0.021055189697845103\n",
      "train loss:0.010643038904861811\n",
      "train loss:0.011188636532580579\n",
      "train loss:0.028682913690083012\n",
      "train loss:0.04290166478816829\n",
      "train loss:0.05483396697329418\n",
      "train loss:0.06460523769616708\n",
      "train loss:0.07554393745239006\n",
      "train loss:0.042740658715311255\n",
      "train loss:0.010014024895737862\n",
      "train loss:0.015183489647707583\n",
      "train loss:0.03565692895920717\n",
      "train loss:0.041013041194879925\n",
      "train loss:0.06777353580160275\n",
      "train loss:0.027265920577102528\n",
      "train loss:0.030323263029557804\n",
      "train loss:0.056447836430964404\n",
      "train loss:0.07565661651682486\n",
      "train loss:0.17317697102699992\n",
      "train loss:0.1163753862652904\n",
      "train loss:0.014927233758756748\n",
      "train loss:0.04103513394241384\n",
      "train loss:0.03138841219236035\n",
      "train loss:0.056795599732574885\n",
      "train loss:0.11229333710040462\n",
      "train loss:0.10869352817505336\n",
      "train loss:0.039386614561437605\n",
      "train loss:0.04392789336803325\n",
      "train loss:0.0066311711385643336\n",
      "train loss:0.02636805211028208\n",
      "train loss:0.01530639831552195\n",
      "train loss:0.022519704878783995\n",
      "train loss:0.009725643419990037\n",
      "train loss:0.02135065796565242\n",
      "train loss:0.0928479911540435\n",
      "train loss:0.044414910789467105\n",
      "train loss:0.019353168076249743\n",
      "train loss:0.05104938433508429\n",
      "train loss:0.06405146524707868\n",
      "train loss:0.035316752092589394\n",
      "train loss:0.07528363945991944\n",
      "train loss:0.03510682325392666\n",
      "train loss:0.03025369551683937\n",
      "train loss:0.08230849006799422\n",
      "train loss:0.01706041915739686\n",
      "train loss:0.02048883757013785\n",
      "train loss:0.024063074455367595\n",
      "train loss:0.016767190222161707\n",
      "train loss:0.009760154239288254\n",
      "train loss:0.07799378631874113\n",
      "train loss:0.015776083678650573\n",
      "train loss:0.025057222106573008\n",
      "train loss:0.0176469328953569\n",
      "train loss:0.05787558450257002\n",
      "train loss:0.03509265065594438\n",
      "train loss:0.04701062504927799\n",
      "train loss:0.034296824763505936\n",
      "train loss:0.031532016152243644\n",
      "train loss:0.1484909532373655\n",
      "train loss:0.07915821963038615\n",
      "train loss:0.016198257420745082\n",
      "train loss:0.0197636313617025\n",
      "train loss:0.05085722424756848\n",
      "train loss:0.06461319598760543\n",
      "train loss:0.02688275437875328\n",
      "train loss:0.06657956974990854\n",
      "train loss:0.019005301586020155\n",
      "train loss:0.04623679124662652\n",
      "train loss:0.023267896243241003\n",
      "train loss:0.028915242933323232\n",
      "train loss:0.02276715961817016\n",
      "train loss:0.07456283811499972\n",
      "train loss:0.05674224054583671\n",
      "train loss:0.023064955147573084\n",
      "train loss:0.020374351157912772\n",
      "train loss:0.09503058359357609\n",
      "train loss:0.03586690724403782\n",
      "train loss:0.07507858378290039\n",
      "train loss:0.04621841936254484\n",
      "train loss:0.0435469814728796\n",
      "train loss:0.013400110003765007\n",
      "train loss:0.04496213410235723\n",
      "train loss:0.03989529193371297\n",
      "train loss:0.04823904853184868\n",
      "train loss:0.07097685805982404\n",
      "train loss:0.017987321637695945\n",
      "train loss:0.014776089989612844\n",
      "train loss:0.034207601701605854\n",
      "train loss:0.009602400054074194\n",
      "train loss:0.044193578979796505\n",
      "train loss:0.03382641617112526\n",
      "train loss:0.04536418245513283\n",
      "train loss:0.06026774982939864\n",
      "train loss:0.09591222609310186\n",
      "train loss:0.06240655898021989\n",
      "train loss:0.08874504706875368\n",
      "train loss:0.017243979912811864\n",
      "train loss:0.010278147787115968\n",
      "train loss:0.045488632882323875\n",
      "train loss:0.03982046103199246\n",
      "train loss:0.0870038650280288\n",
      "train loss:0.05412054616300498\n",
      "train loss:0.025425962579861044\n",
      "train loss:0.06684370833854784\n",
      "train loss:0.0696131770764011\n",
      "train loss:0.025494930254469708\n",
      "train loss:0.05071433961226843\n",
      "train loss:0.02455018885277843\n",
      "train loss:0.05056595689321708\n",
      "train loss:0.02870263351510613\n",
      "train loss:0.02565243610693233\n",
      "train loss:0.0206257444859799\n",
      "train loss:0.07484595050603217\n",
      "train loss:0.0412581083531912\n",
      "train loss:0.01580321756023727\n",
      "train loss:0.022250788118693837\n",
      "train loss:0.07393738064817976\n",
      "train loss:0.025729243708279118\n",
      "train loss:0.008405948687717352\n",
      "train loss:0.036815900486329815\n",
      "train loss:0.05176073365362801\n",
      "train loss:0.020728783869902442\n",
      "train loss:0.027005406561198087\n",
      "train loss:0.05326210186654304\n",
      "train loss:0.03857389304640826\n",
      "train loss:0.027886084130326432\n",
      "train loss:0.08087376201966129\n",
      "train loss:0.006097465211751164\n",
      "train loss:0.08139800500912411\n",
      "train loss:0.01748813422953561\n",
      "train loss:0.09461200393328108\n",
      "train loss:0.022769692687212774\n",
      "train loss:0.025922271682562083\n",
      "train loss:0.00659553683854305\n",
      "train loss:0.03864305234681162\n",
      "train loss:0.02340808211460916\n",
      "train loss:0.09521196610532007\n",
      "train loss:0.019834791402875463\n",
      "train loss:0.019306331623505996\n",
      "train loss:0.03828759864359008\n",
      "train loss:0.08652092824181937\n",
      "train loss:0.029911206414222215\n",
      "train loss:0.02199141604484399\n",
      "train loss:0.026262453639103896\n",
      "train loss:0.11941511324459762\n",
      "train loss:0.033125915871424494\n",
      "train loss:0.008096404890629202\n",
      "train loss:0.04642679354146426\n",
      "train loss:0.12809618929281474\n",
      "train loss:0.01690019108779665\n",
      "train loss:0.024050403809736564\n",
      "train loss:0.11309375776457015\n",
      "train loss:0.02651817582308918\n",
      "train loss:0.01605313133479325\n",
      "train loss:0.046210676524962094\n",
      "train loss:0.016370031340842958\n",
      "train loss:0.06874953802080301\n",
      "train loss:0.02472460785222621\n",
      "train loss:0.049634670941397084\n",
      "train loss:0.012631896264592456\n",
      "train loss:0.046090652266850635\n",
      "train loss:0.02790786234223682\n",
      "train loss:0.020647518190616176\n",
      "train loss:0.007566681623323933\n",
      "train loss:0.0230039140875228\n",
      "train loss:0.03424254721866727\n",
      "train loss:0.01892766368781175\n",
      "train loss:0.019834857449278324\n",
      "train loss:0.018416557444680376\n",
      "train loss:0.020593249884206334\n",
      "train loss:0.018685896191329012\n",
      "train loss:0.0840648858823271\n",
      "train loss:0.04975723795693243\n",
      "train loss:0.1531041620067604\n",
      "train loss:0.026762933304215447\n",
      "train loss:0.03816656186477121\n",
      "train loss:0.012780757073691236\n",
      "train loss:0.014540974558686985\n",
      "train loss:0.016509508285638696\n",
      "train loss:0.013191897942289255\n",
      "train loss:0.01679078405980299\n",
      "train loss:0.01899890298148917\n",
      "train loss:0.026783913847656406\n",
      "train loss:0.01440288777892571\n",
      "train loss:0.012849299688830331\n",
      "train loss:0.03248893753398494\n",
      "train loss:0.005813387690371485\n",
      "train loss:0.012076418414889352\n",
      "train loss:0.03385092289637081\n",
      "train loss:0.11792026756445707\n",
      "train loss:0.037059870757396196\n",
      "train loss:0.011248663191660306\n",
      "train loss:0.08186339410781887\n",
      "train loss:0.015942973887351653\n",
      "train loss:0.11081735678824751\n",
      "train loss:0.013035765632623007\n",
      "train loss:0.008055321261337964\n",
      "train loss:0.023100660069514712\n",
      "train loss:0.01926871521752924\n",
      "train loss:0.0408306917555935\n",
      "train loss:0.11078287815378685\n",
      "train loss:0.024308672091468572\n",
      "train loss:0.05099369494198982\n",
      "train loss:0.02664553689405102\n",
      "train loss:0.020866911890577478\n",
      "train loss:0.03332848101998432\n",
      "train loss:0.018688108482229893\n",
      "train loss:0.0065307393300645155\n",
      "train loss:0.02678655282462893\n",
      "train loss:0.020060351607029046\n",
      "train loss:0.023267330061438135\n",
      "train loss:0.007649008491685965\n",
      "train loss:0.015761856131066766\n",
      "train loss:0.06472662260794364\n",
      "train loss:0.018714287383171654\n",
      "train loss:0.043696667635386065\n",
      "train loss:0.028377674316683955\n",
      "train loss:0.05271099003534578\n",
      "train loss:0.0821145620173573\n",
      "train loss:0.015494157276612993\n",
      "train loss:0.008575873259119099\n",
      "train loss:0.034476570807629135\n",
      "train loss:0.03614477286264914\n",
      "train loss:0.039215771088224334\n",
      "train loss:0.07749361657532855\n",
      "train loss:0.018473874305926386\n",
      "train loss:0.03874937426001396\n",
      "train loss:0.028059188631036514\n",
      "train loss:0.06596032025855326\n",
      "train loss:0.01726824446112648\n",
      "train loss:0.024644027550843443\n",
      "train loss:0.005068772091631853\n",
      "train loss:0.04757792555335121\n",
      "train loss:0.034640385896278575\n",
      "train loss:0.023634368452002272\n",
      "train loss:0.028067780890403193\n",
      "train loss:0.011073929104675216\n",
      "train loss:0.06426293579087171\n",
      "train loss:0.05262269446719954\n",
      "train loss:0.02028916165687378\n",
      "train loss:0.024466418476709165\n",
      "train loss:0.029219701644693855\n",
      "train loss:0.022307488034148948\n",
      "=== epoch:5, train acc:0.986, test acc:0.977 ===\n",
      "train loss:0.036055127453778826\n",
      "train loss:0.007003645777343142\n",
      "train loss:0.04567286205736785\n",
      "train loss:0.017737152421707557\n",
      "train loss:0.025846667323308887\n",
      "train loss:0.0826141949965665\n",
      "train loss:0.0655541690982358\n",
      "train loss:0.049635827650434734\n",
      "train loss:0.023049571975772055\n",
      "train loss:0.012363709237645062\n",
      "train loss:0.03913052032560381\n",
      "train loss:0.010737867349918818\n",
      "train loss:0.018264912962730317\n",
      "train loss:0.05867769245446456\n",
      "train loss:0.02121152232189134\n",
      "train loss:0.016042010421883714\n",
      "train loss:0.02897980904310563\n",
      "train loss:0.005969412213713585\n",
      "train loss:0.012803342096206753\n",
      "train loss:0.023833997006635476\n",
      "train loss:0.029134366570144624\n",
      "train loss:0.01904103611819603\n",
      "train loss:0.05337871973295205\n",
      "train loss:0.016732082033750192\n",
      "train loss:0.07685529563016055\n",
      "train loss:0.13620449872805498\n",
      "train loss:0.05138499062422686\n",
      "train loss:0.016631998515480795\n",
      "train loss:0.1358218019103092\n",
      "train loss:0.011793625174183846\n",
      "train loss:0.010513326193457372\n",
      "train loss:0.02822550726691798\n",
      "train loss:0.009714482495333233\n",
      "train loss:0.01673501944614934\n",
      "train loss:0.032360517212348575\n",
      "train loss:0.01987513101541451\n",
      "train loss:0.03278586348192631\n",
      "train loss:0.021559379719859332\n",
      "train loss:0.059407258782523335\n",
      "train loss:0.020504557491400736\n",
      "train loss:0.04110936266897169\n",
      "train loss:0.06964538513069161\n",
      "train loss:0.07266624673635859\n",
      "train loss:0.033645848159682246\n",
      "train loss:0.02308300354155279\n",
      "train loss:0.03871975695342669\n",
      "train loss:0.05545914456774641\n",
      "train loss:0.0202644659734361\n",
      "train loss:0.01388321157025792\n",
      "train loss:0.013840105876585877\n",
      "train loss:0.06985954949500191\n",
      "train loss:0.07524626515303373\n",
      "train loss:0.018076331883200424\n",
      "train loss:0.012790715324223214\n",
      "train loss:0.08361998857190742\n",
      "train loss:0.06440098580535325\n",
      "train loss:0.07605872305282299\n",
      "train loss:0.03388354099217527\n",
      "train loss:0.023445332383341688\n",
      "train loss:0.06453575837007168\n",
      "train loss:0.010372297132581604\n",
      "train loss:0.021463125802150217\n",
      "train loss:0.027776198914801917\n",
      "train loss:0.026634232545752666\n",
      "train loss:0.02170146084597647\n",
      "train loss:0.007914701753963005\n",
      "train loss:0.02326076140944603\n",
      "train loss:0.03142142490737637\n",
      "train loss:0.016609362277989835\n",
      "train loss:0.03387003581857611\n",
      "train loss:0.04076941054892796\n",
      "train loss:0.02173461752795062\n",
      "train loss:0.026761106149229632\n",
      "train loss:0.04363538847803998\n",
      "train loss:0.01723070730696248\n",
      "train loss:0.035558611476341144\n",
      "train loss:0.0345215133446706\n",
      "train loss:0.04249372308797592\n",
      "train loss:0.05432183018135202\n",
      "train loss:0.046160119855164085\n",
      "train loss:0.027334519949937977\n",
      "train loss:0.03625139255926828\n",
      "train loss:0.045962750432305605\n",
      "train loss:0.024210569154945114\n",
      "train loss:0.02549894999685271\n",
      "train loss:0.01629174973435729\n",
      "train loss:0.03331942437944378\n",
      "train loss:0.08650108015980364\n",
      "train loss:0.009922155735341996\n",
      "train loss:0.01571486963729738\n",
      "train loss:0.01646740479603043\n",
      "train loss:0.0616586188854512\n",
      "train loss:0.012526240836205283\n",
      "train loss:0.030667830749460417\n",
      "train loss:0.0536551782199516\n",
      "train loss:0.030648449088580203\n",
      "train loss:0.010876906731759934\n",
      "train loss:0.013957257413825253\n",
      "train loss:0.010032079831663248\n",
      "train loss:0.009551496447465458\n",
      "train loss:0.05874020444758074\n",
      "train loss:0.14109994125483738\n",
      "train loss:0.031044726969878318\n",
      "train loss:0.05312611546921921\n",
      "train loss:0.020847733466426422\n",
      "train loss:0.027342421499142136\n",
      "train loss:0.06660480570593845\n",
      "train loss:0.010970758583321583\n",
      "train loss:0.014640829441182245\n",
      "train loss:0.079317776064859\n",
      "train loss:0.010372223510032398\n",
      "train loss:0.014930188746888015\n",
      "train loss:0.018234662411183867\n",
      "train loss:0.007675265973479134\n",
      "train loss:0.014436590033340418\n",
      "train loss:0.055684916278855544\n",
      "train loss:0.052754175681832285\n",
      "train loss:0.02081756967882054\n",
      "train loss:0.13643750530185808\n",
      "train loss:0.031269660793293304\n",
      "train loss:0.008155982671607073\n",
      "train loss:0.03253211053868382\n",
      "train loss:0.04341548763962289\n",
      "train loss:0.0493791847745685\n",
      "train loss:0.01804007883871714\n",
      "train loss:0.017488055687048414\n",
      "train loss:0.03584640566700417\n",
      "train loss:0.03475423381535115\n",
      "train loss:0.03638781365319977\n",
      "train loss:0.042259163766203506\n",
      "train loss:0.037071888246141854\n",
      "train loss:0.017350241868272675\n",
      "train loss:0.005887692608834772\n",
      "train loss:0.01414346793060102\n",
      "train loss:0.083407098826979\n",
      "train loss:0.013558578912121054\n",
      "train loss:0.00705555962493915\n",
      "train loss:0.07033487642815883\n",
      "train loss:0.030755280844947223\n",
      "train loss:0.021049645368944932\n",
      "train loss:0.018557605856536177\n",
      "train loss:0.029372837691389334\n",
      "train loss:0.012947283647718537\n",
      "train loss:0.029728097379185865\n",
      "train loss:0.0098802894972587\n",
      "train loss:0.0545154770901798\n",
      "train loss:0.005259821955668164\n",
      "train loss:0.022559106834263338\n",
      "train loss:0.011928142744417898\n",
      "train loss:0.021562870974346462\n",
      "train loss:0.0774821222979807\n",
      "train loss:0.011561830659204424\n",
      "train loss:0.0606139693495433\n",
      "train loss:0.08079006666013056\n",
      "train loss:0.03357854462359122\n",
      "train loss:0.04244505753548701\n",
      "train loss:0.08485649279191756\n",
      "train loss:0.018791187147068126\n",
      "train loss:0.02162508351086247\n",
      "train loss:0.016614289125879\n",
      "train loss:0.029432312462982505\n",
      "train loss:0.07013255678374934\n",
      "train loss:0.05435953324153472\n",
      "train loss:0.08121148540292228\n",
      "train loss:0.011902486624061697\n",
      "train loss:0.09269285396614178\n",
      "train loss:0.04183333528916464\n",
      "train loss:0.029694754052662583\n",
      "train loss:0.0196840342918405\n",
      "train loss:0.06575585432632387\n",
      "train loss:0.019432382920533814\n",
      "train loss:0.013614311910272752\n",
      "train loss:0.05311083209080927\n",
      "train loss:0.011599196570444982\n",
      "train loss:0.014924248178710228\n",
      "train loss:0.1078998083509389\n",
      "train loss:0.1547439143453148\n",
      "train loss:0.0198610065505714\n",
      "train loss:0.052373228275088196\n",
      "train loss:0.038922600094930274\n",
      "train loss:0.038386467442830814\n",
      "train loss:0.03388103567280921\n",
      "train loss:0.02177727573730412\n",
      "train loss:0.04781382553682684\n",
      "train loss:0.029085462448574892\n",
      "train loss:0.06960452307991766\n",
      "train loss:0.057664915884691303\n",
      "train loss:0.06234775807812225\n",
      "train loss:0.029243150235594727\n",
      "train loss:0.02742134055878602\n",
      "train loss:0.03468926640058335\n",
      "train loss:0.06827525867434849\n",
      "train loss:0.06803741803120451\n",
      "train loss:0.038300336230533374\n",
      "train loss:0.014751489287660397\n",
      "train loss:0.012987897672731636\n",
      "train loss:0.04714966211727149\n",
      "train loss:0.021624070041933886\n",
      "train loss:0.03016958522205485\n",
      "train loss:0.00958977735822318\n",
      "train loss:0.010428503100123476\n",
      "train loss:0.031551900545964386\n",
      "train loss:0.007418675304218961\n",
      "train loss:0.027207686941736843\n",
      "train loss:0.041191160175998165\n",
      "train loss:0.01343541830679891\n",
      "train loss:0.016653825455850174\n",
      "train loss:0.021844839192721288\n",
      "train loss:0.04288606990091587\n",
      "train loss:0.06358768269679103\n",
      "train loss:0.01432203657310881\n",
      "train loss:0.01668978256930242\n",
      "train loss:0.02060385303410398\n",
      "train loss:0.030388200447970123\n",
      "train loss:0.05005978431787615\n",
      "train loss:0.02593086525151614\n",
      "train loss:0.04669116871574817\n",
      "train loss:0.026337495531872427\n",
      "train loss:0.018300789953794486\n",
      "train loss:0.014322970046232177\n",
      "train loss:0.014807228488193333\n",
      "train loss:0.058560492485444184\n",
      "train loss:0.046116543492395336\n",
      "train loss:0.02475624191131561\n",
      "train loss:0.07281354335605122\n",
      "train loss:0.023918519711774073\n",
      "train loss:0.07329974449992394\n",
      "train loss:0.03698057281179969\n",
      "train loss:0.013892034961944\n",
      "train loss:0.021644810604213596\n",
      "train loss:0.02277169596486381\n",
      "train loss:0.15148938361146239\n",
      "train loss:0.08383227784797546\n",
      "train loss:0.023776074716372587\n",
      "train loss:0.016073258023756747\n",
      "train loss:0.00704624953413553\n",
      "train loss:0.017254091151648963\n",
      "train loss:0.03736869416069692\n",
      "train loss:0.014813535427482483\n",
      "train loss:0.00834564842325843\n",
      "train loss:0.014287791368321881\n",
      "train loss:0.01205287360793469\n",
      "train loss:0.027587715186559326\n",
      "train loss:0.009356236480352696\n",
      "train loss:0.014720300064188917\n",
      "train loss:0.024012402164890354\n",
      "train loss:0.014417087777697048\n",
      "train loss:0.015645118874401703\n",
      "train loss:0.015414854315098047\n",
      "train loss:0.03689808036398647\n",
      "train loss:0.05564869254590826\n",
      "train loss:0.02081111770105605\n",
      "train loss:0.014541595874265671\n",
      "train loss:0.04644495569160889\n",
      "train loss:0.007795693872001456\n",
      "train loss:0.012101548598933308\n",
      "train loss:0.008657372836069542\n",
      "train loss:0.05496220386696603\n",
      "train loss:0.014698974869729564\n",
      "train loss:0.006218805899526051\n",
      "train loss:0.03324606221335364\n",
      "train loss:0.020585147604282192\n",
      "train loss:0.009580775069171942\n",
      "train loss:0.013890429367558294\n",
      "train loss:0.022558921763436875\n",
      "train loss:0.012578902285141438\n",
      "train loss:0.08640068628574457\n",
      "train loss:0.018377202540743563\n",
      "train loss:0.009652344528865906\n",
      "train loss:0.049961493794095556\n",
      "train loss:0.06827291683905215\n",
      "train loss:0.014442892278621626\n",
      "train loss:0.03137505579038073\n",
      "train loss:0.023668439202149002\n",
      "train loss:0.05796957993300837\n",
      "train loss:0.01623301097812317\n",
      "train loss:0.1256204301384608\n",
      "train loss:0.029867177688414168\n",
      "train loss:0.002211498248501547\n",
      "train loss:0.03191455354060084\n",
      "train loss:0.025786364583929227\n",
      "train loss:0.0076452870169996575\n",
      "train loss:0.03256215824328393\n",
      "train loss:0.038892607004351896\n",
      "train loss:0.054601946007274235\n",
      "train loss:0.013759821969289286\n",
      "train loss:0.01675257451254633\n",
      "train loss:0.009691026517560715\n",
      "train loss:0.07229622889528745\n",
      "train loss:0.02096986497206364\n",
      "train loss:0.030544319626067264\n",
      "train loss:0.07302032302309949\n",
      "train loss:0.004853239934762896\n",
      "train loss:0.023312928932418462\n",
      "train loss:0.01776947664553594\n",
      "train loss:0.03534837851674968\n",
      "train loss:0.015562211907845307\n",
      "train loss:0.04148729390423753\n",
      "train loss:0.013141885002794094\n",
      "train loss:0.03050908145758324\n",
      "train loss:0.017716625131518005\n",
      "train loss:0.09889254915056725\n",
      "train loss:0.024242531809778152\n",
      "train loss:0.01797604585556684\n",
      "train loss:0.01721034846408948\n",
      "train loss:0.02232221646702993\n",
      "train loss:0.023329921862129628\n",
      "train loss:0.012624028237014542\n",
      "train loss:0.025060085618899174\n",
      "train loss:0.009446843476204806\n",
      "train loss:0.09299978159488914\n",
      "train loss:0.005151375405594154\n",
      "train loss:0.1183285441663437\n",
      "train loss:0.03172480695849867\n",
      "train loss:0.011799951695257388\n",
      "train loss:0.022096973886554003\n",
      "train loss:0.003978804474627975\n",
      "train loss:0.016163180379291556\n",
      "train loss:0.04999587462401868\n",
      "train loss:0.01879425086802584\n",
      "train loss:0.014153783047905957\n",
      "train loss:0.04294247481808669\n",
      "train loss:0.07612876624376025\n",
      "train loss:0.020715241020591524\n",
      "train loss:0.017726290655408178\n",
      "train loss:0.009276435315174204\n",
      "train loss:0.021407157514551186\n",
      "train loss:0.017019951556272544\n",
      "train loss:0.011514315558978028\n",
      "train loss:0.14827475546692287\n",
      "train loss:0.011638305488880906\n",
      "train loss:0.04332955039598226\n",
      "train loss:0.04135353265167016\n",
      "train loss:0.007697459165885435\n",
      "train loss:0.014140250384491165\n",
      "train loss:0.017535743043038475\n",
      "train loss:0.04878936863481195\n",
      "train loss:0.011521059536224098\n",
      "train loss:0.03373607103884243\n",
      "train loss:0.0031449080611584447\n",
      "train loss:0.040338379581808734\n",
      "train loss:0.0070474446438212315\n",
      "train loss:0.018852020119407207\n",
      "train loss:0.01564728972958445\n",
      "train loss:0.02619466982123715\n",
      "train loss:0.1269483434947998\n",
      "train loss:0.02182711872108519\n",
      "train loss:0.06002945501226703\n",
      "train loss:0.017843648935781147\n",
      "train loss:0.04068427678842166\n",
      "train loss:0.015617951261950267\n",
      "train loss:0.025333401098039433\n",
      "train loss:0.0146277998103428\n",
      "train loss:0.03635081495588658\n",
      "train loss:0.015087378593361527\n",
      "train loss:0.017170219768498758\n",
      "train loss:0.03225739746819042\n",
      "train loss:0.010703126462418396\n",
      "train loss:0.029170248489617537\n",
      "train loss:0.038600747798998435\n",
      "train loss:0.0208776196404841\n",
      "train loss:0.015563184058320538\n",
      "train loss:0.0358454341743697\n",
      "train loss:0.01025958370085715\n",
      "train loss:0.06311894311646582\n",
      "train loss:0.08371762491729014\n",
      "train loss:0.01907552071772082\n",
      "train loss:0.05947394364211828\n",
      "train loss:0.00976942888921265\n",
      "train loss:0.05243624478113583\n",
      "train loss:0.026765883225102042\n",
      "train loss:0.012332596791223623\n",
      "train loss:0.02855102709793124\n",
      "train loss:0.01059897279773511\n",
      "train loss:0.05172067760662995\n",
      "train loss:0.015105276209490814\n",
      "train loss:0.04204446654653944\n",
      "train loss:0.014721316466753369\n",
      "train loss:0.0184409450966843\n",
      "train loss:0.03878723804675686\n",
      "train loss:0.018944513807357066\n",
      "train loss:0.0513903289524727\n",
      "train loss:0.0009497287491837136\n",
      "train loss:0.007226128362232237\n",
      "train loss:0.012843316853727228\n",
      "train loss:0.006502618087160231\n",
      "train loss:0.015475222451334354\n",
      "train loss:0.01611741138420504\n",
      "train loss:0.015362050573667255\n",
      "train loss:0.015156667242697068\n",
      "train loss:0.06454211586216868\n",
      "train loss:0.022130819894015906\n",
      "train loss:0.014591308238473126\n",
      "train loss:0.016650661053379444\n",
      "train loss:0.0070376583456337035\n",
      "train loss:0.04433727431751423\n",
      "train loss:0.0270737972354825\n",
      "train loss:0.0316612544672549\n",
      "train loss:0.035676768728955\n",
      "train loss:0.018322400393951117\n",
      "train loss:0.04345991589657701\n",
      "train loss:0.06645154559282468\n",
      "train loss:0.02160934400668653\n",
      "train loss:0.033474750183753864\n",
      "train loss:0.008576666504062079\n",
      "train loss:0.03477055116692706\n",
      "train loss:0.06241742022390242\n",
      "train loss:0.007455549788969102\n",
      "train loss:0.021797285046000744\n",
      "train loss:0.02213844358109469\n",
      "train loss:0.014078284103310223\n",
      "train loss:0.01122763420008545\n",
      "train loss:0.011114400149170367\n",
      "train loss:0.034959743269868934\n",
      "train loss:0.012710639248543163\n",
      "train loss:0.059241593429237405\n",
      "train loss:0.026240265891600268\n",
      "train loss:0.05436983984376138\n",
      "train loss:0.02032307057914192\n",
      "train loss:0.019221362798730872\n",
      "train loss:0.026041174199112237\n",
      "train loss:0.0279938577771717\n",
      "train loss:0.024666425485794542\n",
      "train loss:0.040692435409607784\n",
      "train loss:0.01768558409410682\n",
      "train loss:0.014823473807714553\n",
      "train loss:0.04352298429607549\n",
      "train loss:0.027422939467152605\n",
      "train loss:0.012070195037073414\n",
      "train loss:0.00956014335804836\n",
      "train loss:0.04944908755848601\n",
      "train loss:0.026134068330208874\n",
      "train loss:0.01147165399619251\n",
      "train loss:0.009175139521451065\n",
      "train loss:0.031604679937279716\n",
      "train loss:0.020308815913042847\n",
      "train loss:0.06174280952567044\n",
      "train loss:0.08230317305255115\n",
      "train loss:0.015252666940557697\n",
      "train loss:0.004857047514780092\n",
      "train loss:0.01587341050583252\n",
      "train loss:0.016907991237482982\n",
      "train loss:0.01444802348237377\n",
      "train loss:0.02818356833904137\n",
      "train loss:0.010850562251814662\n",
      "train loss:0.019954200396030995\n",
      "train loss:0.08790410895633916\n",
      "train loss:0.12831193780578298\n",
      "train loss:0.016654820870174697\n",
      "train loss:0.017938680858672708\n",
      "train loss:0.016847165143037003\n",
      "train loss:0.004174188744292617\n",
      "train loss:0.02607299854156083\n",
      "train loss:0.007482590492684177\n",
      "train loss:0.010029588871489444\n",
      "train loss:0.026863185755771755\n",
      "train loss:0.07320605614146732\n",
      "train loss:0.057534464397548044\n",
      "train loss:0.029725709048421853\n",
      "train loss:0.034928252091108336\n",
      "train loss:0.035904652263622355\n",
      "train loss:0.007275506819014896\n",
      "train loss:0.05208170880128385\n",
      "train loss:0.03637857758701582\n",
      "train loss:0.015978451253934125\n",
      "train loss:0.02865576471502565\n",
      "train loss:0.09765516860002427\n",
      "train loss:0.0558562795070465\n",
      "train loss:0.01531570498930974\n",
      "train loss:0.0022242089413509174\n",
      "train loss:0.05523479313423692\n",
      "train loss:0.03850569264265115\n",
      "train loss:0.05158169719638507\n",
      "train loss:0.0035598532785809907\n",
      "train loss:0.05181467138346952\n",
      "train loss:0.015409680230419194\n",
      "train loss:0.012644068697003996\n",
      "train loss:0.010825411248181484\n",
      "train loss:0.012604879857253917\n",
      "train loss:0.02327115460004593\n",
      "train loss:0.0656180996022109\n",
      "train loss:0.010045226670126348\n",
      "train loss:0.0561118845593135\n",
      "train loss:0.07495812695326189\n",
      "train loss:0.025567396480994136\n",
      "train loss:0.03185261605828042\n",
      "train loss:0.03093776254856549\n",
      "train loss:0.01301770321279895\n",
      "train loss:0.05895535559874888\n",
      "train loss:0.008948175062827114\n",
      "train loss:0.011352317651813744\n",
      "train loss:0.04725456208129528\n",
      "train loss:0.013450837972055558\n",
      "train loss:0.048252578416985596\n",
      "train loss:0.010597474198620078\n",
      "train loss:0.09257338424014082\n",
      "train loss:0.017022779952123212\n",
      "train loss:0.027025194923656203\n",
      "train loss:0.01134489356044193\n",
      "train loss:0.011510350884620364\n",
      "train loss:0.029063132750911345\n",
      "train loss:0.04201462773696229\n",
      "train loss:0.02606655914844103\n",
      "train loss:0.06303631341074369\n",
      "train loss:0.040929522831270965\n",
      "train loss:0.006090163676521921\n",
      "train loss:0.03580740786950773\n",
      "train loss:0.14917527848770348\n",
      "train loss:0.0198194526246265\n",
      "train loss:0.0157934352491668\n",
      "train loss:0.08608982219028036\n",
      "train loss:0.10300736505491524\n",
      "train loss:0.021691853668955118\n",
      "train loss:0.005147859058848767\n",
      "train loss:0.045332305213283366\n",
      "train loss:0.0083310031943753\n",
      "train loss:0.022900928244100234\n",
      "train loss:0.014397529383428904\n",
      "train loss:0.023234577744848033\n",
      "train loss:0.022093607100462025\n",
      "train loss:0.02490863910454598\n",
      "train loss:0.004130728450487181\n",
      "train loss:0.0227210879444258\n",
      "train loss:0.03865543518645619\n",
      "train loss:0.03315173780060832\n",
      "train loss:0.021622919202837383\n",
      "train loss:0.053794589269153016\n",
      "train loss:0.009698745033497177\n",
      "train loss:0.024017529289889098\n",
      "train loss:0.010843931876227468\n",
      "train loss:0.03227668679279179\n",
      "train loss:0.1018411248173029\n",
      "train loss:0.01667904188518973\n",
      "train loss:0.007929701458402341\n",
      "train loss:0.03235329176050393\n",
      "train loss:0.004415550547287125\n",
      "train loss:0.03480215573852202\n",
      "train loss:0.06340387507713104\n",
      "train loss:0.04194358345827424\n",
      "train loss:0.03196129013480624\n",
      "train loss:0.012622070647603643\n",
      "train loss:0.03174766160697308\n",
      "train loss:0.016121853406029837\n",
      "train loss:0.011945327759812257\n",
      "train loss:0.05235654455184485\n",
      "train loss:0.02122592351165206\n",
      "train loss:0.009750964548110913\n",
      "train loss:0.025815864642128337\n",
      "train loss:0.021186269302268444\n",
      "train loss:0.0451357900522562\n",
      "train loss:0.09024965284237325\n",
      "train loss:0.009411825400987507\n",
      "train loss:0.022157464325571544\n",
      "train loss:0.01705610973700951\n",
      "train loss:0.013532425673691345\n",
      "train loss:0.018558093077571565\n",
      "train loss:0.013462218733100719\n",
      "train loss:0.012102039853358636\n",
      "train loss:0.06375997495185197\n",
      "train loss:0.01881737952416585\n",
      "train loss:0.0506771386857895\n",
      "train loss:0.010280530206059292\n",
      "train loss:0.005337566422474877\n",
      "train loss:0.048286557095364356\n",
      "train loss:0.0165884558242518\n",
      "train loss:0.008918783552084428\n",
      "train loss:0.06574743109445551\n",
      "train loss:0.00557200122978041\n",
      "train loss:0.05319452469721134\n",
      "train loss:0.00995157272341605\n",
      "train loss:0.011744603412727593\n",
      "train loss:0.026357385154853037\n",
      "train loss:0.0020568854356554284\n",
      "train loss:0.040700617910212754\n",
      "train loss:0.023004386974132048\n",
      "train loss:0.016378523820023444\n",
      "train loss:0.02166027906384148\n",
      "train loss:0.017833869384226896\n",
      "train loss:0.007244762894841931\n",
      "train loss:0.018743092788400705\n",
      "train loss:0.015703063683626218\n",
      "train loss:0.02691865480430084\n",
      "train loss:0.013870772191969374\n",
      "train loss:0.01654921699381402\n",
      "train loss:0.05091467163184957\n",
      "train loss:0.023547696265446852\n",
      "train loss:0.054705059604061954\n",
      "train loss:0.005604113247553243\n",
      "train loss:0.007420125166734738\n",
      "train loss:0.013666041410998461\n",
      "train loss:0.009304320413752684\n",
      "train loss:0.0097526798893286\n",
      "train loss:0.056126210427149194\n",
      "train loss:0.03685085312560222\n",
      "train loss:0.04148210785846807\n",
      "train loss:0.025185197197084955\n",
      "train loss:0.04474679878653312\n",
      "train loss:0.051788781340185414\n",
      "train loss:0.025884968986829487\n",
      "train loss:0.004535227497354163\n",
      "=== epoch:6, train acc:0.987, test acc:0.978 ===\n",
      "train loss:0.044101941066312184\n",
      "train loss:0.017831655396804976\n",
      "train loss:0.009344753576284006\n",
      "train loss:0.012108920629529283\n",
      "train loss:0.029015583138534872\n",
      "train loss:0.06265443318959035\n",
      "train loss:0.05085436643749871\n",
      "train loss:0.03322900051351351\n",
      "train loss:0.05382031431486075\n",
      "train loss:0.025376114572478802\n",
      "train loss:0.021343045678561188\n",
      "train loss:0.07360362360956746\n",
      "train loss:0.07192060436509812\n",
      "train loss:0.05401291086091197\n",
      "train loss:0.0396709921092648\n",
      "train loss:0.0408021526124489\n",
      "train loss:0.012011253318587389\n",
      "train loss:0.08067602207909745\n",
      "train loss:0.028218165337148403\n",
      "train loss:0.01790238891427562\n",
      "train loss:0.015288572450802696\n",
      "train loss:0.005704164307285559\n",
      "train loss:0.011009395182039017\n",
      "train loss:0.017266588042652204\n",
      "train loss:0.01121181499582161\n",
      "train loss:0.10236076393551842\n",
      "train loss:0.02477182604838922\n",
      "train loss:0.015497226933041423\n",
      "train loss:0.06390237742858812\n",
      "train loss:0.059790627104843744\n",
      "train loss:0.014064683892758278\n",
      "train loss:0.026600329664000423\n",
      "train loss:0.011464911575051695\n",
      "train loss:0.018592101375395388\n",
      "train loss:0.03395368151535\n",
      "train loss:0.09548174989987877\n",
      "train loss:0.015421026847528229\n",
      "train loss:0.005938715360967414\n",
      "train loss:0.024319537227157996\n",
      "train loss:0.013685190186225453\n",
      "train loss:0.044489953632216445\n",
      "train loss:0.09794553552284148\n",
      "train loss:0.02531222225377032\n",
      "train loss:0.02678723244811995\n",
      "train loss:0.016418930233374016\n",
      "train loss:0.0106561683586108\n",
      "train loss:0.02638220138750028\n",
      "train loss:0.045675972643423074\n",
      "train loss:0.02287293211178008\n",
      "train loss:0.05013445121931646\n",
      "train loss:0.0677596441618988\n",
      "train loss:0.02556900487899831\n",
      "train loss:0.03936263093458957\n",
      "train loss:0.11070599741098248\n",
      "train loss:0.0347887946577654\n",
      "train loss:0.0072301417381875534\n",
      "train loss:0.0176649777198183\n",
      "train loss:0.019514264409312993\n",
      "train loss:0.02429124972803176\n",
      "train loss:0.04535250682254132\n",
      "train loss:0.04258385913387432\n",
      "train loss:0.07346586717941188\n",
      "train loss:0.006248092091378339\n",
      "train loss:0.007523531921769969\n",
      "train loss:0.01522666632685386\n",
      "train loss:0.010494298711615476\n",
      "train loss:0.029497001096537454\n",
      "train loss:0.008964124960362233\n",
      "train loss:0.008818996851577636\n",
      "train loss:0.02642766240812349\n",
      "train loss:0.03681550853217436\n",
      "train loss:0.010377242169707299\n",
      "train loss:0.019286660799428985\n",
      "train loss:0.016733133327610682\n",
      "train loss:0.045125530608049094\n",
      "train loss:0.007759827013773424\n",
      "train loss:0.09005531207669866\n",
      "train loss:0.02106314172644281\n",
      "train loss:0.013794525758816852\n",
      "train loss:0.019539708453757954\n",
      "train loss:0.0197005892198225\n",
      "train loss:0.016431925081841704\n",
      "train loss:0.03276100783074891\n",
      "train loss:0.07208251649004863\n",
      "train loss:0.026860182780017262\n",
      "train loss:0.015116922816394575\n",
      "train loss:0.024919996430782354\n",
      "train loss:0.01141606717565936\n",
      "train loss:0.0026008926921876185\n",
      "train loss:0.009708114387289074\n",
      "train loss:0.009103965035343083\n",
      "train loss:0.00526372011771696\n",
      "train loss:0.04606458437568256\n",
      "train loss:0.005492809510744788\n",
      "train loss:0.005246398596570068\n",
      "train loss:0.012481125087853286\n",
      "train loss:0.015465204004032\n",
      "train loss:0.00913889732561615\n",
      "train loss:0.006411949451071769\n",
      "train loss:0.012228636238288813\n",
      "train loss:0.01797112834276137\n",
      "train loss:0.06845820054964212\n",
      "train loss:0.042225227846331544\n",
      "train loss:0.014597881129289598\n",
      "train loss:0.011190052204978578\n",
      "train loss:0.009724513892265484\n",
      "train loss:0.06239981482368782\n",
      "train loss:0.014774680421434592\n",
      "train loss:0.020789586801832866\n",
      "train loss:0.01334967078431193\n",
      "train loss:0.017030487390293544\n",
      "train loss:0.0030834947556089213\n",
      "train loss:0.031693702391045034\n",
      "train loss:0.07627120224357042\n",
      "train loss:0.027874748840709618\n",
      "train loss:0.02894558874087779\n",
      "train loss:0.022407050513899556\n",
      "train loss:0.007938644534185087\n",
      "train loss:0.009512882277700946\n",
      "train loss:0.005611516811749289\n",
      "train loss:0.008774347932201242\n",
      "train loss:0.014338419396537452\n",
      "train loss:0.04702281801567998\n",
      "train loss:0.008035843515722335\n",
      "train loss:0.0110838141831429\n",
      "train loss:0.024033637005424744\n",
      "train loss:0.0172753244100862\n",
      "train loss:0.023800408313143964\n",
      "train loss:0.005599564170423716\n",
      "train loss:0.005873053554596297\n",
      "train loss:0.024853224205101762\n",
      "train loss:0.005874600965872343\n",
      "train loss:0.018602101170856464\n",
      "train loss:0.007963944925639453\n",
      "train loss:0.04646236389638727\n",
      "train loss:0.0051212672650548605\n",
      "train loss:0.009234383428854971\n",
      "train loss:0.009455580136695273\n",
      "train loss:0.020899575418483057\n",
      "train loss:0.01705615349142847\n",
      "train loss:0.0054679274740758645\n",
      "train loss:0.046236288563781015\n",
      "train loss:0.0414786398211114\n",
      "train loss:0.15231831655357542\n",
      "train loss:0.05213472684744934\n",
      "train loss:0.04865833459567482\n",
      "train loss:0.005161559509744878\n",
      "train loss:0.016789413872067085\n",
      "train loss:0.05798442366244412\n",
      "train loss:0.043639176664186\n",
      "train loss:0.011377799061958637\n",
      "train loss:0.00488235296478573\n",
      "train loss:0.012629500759056141\n",
      "train loss:0.016016286051043556\n",
      "train loss:0.013252881817877123\n",
      "train loss:0.019359169495826677\n",
      "train loss:0.014935443638843297\n",
      "train loss:0.017785468630727837\n",
      "train loss:0.005292496602329011\n",
      "train loss:0.09230231101714029\n",
      "train loss:0.002777647985993376\n",
      "train loss:0.031311910697249194\n",
      "train loss:0.016991031688365684\n",
      "train loss:0.021263020753309964\n",
      "train loss:0.0029977635875955404\n",
      "train loss:0.03579494022161616\n",
      "train loss:0.01365016053264526\n",
      "train loss:0.0022152971736688534\n",
      "train loss:0.02531578075144588\n",
      "train loss:0.007231791931262613\n",
      "train loss:0.01686529118959551\n",
      "train loss:0.02264337084308878\n",
      "train loss:0.023104375816914477\n",
      "train loss:0.01668418888394923\n",
      "train loss:0.00826719468092761\n",
      "train loss:0.03207236169392709\n",
      "train loss:0.01897467348271123\n",
      "train loss:0.013207517233716373\n",
      "train loss:0.047616920748588205\n",
      "train loss:0.018871523782740138\n",
      "train loss:0.06812250250955876\n",
      "train loss:0.015808315009914652\n",
      "train loss:0.015938002292185557\n",
      "train loss:0.02035925584520755\n",
      "train loss:0.010094606537078624\n",
      "train loss:0.028385165100783687\n",
      "train loss:0.016320934463646134\n",
      "train loss:0.08399366210422059\n",
      "train loss:0.06804360647314366\n",
      "train loss:0.02633161867960456\n",
      "train loss:0.03659753452908137\n",
      "train loss:0.03344173426315852\n",
      "train loss:0.015781015016013818\n",
      "train loss:0.0019437854383459168\n",
      "train loss:0.052238172750699016\n",
      "train loss:0.022813573451806537\n",
      "train loss:0.007716379290108729\n",
      "train loss:0.020532390766845696\n",
      "train loss:0.020297927982559538\n",
      "train loss:0.006576533255740652\n",
      "train loss:0.02196354326968807\n",
      "train loss:0.00996939507811159\n",
      "train loss:0.013134907015894511\n",
      "train loss:0.011776817121640016\n",
      "train loss:0.01566992594038968\n",
      "train loss:0.02744165717336303\n",
      "train loss:0.02762096996309663\n",
      "train loss:0.004833076321783918\n",
      "train loss:0.020112525653147046\n",
      "train loss:0.08555611100705812\n",
      "train loss:0.005505287612156503\n",
      "train loss:0.0633742501118067\n",
      "train loss:0.038659116824745654\n",
      "train loss:0.014596713536846086\n",
      "train loss:0.007531779888335287\n",
      "train loss:0.02055951927219941\n",
      "train loss:0.011718982870630288\n",
      "train loss:0.036247412270418326\n",
      "train loss:0.08246781379432383\n",
      "train loss:0.03977842756983581\n",
      "train loss:0.004599284758237574\n",
      "train loss:0.029397697045389318\n",
      "train loss:0.021018303394337634\n",
      "train loss:0.03082432767789632\n",
      "train loss:0.028080399986109804\n",
      "train loss:0.013429362664740967\n",
      "train loss:0.020062578157870544\n",
      "train loss:0.007990123501063904\n",
      "train loss:0.016732329169626407\n",
      "train loss:0.004370739800640232\n",
      "train loss:0.1080078004664879\n",
      "train loss:0.01362964940716604\n",
      "train loss:0.011419844657882365\n",
      "train loss:0.023953294324185036\n",
      "train loss:0.01940757796840537\n",
      "train loss:0.012427324835057258\n",
      "train loss:0.03514770694715271\n",
      "train loss:0.022633888644192184\n",
      "train loss:0.009245459443923909\n",
      "train loss:0.03260090268012617\n",
      "train loss:0.011601390088985556\n",
      "train loss:0.013731810204042611\n",
      "train loss:0.013716060599480974\n",
      "train loss:0.029973532107472195\n",
      "train loss:0.01354583404280742\n",
      "train loss:0.019445955866513754\n",
      "train loss:0.006051335651953878\n",
      "train loss:0.013010552603166282\n",
      "train loss:0.002031348099213474\n",
      "train loss:0.009675507472479387\n",
      "train loss:0.03599570409111872\n",
      "train loss:0.022256336742700188\n",
      "train loss:0.00650098244045668\n",
      "train loss:0.011342570343945568\n",
      "train loss:0.022279343995228948\n",
      "train loss:0.00561709630192529\n",
      "train loss:0.05755164820389304\n",
      "train loss:0.016369935512032485\n",
      "train loss:0.008720072819474225\n",
      "train loss:0.02870704942069383\n",
      "train loss:0.01956331151735994\n",
      "train loss:0.005832836411026866\n",
      "train loss:0.026353311710024134\n",
      "train loss:0.006068847943174734\n",
      "train loss:0.02227457534956884\n",
      "train loss:0.013145777619576159\n",
      "train loss:0.0034826972737007555\n",
      "train loss:0.004376271264572699\n",
      "train loss:0.012032592831429411\n",
      "train loss:0.011203329544953082\n",
      "train loss:0.06005949962984326\n",
      "train loss:0.007973821543173923\n",
      "train loss:0.008290467916518364\n",
      "train loss:0.029340370343035077\n",
      "train loss:0.005182829593171393\n",
      "train loss:0.003257048236767592\n",
      "train loss:0.03258691857356833\n",
      "train loss:0.03051870421942178\n",
      "train loss:0.0064248325858972935\n",
      "train loss:0.00431822025476027\n",
      "train loss:0.005274864181748062\n",
      "train loss:0.07866791422370698\n",
      "train loss:0.06983141991901554\n",
      "train loss:0.012614687854785431\n",
      "train loss:0.0715312034138358\n",
      "train loss:0.011273639889814873\n",
      "train loss:0.060910414840206165\n",
      "train loss:0.01069866697879356\n",
      "train loss:0.01607654403633819\n",
      "train loss:0.029885990150565186\n",
      "train loss:0.027024719620456952\n",
      "train loss:0.011728934830821117\n",
      "train loss:0.03289517391616952\n",
      "train loss:0.006444073331780181\n",
      "train loss:0.0018704633892956547\n",
      "train loss:0.02317557703550551\n",
      "train loss:0.03305588196981371\n",
      "train loss:0.013412009786755975\n",
      "train loss:0.03690705273145471\n",
      "train loss:0.007473437530447464\n",
      "train loss:0.007898377834734046\n",
      "train loss:0.01653926436349664\n",
      "train loss:0.015157722078481851\n",
      "train loss:0.0071715737922995225\n",
      "train loss:0.01048279895066775\n",
      "train loss:0.005916407975256515\n",
      "train loss:0.00948917248272706\n",
      "train loss:0.014073395593884774\n",
      "train loss:0.013498457606569505\n",
      "train loss:0.03475324470364741\n",
      "train loss:0.015906060279161492\n",
      "train loss:0.17863112553739646\n",
      "train loss:0.033300410277052446\n",
      "train loss:0.03073426613711335\n",
      "train loss:0.0032764777527377713\n",
      "train loss:0.014293455715791725\n",
      "train loss:0.01748661840739608\n",
      "train loss:0.00934468222042429\n",
      "train loss:0.007765440296615323\n",
      "train loss:0.024794544777635583\n",
      "train loss:0.04312094404975615\n",
      "train loss:0.011972954845295047\n",
      "train loss:0.007046574171629969\n",
      "train loss:0.022029499754677734\n",
      "train loss:0.011743220718301604\n",
      "train loss:0.022989220356713486\n",
      "train loss:0.01908283746543604\n",
      "train loss:0.05054691612832863\n",
      "train loss:0.010059400705495573\n",
      "train loss:0.03402814219550093\n",
      "train loss:0.02481934960671151\n",
      "train loss:0.018253718507237053\n",
      "train loss:0.017099742900751697\n",
      "train loss:0.02824226985124796\n",
      "train loss:0.021919251490667615\n",
      "train loss:0.029680691463985484\n",
      "train loss:0.01719789930283292\n",
      "train loss:0.008733746330050952\n",
      "train loss:0.00446434561841645\n",
      "train loss:0.010341355081078907\n",
      "train loss:0.025412941017161607\n",
      "train loss:0.031240399073553315\n",
      "train loss:0.006647075335171465\n",
      "train loss:0.023404276213082945\n",
      "train loss:0.0030496746658027845\n",
      "train loss:0.010060793633013845\n",
      "train loss:0.022170752872714607\n",
      "train loss:0.005755939799083982\n",
      "train loss:0.023063816761402064\n",
      "train loss:0.019439519525755365\n",
      "train loss:0.01190702350497181\n",
      "train loss:0.006913044616940348\n",
      "train loss:0.005186653398892168\n",
      "train loss:0.06731240718422005\n",
      "train loss:0.009745027071301253\n",
      "train loss:0.0068746911620970585\n",
      "train loss:0.042501089555160305\n",
      "train loss:0.03432908368044691\n",
      "train loss:0.006431044376229924\n",
      "train loss:0.03216170118779397\n",
      "train loss:0.013563430784329422\n",
      "train loss:0.004405959495091409\n",
      "train loss:0.009156714227878502\n",
      "train loss:0.015051222890724102\n",
      "train loss:0.016373366295114697\n",
      "train loss:0.004255307866193404\n",
      "train loss:0.009864747495877828\n",
      "train loss:0.02570163265869255\n",
      "train loss:0.030385050958075886\n",
      "train loss:0.053374694236251\n",
      "train loss:0.058822231225499115\n",
      "train loss:0.04507085799392439\n",
      "train loss:0.06629017332935341\n",
      "train loss:0.03198199876518327\n",
      "train loss:0.006171501274087887\n",
      "train loss:0.04037065786900552\n",
      "train loss:0.025490670486388343\n",
      "train loss:0.04403857804615824\n",
      "train loss:0.023035350267787157\n",
      "train loss:0.011278180306312334\n",
      "train loss:0.04694455765702358\n",
      "train loss:0.012830027219532974\n",
      "train loss:0.023802230456550357\n",
      "train loss:0.002775207205473924\n",
      "train loss:0.009692261683547988\n",
      "train loss:0.007827815977540283\n",
      "train loss:0.009512719038010067\n",
      "train loss:0.01097770656520792\n",
      "train loss:0.029920595309697772\n",
      "train loss:0.022569166363891793\n",
      "train loss:0.019366199501199523\n",
      "train loss:0.041984240284140564\n",
      "train loss:0.02261661979911798\n",
      "train loss:0.023589123033520863\n",
      "train loss:0.11482668596452922\n",
      "train loss:0.02401425149873713\n",
      "train loss:0.01354238040643837\n",
      "train loss:0.06798760628715898\n",
      "train loss:0.003248657185015802\n",
      "train loss:0.1125465282405349\n",
      "train loss:0.030204849338218587\n",
      "train loss:0.020036287618797385\n",
      "train loss:0.003645319360602493\n",
      "train loss:0.023199850769153124\n",
      "train loss:0.01444743591484524\n",
      "train loss:0.015807845226713267\n",
      "train loss:0.011730149389238173\n",
      "train loss:0.017543876396427512\n",
      "train loss:0.10322784964756104\n",
      "train loss:0.012068292997757938\n",
      "train loss:0.024160600338031454\n",
      "train loss:0.04664006827808101\n",
      "train loss:0.013372163599089952\n",
      "train loss:0.007221856255858234\n",
      "train loss:0.014170409098278911\n",
      "train loss:0.015777880177886002\n",
      "train loss:0.007669480352286333\n",
      "train loss:0.005270431367199561\n",
      "train loss:0.07406665204476406\n",
      "train loss:0.0818849205551539\n",
      "train loss:0.024261080703550014\n",
      "train loss:0.05344653665281504\n",
      "train loss:0.09337718766106551\n",
      "train loss:0.0248403986080086\n",
      "train loss:0.02802792585427369\n",
      "train loss:0.00707301864995544\n",
      "train loss:0.02206115062474425\n",
      "train loss:0.014225166802514637\n",
      "train loss:0.005172417535835619\n",
      "train loss:0.016629103640297196\n",
      "train loss:0.05498304433918132\n",
      "train loss:0.005031065250534108\n",
      "train loss:0.004454682873108916\n",
      "train loss:0.027342725573770824\n",
      "train loss:0.01592240901985456\n",
      "train loss:0.03372061037187472\n",
      "train loss:0.040155949466687\n",
      "train loss:0.06544205638132508\n",
      "train loss:0.006759052328976387\n",
      "train loss:0.06156474351613061\n",
      "train loss:0.05302553243492326\n",
      "train loss:0.01011721170388196\n",
      "train loss:0.005638091698611168\n",
      "train loss:0.013130542662699729\n",
      "train loss:0.02531251425211158\n",
      "train loss:0.0074000169160978535\n",
      "train loss:0.0039990368297588095\n",
      "train loss:0.02892556465146999\n",
      "train loss:0.027409401893306127\n",
      "train loss:0.026147243994082654\n",
      "train loss:0.008326125828794783\n",
      "train loss:0.016915539013697977\n",
      "train loss:0.02054321121862129\n",
      "train loss:0.021140651382372696\n",
      "train loss:0.09379511329626297\n",
      "train loss:0.007269274167343854\n",
      "train loss:0.022140247117010986\n",
      "train loss:0.03645299549963154\n",
      "train loss:0.0283567538831556\n",
      "train loss:0.016009599502267536\n",
      "train loss:0.03052263378430953\n",
      "train loss:0.07964126627609723\n",
      "train loss:0.028552506751528952\n",
      "train loss:0.0415087468943313\n",
      "train loss:0.01984429434544144\n",
      "train loss:0.006953974211802343\n",
      "train loss:0.04023552147530458\n",
      "train loss:0.012391239721508872\n",
      "train loss:0.011783685446144958\n",
      "train loss:0.01091482022069729\n",
      "train loss:0.03902285258478659\n",
      "train loss:0.029234129485003764\n",
      "train loss:0.018208244863815642\n",
      "train loss:0.013713660209790013\n",
      "train loss:0.02296358589441047\n",
      "train loss:0.03424474040739316\n",
      "train loss:0.027970944799651214\n",
      "train loss:0.037186995409859896\n",
      "train loss:0.028839497781428993\n",
      "train loss:0.007418613475979973\n",
      "train loss:0.02998363288099926\n",
      "train loss:0.009959301855630716\n",
      "train loss:0.0024363852263548005\n",
      "train loss:0.005779412014485433\n",
      "train loss:0.022676335114297647\n",
      "train loss:0.014872693378218753\n",
      "train loss:0.014027658037359154\n",
      "train loss:0.017207032610636476\n",
      "train loss:0.004597723565962655\n",
      "train loss:0.046000268411422206\n",
      "train loss:0.1376615219097384\n",
      "train loss:0.07191256360148345\n",
      "train loss:0.02067125778278881\n",
      "train loss:0.06252174859650278\n",
      "train loss:0.025073252929257443\n",
      "train loss:0.006803866086887561\n",
      "train loss:0.021033034623208713\n",
      "train loss:0.0045504676567895315\n",
      "train loss:0.042146207680031376\n",
      "train loss:0.004700985671112625\n",
      "train loss:0.01745790076805944\n",
      "train loss:0.03021730155623019\n",
      "train loss:0.013862907136131333\n",
      "train loss:0.006083186718665526\n",
      "train loss:0.022720821335666285\n",
      "train loss:0.005896938818057419\n",
      "train loss:0.022689130709571237\n",
      "train loss:0.014734508701162946\n",
      "train loss:0.016503523366644755\n",
      "train loss:0.006890498295090197\n",
      "train loss:0.00723054156976929\n",
      "train loss:0.004168791526403341\n",
      "train loss:0.026950419138992078\n",
      "train loss:0.026795565411334384\n",
      "train loss:0.004316195670812103\n",
      "train loss:0.037045020828777114\n",
      "train loss:0.014886343685492708\n",
      "train loss:0.0034409668750980376\n",
      "train loss:0.009278348117222267\n",
      "train loss:0.01298544381315296\n",
      "train loss:0.05611259047773518\n",
      "train loss:0.01624035388738359\n",
      "train loss:0.018012497718374966\n",
      "train loss:0.07197835528004491\n",
      "train loss:0.005470448471721755\n",
      "train loss:0.036958791672622\n",
      "train loss:0.020976063778762967\n",
      "train loss:0.019367803753037087\n",
      "train loss:0.03304730399331038\n",
      "train loss:0.007516613437121259\n",
      "train loss:0.01110563841567918\n",
      "train loss:0.025773886700525366\n",
      "train loss:0.0172135110832101\n",
      "train loss:0.07321899523453526\n",
      "train loss:0.010132680831581904\n",
      "train loss:0.003785498499333786\n",
      "train loss:0.031710141649903816\n",
      "train loss:0.012929591982195995\n",
      "train loss:0.01803106545841396\n",
      "train loss:0.011429138941800794\n",
      "train loss:0.011794030964366662\n",
      "train loss:0.032432485379916096\n",
      "train loss:0.008266579756779269\n",
      "train loss:0.0671907917570836\n",
      "train loss:0.03201325417156822\n",
      "train loss:0.007256418270856637\n",
      "train loss:0.01873908675201318\n",
      "train loss:0.01722427150094749\n",
      "train loss:0.025212591676881713\n",
      "train loss:0.01042260572335044\n",
      "train loss:0.0035348285718365545\n",
      "train loss:0.017213094117332458\n",
      "train loss:0.03777823609107549\n",
      "train loss:0.029527869557270835\n",
      "train loss:0.02787447704318361\n",
      "train loss:0.03455865045911836\n",
      "train loss:0.03328434985897105\n",
      "train loss:0.014869803625146399\n",
      "train loss:0.03923725738994083\n",
      "train loss:0.04217845376336294\n",
      "train loss:0.041763675592519194\n",
      "train loss:0.020902692840377865\n",
      "train loss:0.027819994268561268\n",
      "train loss:0.009270888571452661\n",
      "train loss:0.027844592987696114\n",
      "train loss:0.18504788478527615\n",
      "train loss:0.023081412176052595\n",
      "train loss:0.009679624509125791\n",
      "train loss:0.08063150826922844\n",
      "train loss:0.035012742762050394\n",
      "train loss:0.005921230553514082\n",
      "train loss:0.005192471968887219\n",
      "train loss:0.0038042087302108457\n",
      "train loss:0.010339408911797594\n",
      "train loss:0.018781631396405984\n",
      "train loss:0.08282932929794934\n",
      "train loss:0.013853217687526027\n",
      "train loss:0.0772316616136355\n",
      "train loss:0.018573199224771155\n",
      "train loss:0.007427200375687274\n",
      "train loss:0.005106167479957957\n",
      "train loss:0.013384374413645086\n",
      "train loss:0.009020260226105529\n",
      "train loss:0.09049592779673205\n",
      "train loss:0.01174970847340334\n",
      "train loss:0.027699921622554743\n",
      "train loss:0.008480581663279292\n",
      "train loss:0.030013783947988596\n",
      "train loss:0.010100965522806885\n",
      "train loss:0.006593365192624889\n",
      "train loss:0.02052311780876208\n",
      "train loss:0.0069114660961189164\n",
      "train loss:0.011893769320277707\n",
      "train loss:0.05241004009519577\n",
      "train loss:0.03510897944815422\n",
      "train loss:0.009119902329784473\n",
      "train loss:0.0025023633419449976\n",
      "train loss:0.04904358512534973\n",
      "train loss:0.0020276468921241026\n",
      "train loss:0.030551954338351086\n",
      "=== epoch:7, train acc:0.99, test acc:0.984 ===\n",
      "train loss:0.0176623455189134\n",
      "train loss:0.0038803211148074157\n",
      "train loss:0.0400036378901123\n",
      "train loss:0.011123884271455099\n",
      "train loss:0.0041488132752596244\n",
      "train loss:0.01694091628282068\n",
      "train loss:0.04316734256795383\n",
      "train loss:0.007429402179653001\n",
      "train loss:0.019755817095559222\n",
      "train loss:0.008193082388742416\n",
      "train loss:0.03090144730864537\n",
      "train loss:0.029332715246783443\n",
      "train loss:0.019089803494763267\n",
      "train loss:0.01965660114069421\n",
      "train loss:0.002300234479420769\n",
      "train loss:0.01842179886216135\n",
      "train loss:0.0030281862787112108\n",
      "train loss:0.0151845021260475\n",
      "train loss:0.008475550610012574\n",
      "train loss:0.006682221304241076\n",
      "train loss:0.01150318037971497\n",
      "train loss:0.01824969351466583\n",
      "train loss:0.009990488065573698\n",
      "train loss:0.02049580889634818\n",
      "train loss:0.028070003173881843\n",
      "train loss:0.00969916899119929\n",
      "train loss:0.017183507560400733\n",
      "train loss:0.014964834087143468\n",
      "train loss:0.014143195562277288\n",
      "train loss:0.011853162944691522\n",
      "train loss:0.01576347615255254\n",
      "train loss:0.003827730824301573\n",
      "train loss:0.01345437740552774\n",
      "train loss:0.012512050934568726\n",
      "train loss:0.022776009402471962\n",
      "train loss:0.03142653572734278\n",
      "train loss:0.00853416182766105\n",
      "train loss:0.05129619021256565\n",
      "train loss:0.08527414374224832\n",
      "train loss:0.045088370043457865\n",
      "train loss:0.0051739349911245494\n",
      "train loss:0.009139682396759989\n",
      "train loss:0.06738750001253903\n",
      "train loss:0.008569104235716496\n",
      "train loss:0.011387488176553256\n",
      "train loss:0.006280620157777084\n",
      "train loss:0.004095794859821477\n",
      "train loss:0.009118283549701794\n",
      "train loss:0.012145225878767067\n",
      "train loss:0.08077829364593332\n",
      "train loss:0.006902455886045006\n",
      "train loss:0.04784592662132458\n",
      "train loss:0.00540990833050979\n",
      "train loss:0.006483346342882596\n",
      "train loss:0.005176475646167514\n",
      "train loss:0.03707693813786959\n",
      "train loss:0.01152671023079813\n",
      "train loss:0.004114943146068174\n",
      "train loss:0.014504464296985152\n",
      "train loss:0.0031670469096538867\n",
      "train loss:0.024675537173566346\n",
      "train loss:0.013162632953601825\n",
      "train loss:0.014244245051342393\n",
      "train loss:0.0073676721714083515\n",
      "train loss:0.006167302137513552\n",
      "train loss:0.018601768672133118\n",
      "train loss:0.0031725910119712888\n",
      "train loss:0.011731017303175488\n",
      "train loss:0.03392467493549905\n",
      "train loss:0.006994982002293776\n",
      "train loss:0.01851598949403336\n",
      "train loss:0.03437245238528592\n",
      "train loss:0.03876864855262503\n",
      "train loss:0.026253947921518028\n",
      "train loss:0.006042681862447349\n",
      "train loss:0.014509610662302013\n",
      "train loss:0.021423310099723803\n",
      "train loss:0.02105429461201071\n",
      "train loss:0.01758582803928703\n",
      "train loss:0.020217597830643013\n",
      "train loss:0.031638451677616405\n",
      "train loss:0.0050606099370180254\n",
      "train loss:0.0016251225945591845\n",
      "train loss:0.024990677049218726\n",
      "train loss:0.020633573809206888\n",
      "train loss:0.01562129047959684\n",
      "train loss:0.03877299102782802\n",
      "train loss:0.08524115339372519\n",
      "train loss:0.004321390859315156\n",
      "train loss:0.0071914834233911306\n",
      "train loss:0.031535949914889824\n",
      "train loss:0.020094153269280848\n",
      "train loss:0.007872663122099405\n",
      "train loss:0.004771342064852285\n",
      "train loss:0.06408868288477902\n",
      "train loss:0.04280267031782829\n",
      "train loss:0.004409310972398887\n",
      "train loss:0.010192713079670719\n",
      "train loss:0.06349335697893584\n",
      "train loss:0.008695835368242309\n",
      "train loss:0.004948592648212974\n",
      "train loss:0.00406036461269324\n",
      "train loss:0.00277812263611236\n",
      "train loss:0.008652139170233787\n",
      "train loss:0.015883928402008144\n",
      "train loss:0.06357924701993568\n",
      "train loss:0.018270528920704625\n",
      "train loss:0.0077273515896907966\n",
      "train loss:0.017754350357363002\n",
      "train loss:0.013283233783265734\n",
      "train loss:0.01365792373015117\n",
      "train loss:0.03008020034256947\n",
      "train loss:0.030577731101075957\n",
      "train loss:0.023715325775141925\n",
      "train loss:0.00937556466344238\n",
      "train loss:0.006655634874248671\n",
      "train loss:0.01361635454433379\n",
      "train loss:0.04635515152827521\n",
      "train loss:0.014188948974569697\n",
      "train loss:0.004947959937525945\n",
      "train loss:0.011985334273633219\n",
      "train loss:0.00513592726494008\n",
      "train loss:0.005724030948112521\n",
      "train loss:0.010917407161247122\n",
      "train loss:0.006090547895495976\n",
      "train loss:0.012593071160349018\n",
      "train loss:0.012090463430070425\n",
      "train loss:0.018094185171105295\n",
      "train loss:0.016177215319743147\n",
      "train loss:0.02851418895337995\n",
      "train loss:0.016512645456156157\n",
      "train loss:0.018944280581169523\n",
      "train loss:0.014111856651340484\n",
      "train loss:0.021862798590309773\n",
      "train loss:0.02155733880505076\n",
      "train loss:0.01348708868758378\n",
      "train loss:0.033722901699145914\n",
      "train loss:0.0032633718076468875\n",
      "train loss:0.006671221091899771\n",
      "train loss:0.003813929718929164\n",
      "train loss:0.013918072107071989\n",
      "train loss:0.005029851638150357\n",
      "train loss:0.037756850919125826\n",
      "train loss:0.022460520958988622\n",
      "train loss:0.006391230511285475\n",
      "train loss:0.005516177560595347\n",
      "train loss:0.004611624777028183\n",
      "train loss:0.01020147286327946\n",
      "train loss:0.005187404752532016\n",
      "train loss:0.011353224612889783\n",
      "train loss:0.030250475644217192\n",
      "train loss:0.07194424540217462\n",
      "train loss:0.00799474051473761\n",
      "train loss:0.043820536715676664\n",
      "train loss:0.0021939578436815915\n",
      "train loss:0.024244599508920092\n",
      "train loss:0.018453079125500448\n",
      "train loss:0.027988792011152005\n",
      "train loss:0.006300276332524494\n",
      "train loss:0.01894553628634022\n",
      "train loss:0.04969636647075506\n",
      "train loss:0.010809176586987365\n",
      "train loss:0.012722907112235886\n",
      "train loss:0.006608662374344336\n",
      "train loss:0.044898278993615336\n",
      "train loss:0.010080401595662716\n",
      "train loss:0.01550221151299067\n",
      "train loss:0.02389689975987456\n",
      "train loss:0.013072489812549297\n",
      "train loss:0.009881636665548515\n",
      "train loss:0.010854605283559892\n",
      "train loss:0.008466924955388054\n",
      "train loss:0.009881464479324984\n",
      "train loss:0.017557627074277483\n",
      "train loss:0.01522194689697299\n",
      "train loss:0.0033226632599132866\n",
      "train loss:0.02171399010490209\n",
      "train loss:0.011886007741811361\n",
      "train loss:0.008221420366699811\n",
      "train loss:0.09397282654660229\n",
      "train loss:0.014937259444189624\n",
      "train loss:0.005825240544069804\n",
      "train loss:0.015362820435472475\n",
      "train loss:0.031187626779985517\n",
      "train loss:0.029397771386861456\n",
      "train loss:0.01134848620280021\n",
      "train loss:0.003248249714573439\n",
      "train loss:0.030093988148450088\n",
      "train loss:0.03197656318511535\n",
      "train loss:0.019092301891998394\n",
      "train loss:0.08618150104307602\n",
      "train loss:0.06351686518861971\n",
      "train loss:0.016178764245246557\n",
      "train loss:0.0066774182809876135\n",
      "train loss:0.015295367766051502\n",
      "train loss:0.030517854682185902\n",
      "train loss:0.015064210137509932\n",
      "train loss:0.02303314141774291\n",
      "train loss:0.007110169538441128\n",
      "train loss:0.03153399922158499\n",
      "train loss:0.016231128018209424\n",
      "train loss:0.029679283474613637\n",
      "train loss:0.007196301311389402\n",
      "train loss:0.010240139128689495\n",
      "train loss:0.004565342317836337\n",
      "train loss:0.005043130880569996\n",
      "train loss:0.034903328879112426\n",
      "train loss:0.005739996755098794\n",
      "train loss:0.049180932704659855\n",
      "train loss:0.014552255553543714\n",
      "train loss:0.0018302384273334465\n",
      "train loss:0.014554835434317969\n",
      "train loss:0.01434933249151407\n",
      "train loss:0.009340345206717992\n",
      "train loss:0.014732028873296759\n",
      "train loss:0.022671515615716183\n",
      "train loss:0.017148922273094627\n",
      "train loss:0.005449763594524686\n",
      "train loss:0.01192683936822668\n",
      "train loss:0.029013955868630547\n",
      "train loss:0.01765353061579095\n",
      "train loss:0.004325233804544634\n",
      "train loss:0.006450016726571029\n",
      "train loss:0.015386000759960551\n",
      "train loss:0.020767085651886785\n",
      "train loss:0.017639351295388746\n",
      "train loss:0.007371818493322042\n",
      "train loss:0.06468762539562642\n",
      "train loss:0.09965190095546556\n",
      "train loss:0.0067321018786911406\n",
      "train loss:0.018675151371430106\n",
      "train loss:0.008184441611884374\n",
      "train loss:0.002436472574812139\n",
      "train loss:0.047258643808019335\n",
      "train loss:0.011037906654334512\n",
      "train loss:0.054988847392733\n",
      "train loss:0.03566119117246404\n",
      "train loss:0.02191352700585731\n",
      "train loss:0.005234272083419203\n",
      "train loss:0.008731517217737067\n",
      "train loss:0.010041652436948108\n",
      "train loss:0.005790506861871699\n",
      "train loss:0.03387524347191579\n",
      "train loss:0.04588894795347875\n",
      "train loss:0.013670872360279829\n",
      "train loss:0.05500461455285871\n",
      "train loss:0.011312141107823373\n",
      "train loss:0.006720945882242209\n",
      "train loss:0.004110099604858631\n",
      "train loss:0.028492622822254062\n",
      "train loss:0.04251502662577286\n",
      "train loss:0.007712489763639106\n",
      "train loss:0.027263966518264352\n",
      "train loss:0.02902659819016037\n",
      "train loss:0.04151630668953236\n",
      "train loss:0.010326807297782124\n",
      "train loss:0.10677366888438973\n",
      "train loss:0.010937882416621629\n",
      "train loss:0.01001304713300819\n",
      "train loss:0.03719903620457485\n",
      "train loss:0.00796044014431036\n",
      "train loss:0.010256662606542401\n",
      "train loss:0.005179070219651819\n",
      "train loss:0.00785702326724323\n",
      "train loss:0.004186094846930262\n",
      "train loss:0.026329707114289128\n",
      "train loss:0.013915459413824343\n",
      "train loss:0.017892256775334067\n",
      "train loss:0.009297751270137945\n",
      "train loss:0.010305675290376466\n",
      "train loss:0.030585712551765773\n",
      "train loss:0.008105520341827753\n",
      "train loss:0.004280263783063142\n",
      "train loss:0.010048030158408206\n",
      "train loss:0.012834854230716753\n",
      "train loss:0.007521859779637219\n",
      "train loss:0.013340335546019927\n",
      "train loss:0.013445871443848272\n",
      "train loss:0.020781585179006346\n",
      "train loss:0.024067268075322664\n",
      "train loss:0.0025367925820775237\n",
      "train loss:0.005092983656797605\n",
      "train loss:0.020794754604579767\n",
      "train loss:0.007385500411595956\n",
      "train loss:0.004665307059623181\n",
      "train loss:0.001879060246950784\n",
      "train loss:0.006611418907304285\n",
      "train loss:0.006462925838563538\n",
      "train loss:0.004528644656963326\n",
      "train loss:0.040219857204433565\n",
      "train loss:0.01041771465431349\n",
      "train loss:0.017541281208219846\n",
      "train loss:0.015582709013515012\n",
      "train loss:0.0050159672744836785\n",
      "train loss:0.06877022788873076\n",
      "train loss:0.005237889024915713\n",
      "train loss:0.002650062384278536\n",
      "train loss:0.17805291254294958\n",
      "train loss:0.021584168636631245\n",
      "train loss:0.01839525388825643\n",
      "train loss:0.01812717371642454\n",
      "train loss:0.08223927259943556\n",
      "train loss:0.027007980127893395\n",
      "train loss:0.009405677160943024\n",
      "train loss:0.01867891681540706\n",
      "train loss:0.06208053487666451\n",
      "train loss:0.006608529846814681\n",
      "train loss:0.02242372477423923\n",
      "train loss:0.014638905936524728\n",
      "train loss:0.00412993301689461\n",
      "train loss:0.035512427909674925\n",
      "train loss:0.007669764976620729\n",
      "train loss:0.017509541265132135\n",
      "train loss:0.01025351822274755\n",
      "train loss:0.014222971966383533\n",
      "train loss:0.006385767975434164\n",
      "train loss:0.005734641612693814\n",
      "train loss:0.02151506203574652\n",
      "train loss:0.0025852622277151697\n",
      "train loss:0.008501103226352148\n",
      "train loss:0.03676080733507122\n",
      "train loss:0.021512321219259927\n",
      "train loss:0.07129325697327171\n",
      "train loss:0.02474610036321211\n",
      "train loss:0.011230792227577318\n",
      "train loss:0.028328018631923687\n",
      "train loss:0.007747326211463711\n",
      "train loss:0.007574568431626552\n",
      "train loss:0.004947494071488985\n",
      "train loss:0.021435606505451076\n",
      "train loss:0.05088612840289275\n",
      "train loss:0.0058438249424359765\n",
      "train loss:0.006441315168033959\n",
      "train loss:0.04392531114285684\n",
      "train loss:0.0043051296640951005\n",
      "train loss:0.003706349959816092\n",
      "train loss:0.00418920199869343\n",
      "train loss:0.024719783979357268\n",
      "train loss:0.015281905687756763\n",
      "train loss:0.008135033441580057\n",
      "train loss:0.004809334830269049\n",
      "train loss:0.0033274784190834733\n",
      "train loss:0.015391545598376301\n",
      "train loss:0.005406620228211534\n",
      "train loss:0.10288676604793084\n",
      "train loss:0.022701590718510977\n",
      "train loss:0.004864382545141069\n",
      "train loss:0.01704208606453614\n",
      "train loss:0.01385392352179374\n",
      "train loss:0.01486080236010192\n",
      "train loss:0.0037778435230907616\n",
      "train loss:0.019876741163596504\n",
      "train loss:0.027956978343343163\n",
      "train loss:0.022388212778458306\n",
      "train loss:0.0027657037195597946\n",
      "train loss:0.01672153574610515\n",
      "train loss:0.045617195640379575\n",
      "train loss:0.019264369538651106\n",
      "train loss:0.009930093985539496\n",
      "train loss:0.01440382828958404\n",
      "train loss:0.01402116028724323\n",
      "train loss:0.0070056956478577095\n",
      "train loss:0.01990275572332492\n",
      "train loss:0.013597181340876386\n",
      "train loss:0.0031737759808656723\n",
      "train loss:0.01018493702833319\n",
      "train loss:0.01578808076007036\n",
      "train loss:0.016455653841033088\n",
      "train loss:0.004892613659713672\n",
      "train loss:0.02074057216541866\n",
      "train loss:0.0037726262963578864\n",
      "train loss:0.022969614003988732\n",
      "train loss:0.017650591779534475\n",
      "train loss:0.008527860661172375\n",
      "train loss:0.024629150418053584\n",
      "train loss:0.010473724458923606\n",
      "train loss:0.028076320331784997\n",
      "train loss:0.0054578035253986116\n",
      "train loss:0.008054622428033587\n",
      "train loss:0.0041285107666889686\n",
      "train loss:0.002577297176169881\n",
      "train loss:0.012035465867519437\n",
      "train loss:0.01726773018726038\n",
      "train loss:0.015367150936748333\n",
      "train loss:0.038119777772861754\n",
      "train loss:0.02366649070956471\n",
      "train loss:0.020507152330594133\n",
      "train loss:0.006819446790414977\n",
      "train loss:0.003719715297936434\n",
      "train loss:0.001174612308244931\n",
      "train loss:0.02627121299197548\n",
      "train loss:0.007968877719855686\n",
      "train loss:0.030046330043386558\n",
      "train loss:0.027947265550228015\n",
      "train loss:0.007218105649725065\n",
      "train loss:0.02427522499716162\n",
      "train loss:0.007686538865158053\n",
      "train loss:0.018605667375242614\n",
      "train loss:0.03474248850156902\n",
      "train loss:0.007416021683832485\n",
      "train loss:0.0065177508541937835\n",
      "train loss:0.014703725068376097\n",
      "train loss:0.00916709383182816\n",
      "train loss:0.0032916272783320515\n",
      "train loss:0.007706988186695039\n",
      "train loss:0.050874007762637445\n",
      "train loss:0.025186087390292173\n",
      "train loss:0.012336241087510448\n",
      "train loss:0.00553921439250276\n",
      "train loss:0.005032672028874532\n",
      "train loss:0.04272195630964361\n",
      "train loss:0.004804651597603974\n",
      "train loss:0.0014414831992253152\n",
      "train loss:0.013293620938809203\n",
      "train loss:0.0188641327097366\n",
      "train loss:0.022931187135934793\n",
      "train loss:0.003932801339462685\n",
      "train loss:0.019032877418111126\n",
      "train loss:0.04447350454149757\n",
      "train loss:0.01475192648726834\n",
      "train loss:0.01990671113613772\n",
      "train loss:0.009766630301623311\n",
      "train loss:0.00361321477414001\n",
      "train loss:0.012593930700362339\n",
      "train loss:0.0009844894944478855\n",
      "train loss:0.02205521677154727\n",
      "train loss:0.013581636448436327\n",
      "train loss:0.04933110525691706\n",
      "train loss:0.010577776264441516\n",
      "train loss:0.019090847139333414\n",
      "train loss:0.023677064244285603\n",
      "train loss:0.018460940005550076\n",
      "train loss:0.008132519853667615\n",
      "train loss:0.01134970061473586\n",
      "train loss:0.01257696736610284\n",
      "train loss:0.00607891071193502\n",
      "train loss:0.015379343008347597\n",
      "train loss:0.0139955578304378\n",
      "train loss:0.029651276661107936\n",
      "train loss:0.010431868080500648\n",
      "train loss:0.012306177414508603\n",
      "train loss:0.06268694130361444\n",
      "train loss:0.002775105144391654\n",
      "train loss:0.016211116991153546\n",
      "train loss:0.0028708333416704985\n",
      "train loss:0.004611032049890445\n",
      "train loss:0.009416163798513243\n",
      "train loss:0.020956925258957305\n",
      "train loss:0.0059870992478834465\n",
      "train loss:0.018700484823791556\n",
      "train loss:0.04173129831729568\n",
      "train loss:0.0033495432865085443\n",
      "train loss:0.0116408073831317\n",
      "train loss:0.007508979026822198\n",
      "train loss:0.09793757161859037\n",
      "train loss:0.042218897080020204\n",
      "train loss:0.004642428657203444\n",
      "train loss:0.04430587864779345\n",
      "train loss:0.05121396975382073\n",
      "train loss:0.02007017092176564\n",
      "train loss:0.002789953899910315\n",
      "train loss:0.01177153493952715\n",
      "train loss:0.005312427953631262\n",
      "train loss:0.0016693322631925966\n",
      "train loss:0.0028648695453265542\n",
      "train loss:0.011366798355042246\n",
      "train loss:0.004736197614080003\n",
      "train loss:0.051126606693838124\n",
      "train loss:0.002755699150089973\n",
      "train loss:0.018968510814537744\n",
      "train loss:0.18535638969786078\n",
      "train loss:0.0393944902671105\n",
      "train loss:0.03195029398720827\n",
      "train loss:0.013600694387428003\n",
      "train loss:0.014658844314920993\n",
      "train loss:0.010004836529918333\n",
      "train loss:0.01485386152924771\n",
      "train loss:0.0031774821163972804\n",
      "train loss:0.0034867126414636195\n",
      "train loss:0.016873610940788056\n",
      "train loss:0.006026401168209744\n",
      "train loss:0.029677638236044822\n",
      "train loss:0.00882678957132973\n",
      "train loss:0.009292777957329027\n",
      "train loss:0.02628877110074358\n",
      "train loss:0.06916544420787164\n",
      "train loss:0.0673696662181864\n",
      "train loss:0.01055178682659368\n",
      "train loss:0.011065533893402424\n",
      "train loss:0.0028842988215260794\n",
      "train loss:0.010017408508940775\n",
      "train loss:0.01494150681489801\n",
      "train loss:0.014843736960354176\n",
      "train loss:0.008873286142558607\n",
      "train loss:0.00882822429305083\n",
      "train loss:0.04426208415745724\n",
      "train loss:0.025648633301176216\n",
      "train loss:0.005061882871436095\n",
      "train loss:0.0013490560417661326\n",
      "train loss:0.01832627673832312\n",
      "train loss:0.006270940576874553\n",
      "train loss:0.002759115554213025\n",
      "train loss:0.03014929434122663\n",
      "train loss:0.031401608319429675\n",
      "train loss:0.013367425526281356\n",
      "train loss:0.01707264065584748\n",
      "train loss:0.005912970308664499\n",
      "train loss:0.04866163511920381\n",
      "train loss:0.011740253669653975\n",
      "train loss:0.014233293782654392\n",
      "train loss:0.08371021991584093\n",
      "train loss:0.040431049957867525\n",
      "train loss:0.0020149279621218957\n",
      "train loss:0.0035314983288249648\n",
      "train loss:0.008339819041997184\n",
      "train loss:0.009281946940008402\n",
      "train loss:0.011770021693617854\n",
      "train loss:0.018734805199499934\n",
      "train loss:0.024446469591959518\n",
      "train loss:0.0332496608252548\n",
      "train loss:0.01794846720197019\n",
      "train loss:0.041892253664393364\n",
      "train loss:0.014671142498473401\n",
      "train loss:0.007026391463406608\n",
      "train loss:0.00898958487463656\n",
      "train loss:0.008156846851621604\n",
      "train loss:0.039542870543449064\n",
      "train loss:0.06595180866441898\n",
      "train loss:0.020007207833381262\n",
      "train loss:0.033984168136035595\n",
      "train loss:0.03152729007869326\n",
      "train loss:0.00912207946204378\n",
      "train loss:0.08362224863101472\n",
      "train loss:0.036367236826638\n",
      "train loss:0.021706741619359638\n",
      "train loss:0.010397596194116523\n",
      "train loss:0.014499926303988662\n",
      "train loss:0.00907731346553707\n",
      "train loss:0.037011098922782135\n",
      "train loss:0.013237113511928505\n",
      "train loss:0.0051130060781427004\n",
      "train loss:0.01413927390790225\n",
      "train loss:0.07778628817966779\n",
      "train loss:0.02070649112078856\n",
      "train loss:0.031592773320104975\n",
      "train loss:0.02910126756185272\n",
      "train loss:0.03146157598415123\n",
      "train loss:0.010993466955548094\n",
      "train loss:0.008332901060145677\n",
      "train loss:0.0056177472774429586\n",
      "train loss:0.006391295790366663\n",
      "train loss:0.014460968644054367\n",
      "train loss:0.01424518564084726\n",
      "train loss:0.022812030466732747\n",
      "train loss:0.030616349127705854\n",
      "train loss:0.06123246654282836\n",
      "train loss:0.01554914615890829\n",
      "train loss:0.007862661423091097\n",
      "train loss:0.012200874993430817\n",
      "train loss:0.01423128490585519\n",
      "train loss:0.029361680620716885\n",
      "train loss:0.014500018297046977\n",
      "train loss:0.03212730635106193\n",
      "train loss:0.007370005347021755\n",
      "train loss:0.012744014258645494\n",
      "train loss:0.013645824024544373\n",
      "train loss:0.013493848868842324\n",
      "train loss:0.006224337993758744\n",
      "train loss:0.005977892133079938\n",
      "train loss:0.01568123212427276\n",
      "train loss:0.03291158191649224\n",
      "train loss:0.024666712031307533\n",
      "train loss:0.02914053080639316\n",
      "train loss:0.06542738155935934\n",
      "train loss:0.005676460420579637\n",
      "train loss:0.009188287934068583\n",
      "train loss:0.002689957622465271\n",
      "train loss:0.0025306146332258576\n",
      "train loss:0.0452584216383914\n",
      "train loss:0.0062000842087157745\n",
      "train loss:0.009489766273182034\n",
      "train loss:0.014972310425097429\n",
      "train loss:0.01077497278800825\n",
      "train loss:0.01799634782557209\n",
      "train loss:0.001602792495582416\n",
      "train loss:0.0017708933036473906\n",
      "train loss:0.023102491810739645\n",
      "train loss:0.024249703810241514\n",
      "train loss:0.0067798561300385154\n",
      "train loss:0.016377702524498067\n",
      "train loss:0.006861365504301237\n",
      "train loss:0.023103378062470616\n",
      "train loss:0.014894726528564663\n",
      "train loss:0.04122992688002187\n",
      "train loss:0.0070805184695342215\n",
      "train loss:0.007893142729164918\n",
      "train loss:0.02244965473233445\n",
      "train loss:0.005695831560351005\n",
      "train loss:0.020915472233311548\n",
      "train loss:0.028150813027414862\n",
      "=== epoch:8, train acc:0.992, test acc:0.985 ===\n",
      "train loss:0.009104105928178407\n",
      "train loss:0.008168246094049402\n",
      "train loss:0.032141803142149775\n",
      "train loss:0.02529143078297274\n",
      "train loss:0.004127525417586919\n",
      "train loss:0.016106390039741778\n",
      "train loss:0.03304850921026628\n",
      "train loss:0.010748649970735052\n",
      "train loss:0.011935507899403137\n",
      "train loss:0.01522415682478028\n",
      "train loss:0.015680097200889073\n",
      "train loss:0.00467518597027214\n",
      "train loss:0.030934782440258123\n",
      "train loss:0.036504573107240874\n",
      "train loss:0.008578274635875049\n",
      "train loss:0.025880475537260996\n",
      "train loss:0.04569321586613064\n",
      "train loss:0.03181286384948734\n",
      "train loss:0.006042258337209294\n",
      "train loss:0.04031691015807996\n",
      "train loss:0.06129283010486887\n",
      "train loss:0.007639691723099449\n",
      "train loss:0.031176034478945448\n",
      "train loss:0.003248816405618314\n",
      "train loss:0.020273374687749382\n",
      "train loss:0.02289147648279304\n",
      "train loss:0.008972475779724327\n",
      "train loss:0.050049073092414915\n",
      "train loss:0.03390885045122902\n",
      "train loss:0.004728352579518407\n",
      "train loss:0.04797357189672723\n",
      "train loss:0.011236354564967197\n",
      "train loss:0.01425905378567446\n",
      "train loss:0.008808741180325687\n",
      "train loss:0.01093909217457833\n",
      "train loss:0.06140626997304392\n",
      "train loss:0.06486101013547935\n",
      "train loss:0.03293347728752525\n",
      "train loss:0.007054666631201725\n",
      "train loss:0.01751429146489204\n",
      "train loss:0.011725844557351634\n",
      "train loss:0.0428123624179273\n",
      "train loss:0.013761292544934987\n",
      "train loss:0.02352034247712314\n",
      "train loss:0.01849278262843702\n",
      "train loss:0.027756419495178\n",
      "train loss:0.0041827172195553226\n",
      "train loss:0.007243677035272452\n",
      "train loss:0.004955547972827245\n",
      "train loss:0.0044478517497695495\n",
      "train loss:0.015448699821227024\n",
      "train loss:0.03992891575133257\n",
      "train loss:0.023604620273698182\n",
      "train loss:0.005385103675520008\n",
      "train loss:0.009729151923273541\n",
      "train loss:0.007273918682725915\n",
      "train loss:0.03869391302458592\n",
      "train loss:0.004885013692398553\n",
      "train loss:0.002575669094032769\n",
      "train loss:0.003246599459246921\n",
      "train loss:0.0065751454735704954\n",
      "train loss:0.03200295261570862\n",
      "train loss:0.018030108901021472\n",
      "train loss:0.0069461027032070655\n",
      "train loss:0.004372602557154799\n",
      "train loss:0.013614799145021957\n",
      "train loss:0.009212601321053776\n",
      "train loss:0.013162070885601769\n",
      "train loss:0.0021257629896351887\n",
      "train loss:0.06914236837099431\n",
      "train loss:0.004241849801844233\n",
      "train loss:0.05937724947694118\n",
      "train loss:0.0074172527969999305\n",
      "train loss:0.010638529088745758\n",
      "train loss:0.019664797023988052\n",
      "train loss:0.05684708747060222\n",
      "train loss:0.01578623675306781\n",
      "train loss:0.018356923606460962\n",
      "train loss:0.00711250799456749\n",
      "train loss:0.01534644749640961\n",
      "train loss:0.004768634637061735\n",
      "train loss:0.08873031983048346\n",
      "train loss:0.00957595710367787\n",
      "train loss:0.009632527044121568\n",
      "train loss:0.005721719807410092\n",
      "train loss:0.040593740834212175\n",
      "train loss:0.016825359096377807\n",
      "train loss:0.024565003219418964\n",
      "train loss:0.030941996067023362\n",
      "train loss:0.038190332784036746\n",
      "train loss:0.024219093599654712\n",
      "train loss:0.010981785980213765\n",
      "train loss:0.019726497877599492\n",
      "train loss:0.005412050026559078\n",
      "train loss:0.012244172692255507\n",
      "train loss:0.009691549631479383\n",
      "train loss:0.005028279430043255\n",
      "train loss:0.038636835709686304\n",
      "train loss:0.011022477464502285\n",
      "train loss:0.017361555470065484\n",
      "train loss:0.01521157212964319\n",
      "train loss:0.006975892152538907\n",
      "train loss:0.015848086791627282\n",
      "train loss:0.0167918649188224\n",
      "train loss:0.0066596227972853675\n",
      "train loss:0.010808836883844465\n",
      "train loss:0.008188215218032846\n",
      "train loss:0.01739114243137441\n",
      "train loss:0.007977235687259433\n",
      "train loss:0.01686336063855186\n",
      "train loss:0.010956927717458731\n",
      "train loss:0.0035805098741908755\n",
      "train loss:0.05015426871130546\n",
      "train loss:0.018399324129418627\n",
      "train loss:0.007020329727529401\n",
      "train loss:0.0024092418180596247\n",
      "train loss:0.01752425847573363\n",
      "train loss:0.012537348721982138\n",
      "train loss:0.007031916065686244\n",
      "train loss:0.004625761624223601\n",
      "train loss:0.02797860294253052\n",
      "train loss:0.02329661755656622\n",
      "train loss:0.00384502565008609\n",
      "train loss:0.008779161921855053\n",
      "train loss:0.006343349065976647\n",
      "train loss:0.008559743719125108\n",
      "train loss:0.004627813786949394\n",
      "train loss:0.007554599965181509\n",
      "train loss:0.051348167358929525\n",
      "train loss:0.025538675356848332\n",
      "train loss:0.021053246807967736\n",
      "train loss:0.008807769314907085\n",
      "train loss:0.0192197860343227\n",
      "train loss:0.022400841436108366\n",
      "train loss:0.021029947930296743\n",
      "train loss:0.011083193964968046\n",
      "train loss:0.011077557945854107\n",
      "train loss:0.0018171308182571705\n",
      "train loss:0.013453234447539238\n",
      "train loss:0.005359892064401096\n",
      "train loss:0.015957403810938676\n",
      "train loss:0.006062837312984951\n",
      "train loss:0.009835081315741508\n",
      "train loss:0.025915589609693375\n",
      "train loss:0.006822986448062567\n",
      "train loss:0.00760209406379017\n",
      "train loss:0.008224866343142585\n",
      "train loss:0.004971092339283395\n",
      "train loss:0.006089343520013422\n",
      "train loss:0.019884857378726643\n",
      "train loss:0.003905186391536185\n",
      "train loss:0.017486703884021366\n",
      "train loss:0.007305432894454148\n",
      "train loss:0.014632646661584299\n",
      "train loss:0.005913202726672572\n",
      "train loss:0.00831688154514235\n",
      "train loss:0.05695590219643023\n",
      "train loss:0.004547184786473912\n",
      "train loss:0.010399835014337505\n",
      "train loss:0.009858792102642951\n",
      "train loss:0.005355597761801151\n",
      "train loss:0.009230090707791737\n",
      "train loss:0.009905508343264008\n",
      "train loss:0.0008210688431586856\n",
      "train loss:0.13866905527046788\n",
      "train loss:0.004192185388890422\n",
      "train loss:0.0075874635599292315\n",
      "train loss:0.022064525664522473\n",
      "train loss:0.007966421475481333\n",
      "train loss:0.09753316716561208\n",
      "train loss:0.012052600109934634\n",
      "train loss:0.027735182361956223\n",
      "train loss:0.004586106978787854\n",
      "train loss:0.00899586545061935\n",
      "train loss:0.026892097540847343\n",
      "train loss:0.003375045972835535\n",
      "train loss:0.019822649454373086\n",
      "train loss:0.010748532497042296\n",
      "train loss:0.021396682419200626\n",
      "train loss:0.030097039303581975\n",
      "train loss:0.014680729679382942\n",
      "train loss:0.017500406692042537\n",
      "train loss:0.0037893693440585936\n",
      "train loss:0.006617186152506212\n",
      "train loss:0.008089873154605428\n",
      "train loss:0.01429625593780396\n",
      "train loss:0.027680261068217558\n",
      "train loss:0.014510626591179947\n",
      "train loss:0.0074011811513033775\n",
      "train loss:0.006084487103954797\n",
      "train loss:0.003879558310635108\n",
      "train loss:0.01175998246260394\n",
      "train loss:0.007525225245165076\n",
      "train loss:0.009992893127958022\n",
      "train loss:0.008823752580437015\n",
      "train loss:0.014900489511216648\n",
      "train loss:0.032267346962677274\n",
      "train loss:0.0025014310233330422\n",
      "train loss:0.030268784745953233\n",
      "train loss:0.03836799627846797\n",
      "train loss:0.010663221480206644\n",
      "train loss:0.004430192540841451\n",
      "train loss:0.008292019286779771\n",
      "train loss:0.006194115724197149\n",
      "train loss:0.03121768543916019\n",
      "train loss:0.009413153274363132\n",
      "train loss:0.007538483343013953\n",
      "train loss:0.015189888115010947\n",
      "train loss:0.004687776539239956\n",
      "train loss:0.021954281072306854\n",
      "train loss:0.028991695051496368\n",
      "train loss:0.0540158130143925\n",
      "train loss:0.010823261367556257\n",
      "train loss:0.017376550621751377\n",
      "train loss:0.0018246311563205678\n",
      "train loss:0.010527201981891872\n",
      "train loss:0.0033088529722837333\n",
      "train loss:0.006692321935467514\n",
      "train loss:0.04229823624862867\n",
      "train loss:0.008515052489979252\n",
      "train loss:0.006145202506932615\n",
      "train loss:0.033334324920278595\n",
      "train loss:0.0029730281401647183\n",
      "train loss:0.006863770195802101\n",
      "train loss:0.003898191805622293\n",
      "train loss:0.003307057773391641\n",
      "train loss:0.013485440384121668\n",
      "train loss:0.007669619883177545\n",
      "train loss:0.060442417709168465\n",
      "train loss:0.007907908259687407\n",
      "train loss:0.004199520724580651\n",
      "train loss:0.06001505978209034\n",
      "train loss:0.0027684091346999854\n",
      "train loss:0.04957015275987014\n",
      "train loss:0.007443551644698696\n",
      "train loss:0.00889755545169151\n",
      "train loss:0.011986344140998712\n",
      "train loss:0.0043770061062612\n",
      "train loss:0.008087675475419056\n",
      "train loss:0.004345453514273934\n",
      "train loss:0.004358370616825021\n",
      "train loss:0.033784212668398816\n",
      "train loss:0.021954349727251618\n",
      "train loss:0.03143635849607266\n",
      "train loss:0.018317822519878343\n",
      "train loss:0.029444325389657914\n",
      "train loss:0.002333028199741572\n",
      "train loss:0.004545182970980948\n",
      "train loss:0.018107193166744407\n",
      "train loss:0.012827009808796322\n",
      "train loss:0.02086246270242147\n",
      "train loss:0.004028590521975804\n",
      "train loss:0.04942547048302319\n",
      "train loss:0.0053027091415280714\n",
      "train loss:0.0055173966883710936\n",
      "train loss:0.0023073634214760604\n",
      "train loss:0.008132517244980965\n",
      "train loss:0.0036745524554568105\n",
      "train loss:0.010449090749504668\n",
      "train loss:0.009565601899582714\n",
      "train loss:0.007881353091532305\n",
      "train loss:0.01856109987631009\n",
      "train loss:0.023019779881125843\n",
      "train loss:0.015609301921222604\n",
      "train loss:0.0813153226589483\n",
      "train loss:0.03114480265404329\n",
      "train loss:0.010069433915213456\n",
      "train loss:0.005252580247591763\n",
      "train loss:0.004944008683004126\n",
      "train loss:0.015376789762370406\n",
      "train loss:0.003785853372283121\n",
      "train loss:0.0021530040848343583\n",
      "train loss:0.017459589842124937\n",
      "train loss:0.0058376936925126285\n",
      "train loss:0.010880590313548037\n",
      "train loss:0.008949856144533908\n",
      "train loss:0.006913126777450269\n",
      "train loss:0.014734404423464117\n",
      "train loss:0.09481436279748494\n",
      "train loss:0.013804249107211931\n",
      "train loss:0.016976573069953177\n",
      "train loss:0.020110841626636962\n",
      "train loss:0.005283169458460298\n",
      "train loss:0.028356344079608825\n",
      "train loss:0.007630678538210648\n",
      "train loss:0.004338959108207624\n",
      "train loss:0.00836189226954307\n",
      "train loss:0.014029539638800478\n",
      "train loss:0.0030865209735945055\n",
      "train loss:0.01032973989489175\n",
      "train loss:0.004767990422616081\n",
      "train loss:0.011217650656102543\n",
      "train loss:0.007528272066156445\n",
      "train loss:0.014760885372676509\n",
      "train loss:0.010375002286115653\n",
      "train loss:0.00461549924479496\n",
      "train loss:0.035929069829573515\n",
      "train loss:0.011679073977735686\n",
      "train loss:0.004591903414958142\n",
      "train loss:0.015559018300341091\n",
      "train loss:0.006318895809827355\n",
      "train loss:0.03936968680333129\n",
      "train loss:0.00808084521181174\n",
      "train loss:0.05967334636785557\n",
      "train loss:0.00538231816590736\n",
      "train loss:0.019301956090350285\n",
      "train loss:0.017716176783098666\n",
      "train loss:0.004098864821477194\n",
      "train loss:0.007521438007230788\n",
      "train loss:0.0123326358822754\n",
      "train loss:0.0035759440559547055\n",
      "train loss:0.004133425998366478\n",
      "train loss:0.00674534630644991\n",
      "train loss:0.004920799790791575\n",
      "train loss:0.005257484886401891\n",
      "train loss:0.008043352782504154\n",
      "train loss:0.012747256523561004\n",
      "train loss:0.008672718763960498\n",
      "train loss:0.0014390024710435468\n",
      "train loss:0.021064039663748618\n",
      "train loss:0.00932472131998222\n",
      "train loss:0.007013195709311818\n",
      "train loss:0.007290871675636792\n",
      "train loss:0.020052445282742352\n",
      "train loss:0.0025904954953381615\n",
      "train loss:0.011281768521067654\n",
      "train loss:0.007271178570076897\n",
      "train loss:0.0037466760880248483\n",
      "train loss:0.024663276717109356\n",
      "train loss:0.015443665046656039\n",
      "train loss:0.014422350846997974\n",
      "train loss:0.0020974770349154377\n",
      "train loss:0.006406940030935634\n",
      "train loss:0.002377285347007199\n",
      "train loss:0.037025678793812286\n",
      "train loss:0.001317780182367436\n",
      "train loss:0.0035755110760919244\n",
      "train loss:0.003945738611241252\n",
      "train loss:0.0032380515712399794\n",
      "train loss:0.0038131490694183097\n",
      "train loss:0.0027165154505573476\n",
      "train loss:0.01020406481411437\n",
      "train loss:0.02100831702521682\n",
      "train loss:0.005597075792204649\n",
      "train loss:0.0028767799962282105\n",
      "train loss:0.013915040028285816\n",
      "train loss:0.005504436580552195\n",
      "train loss:0.012153065790749257\n",
      "train loss:0.005721840146823651\n",
      "train loss:0.009520004286308332\n",
      "train loss:0.006467653325779388\n",
      "train loss:0.002094174795651483\n",
      "train loss:0.018532334170266743\n",
      "train loss:0.04190763602733503\n",
      "train loss:0.006886972019595579\n",
      "train loss:0.00941157749288103\n",
      "train loss:0.0020316471896481363\n",
      "train loss:0.0024109751002108926\n",
      "train loss:0.01110763450396403\n",
      "train loss:0.002570583957956006\n",
      "train loss:0.006201030342043235\n",
      "train loss:0.007571489025562635\n",
      "train loss:0.006046759536843981\n",
      "train loss:0.0043359248706699325\n",
      "train loss:0.0013491961801232368\n",
      "train loss:0.0046019767304394926\n",
      "train loss:0.006316851096130688\n",
      "train loss:0.02548253618861438\n",
      "train loss:0.004785877197749152\n",
      "train loss:0.004770570240037908\n",
      "train loss:0.020975722793797797\n",
      "train loss:0.014315902492318431\n",
      "train loss:0.0046192533014516556\n",
      "train loss:0.024188960194906586\n",
      "train loss:0.007092642153705725\n",
      "train loss:0.004320571632694734\n",
      "train loss:0.02030948463581955\n",
      "train loss:0.002336977378434474\n",
      "train loss:0.0037309706241707317\n",
      "train loss:0.01374587228168781\n",
      "train loss:0.005750226992315402\n",
      "train loss:0.008623356725601102\n",
      "train loss:0.013173016605161892\n",
      "train loss:0.005163978251513591\n",
      "train loss:0.11058584684271454\n",
      "train loss:0.009542562176220671\n",
      "train loss:0.028212423121870858\n",
      "train loss:0.004285634019397857\n",
      "train loss:0.014624511791645313\n",
      "train loss:0.0063787784206727795\n",
      "train loss:0.0718885525036468\n",
      "train loss:0.007829254123377575\n",
      "train loss:0.008573317608870422\n",
      "train loss:0.005751072761981262\n",
      "train loss:0.017535321683442868\n",
      "train loss:0.003570579682705874\n",
      "train loss:0.009487051593582598\n",
      "train loss:0.008949776099763775\n",
      "train loss:0.024006225627108347\n",
      "train loss:0.003646395169173437\n",
      "train loss:0.015050621755660012\n",
      "train loss:0.0010794966061646815\n",
      "train loss:0.004811821284960717\n",
      "train loss:0.011829228817351751\n",
      "train loss:0.007808536322052953\n",
      "train loss:0.009094193770594387\n",
      "train loss:0.009190538984718173\n",
      "train loss:0.023440250758663607\n",
      "train loss:0.037354919220742\n",
      "train loss:0.006443957273226167\n",
      "train loss:0.013970126755980338\n",
      "train loss:0.002933561151443781\n",
      "train loss:0.0053246001688786685\n",
      "train loss:0.004794760506961913\n",
      "train loss:0.005406729118286872\n",
      "train loss:0.007509396460872359\n",
      "train loss:0.011329445165051657\n",
      "train loss:0.0008037455225810424\n",
      "train loss:0.006286814469687627\n",
      "train loss:0.003347681459120203\n",
      "train loss:0.0098292202232296\n",
      "train loss:0.005110165706005234\n",
      "train loss:0.009803666112454847\n",
      "train loss:0.010105904095466746\n",
      "train loss:0.03430451928831426\n",
      "train loss:0.008659230358706997\n",
      "train loss:0.0029009610203719737\n",
      "train loss:0.00468447395532973\n",
      "train loss:0.004633616289518411\n",
      "train loss:0.004407595075324215\n",
      "train loss:0.05559660322107926\n",
      "train loss:0.0044932097179430485\n",
      "train loss:0.0028276003010735762\n",
      "train loss:0.001413201631835975\n",
      "train loss:0.004599343166079107\n",
      "train loss:0.003941871273796281\n",
      "train loss:0.0006658755972475494\n",
      "train loss:0.005092959911730679\n",
      "train loss:0.015508591284332407\n",
      "train loss:0.020439704814001\n",
      "train loss:0.0017970676646515624\n",
      "train loss:0.01336479512227894\n",
      "train loss:0.04001456628227342\n",
      "train loss:0.0026496080781702124\n",
      "train loss:0.007286245183570662\n",
      "train loss:0.02298637309508165\n",
      "train loss:0.010513811345330104\n",
      "train loss:0.008946622510705882\n",
      "train loss:0.0023356219817592485\n",
      "train loss:0.02083124260076106\n",
      "train loss:0.016015076252090987\n",
      "train loss:0.062009724443995065\n",
      "train loss:0.007025798626198739\n",
      "train loss:0.042078327430773055\n",
      "train loss:0.0024605036151303626\n",
      "train loss:0.02495276990579812\n",
      "train loss:0.05872378317305845\n",
      "train loss:0.007563386409870389\n",
      "train loss:0.0018524746933404345\n",
      "train loss:0.002041891096998593\n",
      "train loss:0.023215534588183786\n",
      "train loss:0.023258968554774367\n",
      "train loss:0.002274294717636806\n",
      "train loss:0.007377695457445128\n",
      "train loss:0.00612767470610303\n",
      "train loss:0.03211658996342708\n",
      "train loss:0.00847222760980145\n",
      "train loss:0.047726272919167555\n",
      "train loss:0.005402182894305184\n",
      "train loss:0.0016295057483643304\n",
      "train loss:0.016723373923420604\n",
      "train loss:0.005281420073944142\n",
      "train loss:0.009092279866654496\n",
      "train loss:0.00229893697729563\n",
      "train loss:0.008984629906770104\n",
      "train loss:0.03748450737045124\n",
      "train loss:0.04538874103277994\n",
      "train loss:0.0177926981863282\n",
      "train loss:0.0015571968849616537\n",
      "train loss:0.016086755872246065\n",
      "train loss:0.04816419105026818\n",
      "train loss:0.006876503346275747\n",
      "train loss:0.003989863876307524\n",
      "train loss:0.04338726148624823\n",
      "train loss:0.012731805970199715\n",
      "train loss:0.0013899129956857553\n",
      "train loss:0.010477965718886019\n",
      "train loss:0.007073921333723805\n",
      "train loss:0.010555512113037792\n",
      "train loss:0.010539475295872925\n",
      "train loss:0.003065084544072507\n",
      "train loss:0.007208029088067284\n",
      "train loss:0.020376008104409538\n",
      "train loss:0.037961356814050204\n",
      "train loss:0.05012409591525035\n",
      "train loss:0.0015086442472387162\n",
      "train loss:0.009224565084587869\n",
      "train loss:0.009332757190521103\n",
      "train loss:0.025243818662626788\n",
      "train loss:0.027116160355953985\n",
      "train loss:0.023456695565216762\n",
      "train loss:0.05701967541846965\n",
      "train loss:0.014229088829577114\n",
      "train loss:0.004968256591539418\n",
      "train loss:0.018226342529411042\n",
      "train loss:0.011868152416918068\n",
      "train loss:0.011773935633974058\n",
      "train loss:0.003973016129319193\n",
      "train loss:0.008760327213588255\n",
      "train loss:0.015727246662177972\n",
      "train loss:0.016448814906926998\n",
      "train loss:0.0032814813647631886\n",
      "train loss:0.013322925839187318\n",
      "train loss:0.009092914496081355\n",
      "train loss:0.0064949714228713816\n",
      "train loss:0.0031671587513605153\n",
      "train loss:0.016353414735511537\n",
      "train loss:0.023785879220549744\n",
      "train loss:0.047274103321662944\n",
      "train loss:0.030376643086863102\n",
      "train loss:0.016820781070434943\n",
      "train loss:0.0068615306359298515\n",
      "train loss:0.010189189812879765\n",
      "train loss:0.00799938423156566\n",
      "train loss:0.021262684345249045\n",
      "train loss:0.025398247780813243\n",
      "train loss:0.004314719095289239\n",
      "train loss:0.0867814488344827\n",
      "train loss:0.01651924428119153\n",
      "train loss:0.014340624672270184\n",
      "train loss:0.003762740236642206\n",
      "train loss:0.018576757530904254\n",
      "train loss:0.0016693185252659793\n",
      "train loss:0.018079121767891157\n",
      "train loss:0.003108776382675118\n",
      "train loss:0.010726739563217165\n",
      "train loss:0.005395953959977515\n",
      "train loss:0.0278916431670606\n",
      "train loss:0.0026473082064614423\n",
      "train loss:0.0045092056463451125\n",
      "train loss:0.0014021643944110878\n",
      "train loss:0.004386221000516759\n",
      "train loss:0.0353109766749359\n",
      "train loss:0.005856666478816979\n",
      "train loss:0.0023646784034466184\n",
      "train loss:0.015805228978653572\n",
      "train loss:0.0066307577767847035\n",
      "train loss:0.008164910292983246\n",
      "train loss:0.019447184445811113\n",
      "train loss:0.01233501624761251\n",
      "train loss:0.009268488496391341\n",
      "train loss:0.01524889812329997\n",
      "train loss:0.021883886303713344\n",
      "train loss:0.015718372774824643\n",
      "train loss:0.005784252037272089\n",
      "train loss:0.019380371057294378\n",
      "train loss:0.00438256403099419\n",
      "train loss:0.0030058874438757843\n",
      "train loss:0.007120005071587907\n",
      "train loss:0.010360942980695556\n",
      "train loss:0.03082683444903445\n",
      "train loss:0.01204511524173235\n",
      "train loss:0.052420663359254126\n",
      "train loss:0.012592239346388171\n",
      "train loss:0.017658147641945753\n",
      "train loss:0.0012216590268731573\n",
      "train loss:0.0030163411646306803\n",
      "train loss:0.010238235543732233\n",
      "train loss:0.010171973872513639\n",
      "train loss:0.059404127313246076\n",
      "train loss:0.037812501793698466\n",
      "train loss:0.029472472815149045\n",
      "train loss:0.003935582891972432\n",
      "train loss:0.07951811170223556\n",
      "train loss:0.004791370690279788\n",
      "train loss:0.01550845094849947\n",
      "train loss:0.007018336551409614\n",
      "train loss:0.007005931616481217\n",
      "train loss:0.0018805628451890035\n",
      "train loss:0.006414513974596328\n",
      "train loss:0.011134195846972464\n",
      "train loss:0.008852892150067387\n",
      "train loss:0.0005755874590885072\n",
      "train loss:0.016193543524333803\n",
      "train loss:0.0015238788732903697\n",
      "train loss:0.016368459125046916\n",
      "train loss:0.008049284312512794\n",
      "train loss:0.08324694762981144\n",
      "train loss:0.00322853045785112\n",
      "train loss:0.01536785061109006\n",
      "train loss:0.00930233177413907\n",
      "train loss:0.013734202782523956\n",
      "train loss:0.003531535284541754\n",
      "train loss:0.02669344527419253\n",
      "train loss:0.021998057603391727\n",
      "train loss:0.01181122014571812\n",
      "train loss:0.011606632735376937\n",
      "train loss:0.004823973781212503\n",
      "train loss:0.010731834299833527\n",
      "train loss:0.012406096320081128\n",
      "=== epoch:9, train acc:0.997, test acc:0.981 ===\n",
      "train loss:0.010405673328139589\n",
      "train loss:0.004972513559186637\n",
      "train loss:0.014078003398020173\n",
      "train loss:0.002079136427121778\n",
      "train loss:0.004453827303715349\n",
      "train loss:0.008969244473117678\n",
      "train loss:0.019733384577659507\n",
      "train loss:0.006294190004962606\n",
      "train loss:0.009960373347493768\n",
      "train loss:0.012876758259814271\n",
      "train loss:0.010638546729512649\n",
      "train loss:0.008182795822775176\n",
      "train loss:0.0060639273457781615\n",
      "train loss:0.045978731638719295\n",
      "train loss:0.009234372837051053\n",
      "train loss:0.005282982566309705\n",
      "train loss:0.008514973109460682\n",
      "train loss:0.027135137053138067\n",
      "train loss:0.0100370762843554\n",
      "train loss:0.006279021097814237\n",
      "train loss:0.005422157501916535\n",
      "train loss:0.003257751445319915\n",
      "train loss:0.008643168892781343\n",
      "train loss:0.036778501336302116\n",
      "train loss:0.0044163274215937584\n",
      "train loss:0.0285991284940457\n",
      "train loss:0.0062038381165781704\n",
      "train loss:0.03600227884414531\n",
      "train loss:0.003761317787409887\n",
      "train loss:0.004140533885289999\n",
      "train loss:0.004109270545737054\n",
      "train loss:0.0011132154775336113\n",
      "train loss:0.00364416795708293\n",
      "train loss:0.006232581551085652\n",
      "train loss:0.021473103360781454\n",
      "train loss:0.001107674385515347\n",
      "train loss:0.0007688785166938779\n",
      "train loss:0.012292240993190562\n",
      "train loss:0.024001346819566546\n",
      "train loss:0.014963317453036386\n",
      "train loss:0.016758749503595637\n",
      "train loss:0.02601357233495971\n",
      "train loss:0.006421677361609878\n",
      "train loss:0.026912354499190005\n",
      "train loss:0.01770784055152129\n",
      "train loss:0.014238705940527176\n",
      "train loss:0.0067821813984021585\n",
      "train loss:0.011066601033724253\n",
      "train loss:0.00750314082366167\n",
      "train loss:0.03500369299635628\n",
      "train loss:0.0018874809873308225\n",
      "train loss:0.050621219160009494\n",
      "train loss:0.004518961754201656\n",
      "train loss:0.0034116217353945557\n",
      "train loss:0.010871391815255602\n",
      "train loss:0.007559917597854678\n",
      "train loss:0.003388550155065206\n",
      "train loss:0.0014648958206977044\n",
      "train loss:0.004479344465668315\n",
      "train loss:0.021279119647898602\n",
      "train loss:0.0027654890425910282\n",
      "train loss:0.014159544640394137\n",
      "train loss:0.04282700419860315\n",
      "train loss:0.008714491539970979\n",
      "train loss:0.005471428817397369\n",
      "train loss:0.036049224754101286\n",
      "train loss:0.009467628248881567\n",
      "train loss:0.028760548085942514\n",
      "train loss:0.003411245895791283\n",
      "train loss:0.011194015995608144\n",
      "train loss:0.0070967443898864654\n",
      "train loss:0.005721515475835458\n",
      "train loss:0.0058404695320263935\n",
      "train loss:0.006581050911056956\n",
      "train loss:0.010239575883263257\n",
      "train loss:0.0058825272913131344\n",
      "train loss:0.0054484940079335045\n",
      "train loss:0.004889375729674416\n",
      "train loss:0.004052370089068626\n",
      "train loss:0.007438898236740317\n",
      "train loss:0.07012833000452857\n",
      "train loss:0.09089031238119336\n",
      "train loss:0.008064178075986039\n",
      "train loss:0.02575647538051803\n",
      "train loss:0.004165179879539604\n",
      "train loss:0.026477964285318078\n",
      "train loss:0.012245780221331937\n",
      "train loss:0.008583322342123008\n",
      "train loss:0.037729173137164444\n",
      "train loss:0.006601435448306404\n",
      "train loss:0.0329461377271524\n",
      "train loss:0.005479455240970317\n",
      "train loss:0.010877129928896927\n",
      "train loss:0.05792725725312858\n",
      "train loss:0.004219343399075656\n",
      "train loss:0.02249086876359847\n",
      "train loss:0.021607045119513328\n",
      "train loss:0.07515482610638646\n",
      "train loss:0.012583107935869434\n",
      "train loss:0.0054837293764556936\n",
      "train loss:0.00467597495926087\n",
      "train loss:0.011278929331771263\n",
      "train loss:0.016774281290372773\n",
      "train loss:0.002715733444744703\n",
      "train loss:0.011935171341872655\n",
      "train loss:0.0057461209026243056\n",
      "train loss:0.00667708064817695\n",
      "train loss:0.007647420574502164\n",
      "train loss:0.0010225889383628927\n",
      "train loss:0.003294477932728563\n",
      "train loss:0.002708314032291161\n",
      "train loss:0.00909600543134188\n",
      "train loss:0.006043530981500638\n",
      "train loss:0.009194120391085822\n",
      "train loss:0.004209480247987316\n",
      "train loss:0.011912368404516236\n",
      "train loss:0.010678462656490724\n",
      "train loss:0.018590173387062744\n",
      "train loss:0.024954909356641183\n",
      "train loss:0.03403380559387605\n",
      "train loss:0.0034702711745379306\n",
      "train loss:0.007168627312479446\n",
      "train loss:0.0015305613801804383\n",
      "train loss:0.005296813738685344\n",
      "train loss:0.0044425864770822245\n",
      "train loss:0.008827628144094922\n",
      "train loss:0.016467929977114132\n",
      "train loss:0.011761986187118602\n",
      "train loss:0.006282979457841334\n",
      "train loss:0.005282718735841317\n",
      "train loss:0.007226074652928833\n",
      "train loss:0.015128159025095306\n",
      "train loss:0.0052294732890104285\n",
      "train loss:0.0993416135118106\n",
      "train loss:0.04205624603725254\n",
      "train loss:0.029666162154528844\n",
      "train loss:0.04260510510692132\n",
      "train loss:0.018179385683579\n",
      "train loss:0.004476756781110248\n",
      "train loss:0.0032011256340664387\n",
      "train loss:0.012590385014978549\n",
      "train loss:0.006623046368529737\n",
      "train loss:0.023292553714583174\n",
      "train loss:0.01611258335818476\n",
      "train loss:0.0021814466611095283\n",
      "train loss:0.007567717134151068\n",
      "train loss:0.00978335854258947\n",
      "train loss:0.014573033970883314\n",
      "train loss:0.02596305993120855\n",
      "train loss:0.005992825896184543\n",
      "train loss:0.0024575502693300628\n",
      "train loss:0.005253352938432151\n",
      "train loss:0.006619186761310941\n",
      "train loss:0.0029297167877387104\n",
      "train loss:0.015408796708206328\n",
      "train loss:0.02599604244634663\n",
      "train loss:0.004616154493113525\n",
      "train loss:0.03230323479120944\n",
      "train loss:0.03115947422073281\n",
      "train loss:0.03658900183366843\n",
      "train loss:0.007999638523216989\n",
      "train loss:0.018348026910275442\n",
      "train loss:0.002350278823880689\n",
      "train loss:0.016165251002311098\n",
      "train loss:0.004554662021073052\n",
      "train loss:0.004324931666947617\n",
      "train loss:0.00931529619846851\n",
      "train loss:0.017564277057570313\n",
      "train loss:0.002650304199206023\n",
      "train loss:0.018291740074822503\n",
      "train loss:0.0037982805659055575\n",
      "train loss:0.0195453593917776\n",
      "train loss:0.012273849587381792\n",
      "train loss:0.0022061389749333956\n",
      "train loss:0.004699045607072635\n",
      "train loss:0.00524225108114406\n",
      "train loss:0.00379852122651888\n",
      "train loss:0.16284992484206742\n",
      "train loss:0.006555337275261823\n",
      "train loss:0.014402954144415291\n",
      "train loss:0.011816766112017036\n",
      "train loss:0.006156750323058352\n",
      "train loss:0.028506130551305405\n",
      "train loss:0.005750091674977408\n",
      "train loss:0.025475417645721036\n",
      "train loss:0.005243447973392161\n",
      "train loss:0.00553136636106226\n",
      "train loss:0.013079687401291956\n",
      "train loss:0.007299376919162333\n",
      "train loss:0.005175440792923927\n",
      "train loss:0.005853143432091086\n",
      "train loss:0.022140570776794016\n",
      "train loss:0.002916568844623253\n",
      "train loss:0.012277478183243267\n",
      "train loss:0.002150563888712973\n",
      "train loss:0.010548043691121271\n",
      "train loss:0.024254070413376426\n",
      "train loss:0.02011977819202115\n",
      "train loss:0.02082641169933175\n",
      "train loss:0.01773972171299106\n",
      "train loss:0.004002536072782714\n",
      "train loss:0.0023992825295703775\n",
      "train loss:0.0017212733757023726\n",
      "train loss:0.016544873539646107\n",
      "train loss:0.02290198529191028\n",
      "train loss:0.019451169673909526\n",
      "train loss:0.0075310900366158005\n",
      "train loss:0.01600027532615502\n",
      "train loss:0.028674349289985895\n",
      "train loss:0.04493984483217523\n",
      "train loss:0.007358294478122379\n",
      "train loss:0.0067478843339918645\n",
      "train loss:0.002449469591063369\n",
      "train loss:0.0076632490748358715\n",
      "train loss:0.020383294419118868\n",
      "train loss:0.007613216391606019\n",
      "train loss:0.014350372965107655\n",
      "train loss:0.009380722333185536\n",
      "train loss:0.019741400521137525\n",
      "train loss:0.015818158056581946\n",
      "train loss:0.004175930646090022\n",
      "train loss:0.01104431367954461\n",
      "train loss:0.007361018977106972\n",
      "train loss:0.006792217602641818\n",
      "train loss:0.009440721794591368\n",
      "train loss:0.020609830126437672\n",
      "train loss:0.023708421482098882\n",
      "train loss:0.004886704013931711\n",
      "train loss:0.0010695581002445151\n",
      "train loss:0.011522650224430558\n",
      "train loss:0.030580269913201338\n",
      "train loss:0.006531429367904632\n",
      "train loss:0.010199083135174918\n",
      "train loss:0.005255205254568178\n",
      "train loss:0.0033884306406989097\n",
      "train loss:0.0066548726710568\n",
      "train loss:0.013623882937371469\n",
      "train loss:0.008925986709226534\n",
      "train loss:0.02463036545753843\n",
      "train loss:0.00401165051616093\n",
      "train loss:0.0012180351972389683\n",
      "train loss:0.003647896298145565\n",
      "train loss:0.004349231915373431\n",
      "train loss:0.00797058227177499\n",
      "train loss:0.009772467835830485\n",
      "train loss:0.039079754759894385\n",
      "train loss:0.001443767325092298\n",
      "train loss:0.015241886364270825\n",
      "train loss:0.03648788918327315\n",
      "train loss:0.011100440629457524\n",
      "train loss:0.012084323220355707\n",
      "train loss:0.027372181132285112\n",
      "train loss:0.025414219561461705\n",
      "train loss:0.020052504410447175\n",
      "train loss:0.025515739920171806\n",
      "train loss:0.00140619327415017\n",
      "train loss:0.0045729541679707785\n",
      "train loss:0.003884438001306369\n",
      "train loss:0.005376328918037099\n",
      "train loss:0.006441746177787916\n",
      "train loss:0.019125119962799995\n",
      "train loss:0.005553428944065724\n",
      "train loss:0.010619611222778292\n",
      "train loss:0.0031074349563239134\n",
      "train loss:0.009402720575014751\n",
      "train loss:0.013342018428201154\n",
      "train loss:0.017613079684291513\n",
      "train loss:0.016134910662815458\n",
      "train loss:0.024928507914070107\n",
      "train loss:0.012115949033000766\n",
      "train loss:0.020050416257033823\n",
      "train loss:0.0070182294809427006\n",
      "train loss:0.019212384917591942\n",
      "train loss:0.026984141674082363\n",
      "train loss:0.0181450483325979\n",
      "train loss:0.004589715828956937\n",
      "train loss:0.0024575300314699984\n",
      "train loss:0.007397190713685776\n",
      "train loss:0.00595830775652112\n",
      "train loss:0.005299631596978295\n",
      "train loss:0.012693015947750552\n",
      "train loss:0.009913928714539992\n",
      "train loss:0.007770885482432873\n",
      "train loss:0.015141694634143006\n",
      "train loss:0.007281480261101113\n",
      "train loss:0.002603566885911467\n",
      "train loss:0.01233366007468303\n",
      "train loss:0.00486658256464195\n",
      "train loss:0.005327021575879811\n",
      "train loss:0.003523324986781487\n",
      "train loss:0.0075664429578693885\n",
      "train loss:0.021366380027939436\n",
      "train loss:0.05029539662277626\n",
      "train loss:0.0052565466679808884\n",
      "train loss:0.007041944493224967\n",
      "train loss:0.03467757596942257\n",
      "train loss:0.08280905507474959\n",
      "train loss:0.0067485005555550336\n",
      "train loss:0.009911184913832383\n",
      "train loss:0.0033533003171130325\n",
      "train loss:0.00898809840830639\n",
      "train loss:0.005775705423023072\n",
      "train loss:0.00033847375246838805\n",
      "train loss:0.00531935092592238\n",
      "train loss:0.014895017641290673\n",
      "train loss:0.02061851253860089\n",
      "train loss:0.006079704900888382\n",
      "train loss:0.005889733185874541\n",
      "train loss:0.017674656210288988\n",
      "train loss:0.006755970925663455\n",
      "train loss:0.0015097774858089266\n",
      "train loss:0.012612739798290234\n",
      "train loss:0.003598822211655947\n",
      "train loss:0.02441480419253125\n",
      "train loss:0.03093107391986337\n",
      "train loss:0.010324958602175673\n",
      "train loss:0.0073185054456057766\n",
      "train loss:0.011860957294489804\n",
      "train loss:0.004823056567570316\n",
      "train loss:0.015774197034910963\n",
      "train loss:0.00952713761222392\n",
      "train loss:0.0043010640170061286\n",
      "train loss:0.015269814124152657\n",
      "train loss:0.0035457539196745887\n",
      "train loss:0.005414125925555613\n",
      "train loss:0.009944409646757883\n",
      "train loss:0.011311713691377099\n",
      "train loss:0.02344830349700106\n",
      "train loss:0.005148504147095851\n",
      "train loss:0.040835403760475904\n",
      "train loss:0.0005728073084507043\n",
      "train loss:0.0037372019513836376\n",
      "train loss:0.008205079311828909\n",
      "train loss:0.004393500457558312\n",
      "train loss:0.00980261646246152\n",
      "train loss:0.006623108678678448\n",
      "train loss:0.006821251292333611\n",
      "train loss:0.02272715572895645\n",
      "train loss:0.0029292571360821844\n",
      "train loss:0.01579565754905109\n",
      "train loss:0.003613996193103651\n",
      "train loss:0.004906410830886252\n",
      "train loss:0.010497392147672122\n",
      "train loss:0.002342404609948188\n",
      "train loss:0.003924598924220183\n",
      "train loss:0.00279070983282537\n",
      "train loss:0.0009498931974453619\n",
      "train loss:0.05346964479348136\n",
      "train loss:0.017136453452080605\n",
      "train loss:0.023440201828255177\n",
      "train loss:0.04440433848492129\n",
      "train loss:0.007486955582932326\n",
      "train loss:0.0028936221465317036\n",
      "train loss:0.01963500862777184\n",
      "train loss:0.012828695574333264\n",
      "train loss:0.00442610738049841\n",
      "train loss:0.017276844486414134\n",
      "train loss:0.019763873413525188\n",
      "train loss:0.022799505905759535\n",
      "train loss:0.019884580684385714\n",
      "train loss:0.0037885276191636343\n",
      "train loss:0.004084177346514889\n",
      "train loss:0.00202098893907673\n",
      "train loss:0.017312225644848404\n",
      "train loss:0.004121151162356171\n",
      "train loss:0.004378863982190006\n",
      "train loss:0.029797459949174585\n",
      "train loss:0.0021805481219462994\n",
      "train loss:0.002882746714729274\n",
      "train loss:0.021959930094884776\n",
      "train loss:0.00613998697392523\n",
      "train loss:0.013362809229942683\n",
      "train loss:0.00852385809180399\n",
      "train loss:0.05283571211419108\n",
      "train loss:0.00623012443368304\n",
      "train loss:0.011062051759366443\n",
      "train loss:0.04329894326411985\n",
      "train loss:0.023637074964987615\n",
      "train loss:0.022566368006242052\n",
      "train loss:0.016324705142374683\n",
      "train loss:0.004741166877714648\n",
      "train loss:0.012432378375647448\n",
      "train loss:0.0034781501793194943\n",
      "train loss:0.006691060165171652\n",
      "train loss:0.013580851232372699\n",
      "train loss:0.0058624220609418175\n",
      "train loss:0.005346692850835657\n",
      "train loss:0.008891192230785503\n",
      "train loss:0.0038080836448817564\n",
      "train loss:0.0204124850049289\n",
      "train loss:0.010511097085613617\n",
      "train loss:0.017301506012267428\n",
      "train loss:0.0016783350581132126\n",
      "train loss:0.020268723088769116\n",
      "train loss:0.00940423427644176\n",
      "train loss:0.0032616901693586046\n",
      "train loss:0.01167336235797819\n",
      "train loss:0.08491945885010717\n",
      "train loss:0.010347462753360064\n",
      "train loss:0.010745664592542233\n",
      "train loss:0.0023817480158849513\n",
      "train loss:0.006299937806896358\n",
      "train loss:0.007712008398802871\n",
      "train loss:0.009772522838186281\n",
      "train loss:0.004912714137632841\n",
      "train loss:0.021479980409453252\n",
      "train loss:0.006664940945022553\n",
      "train loss:0.005333183820483981\n",
      "train loss:0.005040693164943546\n",
      "train loss:0.0033091260218954827\n",
      "train loss:0.005492069077388561\n",
      "train loss:0.001615237482799534\n",
      "train loss:0.0009956519677879895\n",
      "train loss:0.012878190222143706\n",
      "train loss:0.0032228318423804443\n",
      "train loss:0.0038224567931605218\n",
      "train loss:0.01877686258586174\n",
      "train loss:0.035581352012689076\n",
      "train loss:0.013851442835519806\n",
      "train loss:0.008934761079550683\n",
      "train loss:0.003729641253999808\n",
      "train loss:0.004630019851274179\n",
      "train loss:0.005493554757194196\n",
      "train loss:0.008222982773693698\n",
      "train loss:0.0477877632381999\n",
      "train loss:0.010560101487959854\n",
      "train loss:0.008999317108337823\n",
      "train loss:0.008883851426853951\n",
      "train loss:0.002828189232972924\n",
      "train loss:0.010675213148518959\n",
      "train loss:0.0009216377480661072\n",
      "train loss:0.0028121664405725303\n",
      "train loss:0.023251685234053795\n",
      "train loss:0.005984045201541948\n",
      "train loss:0.041129145082102365\n",
      "train loss:0.016790533642884065\n",
      "train loss:0.0012903390278933724\n",
      "train loss:0.009003340173425215\n",
      "train loss:0.003521414417202569\n",
      "train loss:0.01796573430975703\n",
      "train loss:0.0045486339870771655\n",
      "train loss:0.007133348229759017\n",
      "train loss:0.016435800363884173\n",
      "train loss:0.00301284067039954\n",
      "train loss:0.01804217353827341\n",
      "train loss:0.007861009076876986\n",
      "train loss:0.014145748420739186\n",
      "train loss:0.001295366875341138\n",
      "train loss:0.0051422329017126135\n",
      "train loss:0.007188585274150831\n",
      "train loss:0.0038736464658490496\n",
      "train loss:0.011243925348899377\n",
      "train loss:0.0032393815198282637\n",
      "train loss:0.005880128159826709\n",
      "train loss:0.005770472512022454\n",
      "train loss:0.007917270573137923\n",
      "train loss:0.0351554747105809\n",
      "train loss:0.005844062923882064\n",
      "train loss:0.009955947339898595\n",
      "train loss:0.009947998355894053\n",
      "train loss:0.004158398420058378\n",
      "train loss:0.0024470762617463566\n",
      "train loss:0.039876263658029154\n",
      "train loss:0.014940152702047954\n",
      "train loss:0.0071608615886760655\n",
      "train loss:0.018018128844883876\n",
      "train loss:0.02625943707843388\n",
      "train loss:0.01221657894399439\n",
      "train loss:0.0033495640951376167\n",
      "train loss:0.0028004623942030214\n",
      "train loss:0.0024276644829469866\n",
      "train loss:0.03454271320621079\n",
      "train loss:0.022973697938442575\n",
      "train loss:0.007682653385043568\n",
      "train loss:0.02774785379256721\n",
      "train loss:0.060565610064950676\n",
      "train loss:0.019629868124100475\n",
      "train loss:0.0017317800463030433\n",
      "train loss:0.010970529023462423\n",
      "train loss:0.038643510779796386\n",
      "train loss:0.00810919721737243\n",
      "train loss:0.01164525197343908\n",
      "train loss:0.008317907602171642\n",
      "train loss:0.016438966754374406\n",
      "train loss:0.009844249786230497\n",
      "train loss:0.002198926815479405\n",
      "train loss:0.009294333347536808\n",
      "train loss:0.0007866511916841287\n",
      "train loss:0.007691977688818551\n",
      "train loss:0.00532011594963038\n",
      "train loss:0.0018300634289154269\n",
      "train loss:0.014453780510188525\n",
      "train loss:0.012163179966909039\n",
      "train loss:0.012521726134728142\n",
      "train loss:0.012347066600595502\n",
      "train loss:0.006919861234479732\n",
      "train loss:0.00801186256927946\n",
      "train loss:0.05436497798020718\n",
      "train loss:0.012294947156495976\n",
      "train loss:0.004471012965091752\n",
      "train loss:0.008264568884687538\n",
      "train loss:0.12898471376229229\n",
      "train loss:0.03298195571881388\n",
      "train loss:0.007652079797902937\n",
      "train loss:0.03186030735848053\n",
      "train loss:0.006098784635097525\n",
      "train loss:0.022513658025176308\n",
      "train loss:0.003587689912365065\n",
      "train loss:0.005005227618393787\n",
      "train loss:0.04121847932000474\n",
      "train loss:0.01247639729754141\n",
      "train loss:0.005660125012827389\n",
      "train loss:0.005003025430402691\n",
      "train loss:0.01793150714668431\n",
      "train loss:0.005055028429480991\n",
      "train loss:0.0017626329961088218\n",
      "train loss:0.020982724552672815\n",
      "train loss:0.03323883884782593\n",
      "train loss:0.0039585394576823005\n",
      "train loss:0.02663626846558409\n",
      "train loss:0.012089107183440034\n",
      "train loss:0.01909114528188608\n",
      "train loss:0.008599340648701628\n",
      "train loss:0.004537869577364044\n",
      "train loss:0.0017371083295998006\n",
      "train loss:0.01524757323795199\n",
      "train loss:0.0016127796930432562\n",
      "train loss:0.0030685748502209515\n",
      "train loss:0.004495378682236794\n",
      "train loss:0.0038477681104887094\n",
      "train loss:0.056579102695664185\n",
      "train loss:0.020526213923768646\n",
      "train loss:0.003961535643573838\n",
      "train loss:0.05673728386620123\n",
      "train loss:0.004979696083276767\n",
      "train loss:0.005651809983173226\n",
      "train loss:0.009626234835488474\n",
      "train loss:0.011805597136033645\n",
      "train loss:0.00795722080682784\n",
      "train loss:0.00994949282538632\n",
      "train loss:0.001925422778375047\n",
      "train loss:0.050923864596222106\n",
      "train loss:0.005814406323018891\n",
      "train loss:0.0033270465758743024\n",
      "train loss:0.002683591257149408\n",
      "train loss:0.009389868087170981\n",
      "train loss:0.004926167592094969\n",
      "train loss:0.006096462272984736\n",
      "train loss:0.011380761936073727\n",
      "train loss:0.018423647515914732\n",
      "train loss:0.008056424051112288\n",
      "train loss:0.022231677912792614\n",
      "train loss:0.007189268254218677\n",
      "train loss:0.0029138967728060024\n",
      "train loss:0.0018613568488275464\n",
      "train loss:0.07260702561617163\n",
      "train loss:0.006148575243277218\n",
      "train loss:0.02159375672827323\n",
      "train loss:0.004883583814710576\n",
      "train loss:0.03929035655394315\n",
      "train loss:0.0191816856599869\n",
      "train loss:0.00417929309745099\n",
      "train loss:0.009751786885731728\n",
      "train loss:0.06684862202012164\n",
      "train loss:0.0023552390259307648\n",
      "train loss:0.012267383452812081\n",
      "train loss:0.009194907491075531\n",
      "train loss:0.004625587950832972\n",
      "train loss:0.004404609781785402\n",
      "train loss:0.024914634566421637\n",
      "train loss:0.004741555255968163\n",
      "train loss:0.02436201413854171\n",
      "train loss:0.003403123486641133\n",
      "train loss:0.013712141726727434\n",
      "train loss:0.0022896156225615176\n",
      "train loss:0.010741359775183392\n",
      "train loss:0.0026756582028123793\n",
      "train loss:0.002094706556031534\n",
      "train loss:0.007522595466569592\n",
      "train loss:0.010582087970297162\n",
      "train loss:0.0048663134618664285\n",
      "train loss:0.025906123133751836\n",
      "train loss:0.021196025148474167\n",
      "train loss:0.008376051873898359\n",
      "train loss:0.006897379150783854\n",
      "train loss:0.004524804866455195\n",
      "train loss:0.022738655438761413\n",
      "train loss:0.014132310295782182\n",
      "train loss:0.005426967866060729\n",
      "train loss:0.00526029732857828\n",
      "train loss:0.0078238834583833\n",
      "train loss:0.013728485862543207\n",
      "train loss:0.002395806740406581\n",
      "train loss:0.0044118200579242196\n",
      "train loss:0.011041414624782652\n",
      "train loss:0.016256928240034555\n",
      "train loss:0.007707126267428176\n",
      "train loss:0.005239477752270862\n",
      "train loss:0.0012605203135161469\n",
      "train loss:0.03312383282434509\n",
      "=== epoch:10, train acc:0.995, test acc:0.989 ===\n",
      "train loss:0.008092981797789318\n",
      "train loss:0.014179490746624146\n",
      "train loss:0.014067388500636617\n",
      "train loss:0.005539132611894942\n",
      "train loss:0.006052937628561042\n",
      "train loss:0.016924463406674597\n",
      "train loss:0.004931641556765709\n",
      "train loss:0.006916398229174934\n",
      "train loss:0.004655188450371205\n",
      "train loss:0.002493060882539702\n",
      "train loss:0.0022146962664434714\n",
      "train loss:0.01143688131531538\n",
      "train loss:0.0018021721566440793\n",
      "train loss:0.006687384958102698\n",
      "train loss:0.008896977246580267\n",
      "train loss:0.0030192231079010905\n",
      "train loss:0.0007628004178552483\n",
      "train loss:0.005925740941062011\n",
      "train loss:0.03277857543150849\n",
      "train loss:0.010288471426148043\n",
      "train loss:0.006801203843058803\n",
      "train loss:0.07195815436480935\n",
      "train loss:0.012129652150132285\n",
      "train loss:0.00993593800217991\n",
      "train loss:0.006711648665617197\n",
      "train loss:0.01120759865617731\n",
      "train loss:0.007584429440318213\n",
      "train loss:0.003906815802687813\n",
      "train loss:0.00723811049052062\n",
      "train loss:0.0014506141227139618\n",
      "train loss:0.005386092645081426\n",
      "train loss:0.027000382503860148\n",
      "train loss:0.0039293265818846675\n",
      "train loss:0.008568272996720784\n",
      "train loss:0.0013314899171367428\n",
      "train loss:0.0016777068826896555\n",
      "train loss:0.015211933858130572\n",
      "train loss:0.0007707789887815872\n",
      "train loss:0.007960253168764586\n",
      "train loss:0.007542665472689135\n",
      "train loss:0.009946699724028679\n",
      "train loss:0.0038149097599061675\n",
      "train loss:0.013481280682804648\n",
      "train loss:0.00788593130932571\n",
      "train loss:0.02987564235237518\n",
      "train loss:0.0010725591340705885\n",
      "train loss:0.0021106346576820166\n",
      "train loss:0.015439399278052369\n",
      "train loss:0.007136890339247243\n",
      "train loss:0.0018491960318001736\n",
      "train loss:0.0020822460747506916\n",
      "train loss:0.00864386970540224\n",
      "train loss:0.003318601993903735\n",
      "train loss:0.0027566954586459077\n",
      "train loss:0.012180014077263596\n",
      "train loss:0.002923701122085182\n",
      "train loss:0.0055084851075792194\n",
      "train loss:0.009203000294095322\n",
      "train loss:0.01764567723386666\n",
      "train loss:0.0018937960309411965\n",
      "train loss:0.002404953075903536\n",
      "train loss:0.010336539713546802\n",
      "train loss:0.001899738405740123\n",
      "train loss:0.0023738996579468177\n",
      "train loss:0.014696215871462946\n",
      "train loss:0.0023051089629556047\n",
      "train loss:0.0073779633492349235\n",
      "train loss:0.010625840278154393\n",
      "train loss:0.009255535654852447\n",
      "train loss:0.0011457393146055664\n",
      "train loss:0.005860016919700438\n",
      "train loss:0.005300619050338723\n",
      "train loss:0.008561348975340005\n",
      "train loss:0.0025069339029616705\n",
      "train loss:0.004656666875018326\n",
      "train loss:0.036442400413751665\n",
      "train loss:0.025952730884039498\n",
      "train loss:0.0007567633903054628\n",
      "train loss:0.003011951885258009\n",
      "train loss:0.006128827803233405\n",
      "train loss:0.002368038850343528\n",
      "train loss:0.004263876638374225\n",
      "train loss:0.0017588437831534904\n",
      "train loss:0.013882085042311067\n",
      "train loss:0.0023667278934241033\n",
      "train loss:0.011648343358548792\n",
      "train loss:0.0017260853869552142\n",
      "train loss:0.00359410826030616\n",
      "train loss:0.002848050804984328\n",
      "train loss:0.013003119517201227\n",
      "train loss:0.008817409137350653\n",
      "train loss:0.0017052853840705824\n",
      "train loss:0.0033870878776084056\n",
      "train loss:0.005406832215187748\n",
      "train loss:0.003993929484282493\n",
      "train loss:0.011072662617549916\n",
      "train loss:0.023319307504491942\n",
      "train loss:0.0043723802442271085\n",
      "train loss:0.002011941980839904\n",
      "train loss:0.023949639364308606\n",
      "train loss:0.0020474524474528104\n",
      "train loss:0.0013439291648384452\n",
      "train loss:0.0020142426679748532\n",
      "train loss:0.008248733269023896\n",
      "train loss:0.0022734548938999825\n",
      "train loss:0.006511991564973732\n",
      "train loss:0.0012890263597340843\n",
      "train loss:0.005220384480097491\n",
      "train loss:0.001522991160351036\n",
      "train loss:0.0011463807922080372\n",
      "train loss:0.001897758504616196\n",
      "train loss:0.002565701528610734\n",
      "train loss:0.0018255944333129093\n",
      "train loss:0.0013703442044149378\n",
      "train loss:0.011897146373618646\n",
      "train loss:0.006534888933657978\n",
      "train loss:0.018947318288235534\n",
      "train loss:0.0013332117609304583\n",
      "train loss:0.0009680841231265962\n",
      "train loss:0.07306951194907708\n",
      "train loss:0.013887197220753728\n",
      "train loss:0.000947715450117156\n",
      "train loss:0.0017681629807041201\n",
      "train loss:0.003578135433171965\n",
      "train loss:0.001645589983022418\n",
      "train loss:0.011254927169607491\n",
      "train loss:0.026741993860891516\n",
      "train loss:0.008050828534123648\n",
      "train loss:0.0019524024872281534\n",
      "train loss:0.004325238412561766\n",
      "train loss:0.020412091509562483\n",
      "train loss:0.012407109524387052\n",
      "train loss:0.009398680708816665\n",
      "train loss:0.04822810875093045\n",
      "train loss:0.009204448134663484\n",
      "train loss:0.020460047764380537\n",
      "train loss:0.005465276355668046\n",
      "train loss:0.008499160525368036\n",
      "train loss:0.0137427217438514\n",
      "train loss:0.013564317177868346\n",
      "train loss:0.002514699135345044\n",
      "train loss:0.000726882323995717\n",
      "train loss:0.016547761056455116\n",
      "train loss:0.010456546286321153\n",
      "train loss:0.004195927686329729\n",
      "train loss:0.004348504079688637\n",
      "train loss:0.0038496993497845024\n",
      "train loss:0.01814696699068845\n",
      "train loss:0.012461148763753925\n",
      "train loss:0.003090059897940432\n",
      "train loss:0.0021344536264189403\n",
      "train loss:0.004056947800390381\n",
      "train loss:0.004164234067371885\n",
      "train loss:0.006798222215912092\n",
      "train loss:0.002342480393432516\n",
      "train loss:0.003887265772255002\n",
      "train loss:0.004006399414930195\n",
      "train loss:0.002734776801980025\n",
      "train loss:0.010947192409660514\n",
      "train loss:0.0058358483323250445\n",
      "train loss:0.033608540344651425\n",
      "train loss:0.002631599311448029\n",
      "train loss:0.0054093153101441435\n",
      "train loss:0.0047046397523780184\n",
      "train loss:0.032029730657985034\n",
      "train loss:0.008602077540323641\n",
      "train loss:0.0020192751036583385\n",
      "train loss:0.003990547734989261\n",
      "train loss:0.010950179859690636\n",
      "train loss:0.0009068753683125656\n",
      "train loss:0.01218271918180711\n",
      "train loss:0.010445681660553601\n",
      "train loss:0.006770104068989817\n",
      "train loss:0.009971429052184072\n",
      "train loss:0.011284066421618\n",
      "train loss:0.046213789053600685\n",
      "train loss:0.0018947946451502726\n",
      "train loss:0.00445840641985602\n",
      "train loss:0.042153048841619765\n",
      "train loss:0.0076133541664447765\n",
      "train loss:0.00880018110519843\n",
      "train loss:0.001613274181707529\n",
      "train loss:0.015048179707078106\n",
      "train loss:0.004664715596492432\n",
      "train loss:0.004256537368920059\n",
      "train loss:0.008422910813322792\n",
      "train loss:0.000731611343293951\n",
      "train loss:0.011840244015657454\n",
      "train loss:0.005033962937540202\n",
      "train loss:0.0007081044772740012\n",
      "train loss:0.010907987002547328\n",
      "train loss:0.0202010742505092\n",
      "train loss:0.022304245194622815\n",
      "train loss:0.004473152758635674\n",
      "train loss:0.09936825510141083\n",
      "train loss:0.03238397134795508\n",
      "train loss:0.005430383179260435\n",
      "train loss:0.0048929825630684935\n",
      "train loss:0.007080731932534522\n",
      "train loss:0.001410405144403484\n",
      "train loss:0.003207185675324254\n",
      "train loss:0.014189507147798968\n",
      "train loss:0.011572423527833364\n",
      "train loss:0.0016337891322307277\n",
      "train loss:0.011259982224824211\n",
      "train loss:0.039397965475778625\n",
      "train loss:0.019610732535684045\n",
      "train loss:0.004141888004689786\n",
      "train loss:0.002697295015669438\n",
      "train loss:0.008544368455927794\n",
      "train loss:0.006406267045071419\n",
      "train loss:0.015973609323324926\n",
      "train loss:0.0008248230554178057\n",
      "train loss:0.004167643066009302\n",
      "train loss:0.029740390087619882\n",
      "train loss:0.01319375836841716\n",
      "train loss:0.00225129868060996\n",
      "train loss:0.010423289896100769\n",
      "train loss:0.01335541628035159\n",
      "train loss:0.003374343274759434\n",
      "train loss:0.028201093054995487\n",
      "train loss:0.0013801537364100827\n",
      "train loss:0.0020760286297064282\n",
      "train loss:0.007448529500563421\n",
      "train loss:0.015523881603780175\n",
      "train loss:0.0016492484153106612\n",
      "train loss:0.004189055409648284\n",
      "train loss:0.020957589778778804\n",
      "train loss:0.004932345758623095\n",
      "train loss:0.0036808699238016916\n",
      "train loss:0.005617259127860536\n",
      "train loss:0.02259820581774148\n",
      "train loss:0.00254151680555588\n",
      "train loss:0.002234679075841219\n",
      "train loss:0.001459889357460896\n",
      "train loss:0.023228320789131413\n",
      "train loss:0.0019296144250452135\n",
      "train loss:0.013399030211105261\n",
      "train loss:0.006072151105688961\n",
      "train loss:0.002240369989383464\n",
      "train loss:0.009889734958889274\n",
      "train loss:0.005612857950795479\n",
      "train loss:0.005467105194697907\n",
      "train loss:0.019069538494375312\n",
      "train loss:0.009408860822307092\n",
      "train loss:0.004172993083608634\n",
      "train loss:0.0035199207064964292\n",
      "train loss:0.0027149904471700363\n",
      "train loss:0.015362558762792073\n",
      "train loss:0.005929413978092288\n",
      "train loss:0.011703567246183684\n",
      "train loss:0.001884567524483361\n",
      "train loss:0.02279413088290646\n",
      "train loss:0.0052009292696751155\n",
      "train loss:0.006940630948605507\n",
      "train loss:0.0059645685610345365\n",
      "train loss:0.001739074128851627\n",
      "train loss:0.00484094024934682\n",
      "train loss:0.0044632530009304965\n",
      "train loss:0.005411971045057534\n",
      "train loss:0.009220599587924726\n",
      "train loss:0.008749225648493187\n",
      "train loss:0.005450901276307884\n",
      "train loss:0.0012778851857168066\n",
      "train loss:0.005567264786936072\n",
      "train loss:0.006230567523151281\n",
      "train loss:0.02631667150156039\n",
      "train loss:0.003754473546716629\n",
      "train loss:0.002147606643220474\n",
      "train loss:0.012828101992927722\n",
      "train loss:0.005061624734199018\n",
      "train loss:0.0035746040746870906\n",
      "train loss:0.012268316467955835\n",
      "train loss:0.005884443950041594\n",
      "train loss:0.0074133248015288335\n",
      "train loss:0.0007822568374268436\n",
      "train loss:0.011192677796098673\n",
      "train loss:0.05353022584474245\n",
      "train loss:0.0017818893710587827\n",
      "train loss:0.024920174931203215\n",
      "train loss:0.001489399227617023\n",
      "train loss:0.009446635928022026\n",
      "train loss:0.006021019478500549\n",
      "train loss:0.004486014244394813\n",
      "train loss:0.013746424929432559\n",
      "train loss:0.0057743517396527935\n",
      "train loss:0.0027048502395283325\n",
      "train loss:0.0015492738324261934\n",
      "train loss:0.005045564087386581\n",
      "train loss:0.011837787607303031\n",
      "train loss:0.002274729821407132\n",
      "train loss:0.007401073059947213\n",
      "train loss:0.009332217063607658\n",
      "train loss:0.0007318674116792205\n",
      "train loss:0.00037202548638373434\n",
      "train loss:0.01489676914385373\n",
      "train loss:0.03329992396487193\n",
      "train loss:0.017833322542584348\n",
      "train loss:0.011189291021110295\n",
      "train loss:0.005688965200731925\n",
      "train loss:0.020184924501591213\n",
      "train loss:0.002544400088200102\n",
      "train loss:0.010776297343408625\n",
      "train loss:0.004690100363230123\n",
      "train loss:0.004631875589704707\n",
      "train loss:0.002194116091296668\n",
      "train loss:0.015013003332134615\n",
      "train loss:0.0010778143022478917\n",
      "train loss:0.002023221814179907\n",
      "train loss:0.0023393024039177466\n",
      "train loss:0.014355231220051681\n",
      "train loss:0.004034173308914917\n",
      "train loss:0.007357030967702239\n",
      "train loss:0.0023318475988600854\n",
      "train loss:0.0014087041850692358\n",
      "train loss:0.006774007538219794\n",
      "train loss:0.007185339882846177\n",
      "train loss:0.004266652596523247\n",
      "train loss:0.009160532175726462\n",
      "train loss:0.010821491334872993\n",
      "train loss:0.0027378599936070352\n",
      "train loss:0.0056629119974405066\n",
      "train loss:0.0014666978583352196\n",
      "train loss:0.02140061446923559\n",
      "train loss:0.004079774910254016\n",
      "train loss:0.0042448295165692365\n",
      "train loss:0.029953105051741775\n",
      "train loss:0.002962097647860918\n",
      "train loss:0.003492847141480678\n",
      "train loss:0.0009898789340705306\n",
      "train loss:0.001914918575755286\n",
      "train loss:0.01652314459078394\n",
      "train loss:0.009588951387475016\n",
      "train loss:0.005358156569328092\n",
      "train loss:0.005419947874682851\n",
      "train loss:0.01175006703606539\n",
      "train loss:0.003930336459554362\n",
      "train loss:0.006943443897232775\n",
      "train loss:0.003424418275037677\n",
      "train loss:0.006962465270908613\n",
      "train loss:0.007964171102194228\n",
      "train loss:0.0015474399459329206\n",
      "train loss:0.0012671952683969772\n",
      "train loss:0.007947060747878409\n",
      "train loss:0.00473226460450275\n",
      "train loss:0.01615197784982569\n",
      "train loss:0.005699113568900788\n",
      "train loss:0.004054126057978871\n",
      "train loss:0.002530517310861683\n",
      "train loss:0.0021025782121856305\n",
      "train loss:0.002190425860286326\n",
      "train loss:0.005858727360398775\n",
      "train loss:0.011572194666948305\n",
      "train loss:0.005954535124846099\n",
      "train loss:0.010754262978674283\n",
      "train loss:0.0012872592293175914\n",
      "train loss:0.0136237747651338\n",
      "train loss:0.001884485709516628\n",
      "train loss:0.0006129405400199252\n",
      "train loss:0.02462787494195292\n",
      "train loss:0.0016485404046719444\n",
      "train loss:0.0071301126548234785\n",
      "train loss:0.0035745725629694287\n",
      "train loss:0.0028224098857201896\n",
      "train loss:0.006218929314236597\n",
      "train loss:0.0014615396892611705\n",
      "train loss:0.008739556146259849\n",
      "train loss:0.0023321334512690466\n",
      "train loss:0.005938001221636536\n",
      "train loss:0.005257081033796318\n",
      "train loss:0.0026246510224470563\n",
      "train loss:0.0011255635414392862\n",
      "train loss:0.0065885586891882035\n",
      "train loss:0.06420146603603318\n",
      "train loss:0.0008088815979839087\n",
      "train loss:0.009078001925139913\n",
      "train loss:0.005219572916990867\n",
      "train loss:0.021483807002874153\n",
      "train loss:0.0029886488749535866\n",
      "train loss:0.0024081960103599924\n",
      "train loss:0.037338993680588184\n",
      "train loss:0.0030655212615269345\n",
      "train loss:0.004175400735102284\n",
      "train loss:0.005996731293848791\n",
      "train loss:0.005437220658019581\n",
      "train loss:0.048300617759896164\n",
      "train loss:0.007176571572781415\n",
      "train loss:0.003223241632154627\n",
      "train loss:0.009047734393400807\n",
      "train loss:0.011254940543968677\n",
      "train loss:0.004205528280555478\n",
      "train loss:0.0053528299190564324\n",
      "train loss:0.001270707983794307\n",
      "train loss:0.0022875589124676634\n",
      "train loss:0.002055001491627636\n",
      "train loss:0.005667918729160598\n",
      "train loss:0.0019455775860089607\n",
      "train loss:0.004650400162807837\n",
      "train loss:0.0018011009931368377\n",
      "train loss:0.012697091288060706\n",
      "train loss:0.002220726948965873\n",
      "train loss:0.0045683334329089855\n",
      "train loss:0.012314924748010445\n",
      "train loss:0.004995062881282985\n",
      "train loss:0.005084406627462146\n",
      "train loss:0.005124513370357559\n",
      "train loss:0.014370314905674184\n",
      "train loss:0.0009470548037115417\n",
      "train loss:0.005611091065005963\n",
      "train loss:0.005795345914931948\n",
      "train loss:0.009829578346453548\n",
      "train loss:0.0027013549618760593\n",
      "train loss:0.020268336581223137\n",
      "train loss:0.007249857586171471\n",
      "train loss:0.010359630806928073\n",
      "train loss:0.0017692863823303828\n",
      "train loss:0.0036329023831201627\n",
      "train loss:0.0033874097740461168\n",
      "train loss:0.011356290363237233\n",
      "train loss:0.015274594192201674\n",
      "train loss:0.0019948270598810634\n",
      "train loss:0.0035374820349779413\n",
      "train loss:0.007179847027742197\n",
      "train loss:0.0008860672932715145\n",
      "train loss:0.0042152478261356105\n",
      "train loss:0.005302377117454611\n",
      "train loss:0.004427580635776815\n",
      "train loss:0.00766851719902322\n",
      "train loss:0.0009970641041477748\n",
      "train loss:0.004197568613233558\n",
      "train loss:0.0009660238916144632\n",
      "train loss:0.004918504109796815\n",
      "train loss:0.005644283124576768\n",
      "train loss:0.0003804560239044647\n",
      "train loss:0.013384314650777593\n",
      "train loss:0.007616827198855616\n",
      "train loss:0.0068960824136157905\n",
      "train loss:0.0010677433384305157\n",
      "train loss:0.007663219578382078\n",
      "train loss:0.0045907619638740765\n",
      "train loss:0.0088537338472787\n",
      "train loss:0.0033025150828912687\n",
      "train loss:0.0009918797124105496\n",
      "train loss:0.0030381087162923465\n",
      "train loss:0.002543100321943685\n",
      "train loss:0.0031297701488552845\n",
      "train loss:0.0009434790197318562\n",
      "train loss:0.0008662185034501343\n",
      "train loss:0.0009624552235263304\n",
      "train loss:0.011188549138219287\n",
      "train loss:0.00042924199871635703\n",
      "train loss:0.0017177207654550594\n",
      "train loss:0.0025915587544215533\n",
      "train loss:0.0008568274761450736\n",
      "train loss:0.02352352676509772\n",
      "train loss:0.027829099488252868\n",
      "train loss:0.012677087066313318\n",
      "train loss:0.011570903438182248\n",
      "train loss:0.0018162315943164279\n",
      "train loss:0.008227440526022686\n",
      "train loss:0.00709681070454365\n",
      "train loss:0.01352095389547594\n",
      "train loss:0.06540845980084872\n",
      "train loss:0.0009188055293807465\n",
      "train loss:0.0022007643347879153\n",
      "train loss:0.015841298513862668\n",
      "train loss:0.01023846725971505\n",
      "train loss:0.005061424198839044\n",
      "train loss:0.01182504691184073\n",
      "train loss:0.0009392055271893423\n",
      "train loss:0.002734242241941138\n",
      "train loss:0.003741214901118406\n",
      "train loss:0.0041921050495562625\n",
      "train loss:0.019295939932849546\n",
      "train loss:0.06170110523340724\n",
      "train loss:0.0012830766684503638\n",
      "train loss:0.01625102809612471\n",
      "train loss:0.003879309284647241\n",
      "train loss:0.0026643849573558713\n",
      "train loss:0.0028130942894954735\n",
      "train loss:0.004143364740523895\n",
      "train loss:0.0008214880310029064\n",
      "train loss:0.031130310027794567\n",
      "train loss:0.005567907135253433\n",
      "train loss:0.006850762413728751\n",
      "train loss:0.014292075802006478\n",
      "train loss:0.015577719075168878\n",
      "train loss:0.0004912828845411153\n",
      "train loss:0.011843858957092899\n",
      "train loss:0.016261411201537374\n",
      "train loss:0.009657010310793426\n",
      "train loss:0.007546460322663579\n",
      "train loss:0.0008090232795777777\n",
      "train loss:0.011969139039046874\n",
      "train loss:0.009596801202330583\n",
      "train loss:0.003232128954789133\n",
      "train loss:0.0031531425594385778\n",
      "train loss:0.00309539616568745\n",
      "train loss:0.001265779930045266\n",
      "train loss:0.00493766918651573\n",
      "train loss:0.003271953937435936\n",
      "train loss:0.005545868016445693\n",
      "train loss:0.009325285217938694\n",
      "train loss:0.0015910666592531394\n",
      "train loss:0.005644513347037401\n",
      "train loss:0.008407696989009183\n",
      "train loss:0.050073417492172714\n",
      "train loss:0.0038538784686683396\n",
      "train loss:0.007495815189483824\n",
      "train loss:0.0016568794512516133\n",
      "train loss:0.001294075988117521\n",
      "train loss:0.008890372216878469\n",
      "train loss:0.07981267685391127\n",
      "train loss:0.003847795092290559\n",
      "train loss:0.026481411805301783\n",
      "train loss:0.0205220414689066\n",
      "train loss:0.0029664583898573087\n",
      "train loss:0.0014633643385948828\n",
      "train loss:0.005870346558561019\n",
      "train loss:0.0034361689407525014\n",
      "train loss:0.004973656646680761\n",
      "train loss:0.0025232874405141997\n",
      "train loss:0.009141721393335422\n",
      "train loss:0.0055983746125314185\n",
      "train loss:0.0021002364231950416\n",
      "train loss:0.010574208301746\n",
      "train loss:0.00141334218481824\n",
      "train loss:0.0009964554742547139\n",
      "train loss:0.008486649625141025\n",
      "train loss:0.0015954011018984471\n",
      "train loss:0.006444740147670853\n",
      "train loss:0.013253256817918708\n",
      "train loss:0.001633886935920466\n",
      "train loss:0.022946989903376003\n",
      "train loss:0.04447818975497872\n",
      "train loss:0.0015407714342943303\n",
      "train loss:0.005572862227899559\n",
      "train loss:0.001020289124251355\n",
      "train loss:0.0027287611386173226\n",
      "train loss:0.014796776596517992\n",
      "train loss:0.01650630107074265\n",
      "train loss:0.021133989961804357\n",
      "train loss:0.01595940221899883\n",
      "train loss:0.0036912782235314183\n",
      "train loss:0.002488129306474021\n",
      "train loss:0.001122107254824651\n",
      "train loss:0.011546732872488565\n",
      "train loss:0.006167063021588716\n",
      "train loss:0.0006170271288638681\n",
      "train loss:0.007472932489148818\n",
      "train loss:0.02508639249814331\n",
      "train loss:0.012309852641813692\n",
      "train loss:0.008445026520914443\n",
      "train loss:0.00609234462172272\n",
      "train loss:0.0037127385669111852\n",
      "train loss:0.0012613761366142946\n",
      "train loss:0.006991385166322119\n",
      "train loss:0.005417308738691936\n",
      "train loss:0.03645584465718616\n",
      "train loss:0.008841776665447743\n",
      "train loss:0.002819371402637145\n",
      "train loss:0.010575904203350027\n",
      "train loss:0.014525054413119092\n",
      "train loss:0.005788182289931275\n",
      "train loss:0.006683898500519637\n",
      "train loss:0.005163201187887584\n",
      "train loss:0.0038343022900033876\n",
      "train loss:0.0007876515789510941\n",
      "train loss:0.005658616334196354\n",
      "train loss:0.005923800986955779\n",
      "train loss:0.0034380863957183937\n",
      "train loss:0.008935920045123823\n",
      "train loss:0.0038116805651517093\n",
      "train loss:0.01014907396972997\n",
      "train loss:0.004826867878551081\n",
      "train loss:0.006490104122483413\n",
      "train loss:0.005457265292928844\n",
      "train loss:0.002639969292962416\n",
      "train loss:0.0003463514993474455\n",
      "train loss:0.044480578970548273\n",
      "train loss:0.021213023129340525\n",
      "train loss:0.0028749694076617167\n",
      "train loss:0.011330678549444632\n",
      "train loss:0.002330783703242776\n",
      "train loss:0.001408318440124064\n",
      "train loss:0.004657089092775299\n",
      "train loss:0.0024804884064425093\n",
      "train loss:0.008542000089868215\n",
      "train loss:0.038736445061294734\n",
      "train loss:0.009859919274173911\n",
      "train loss:0.005104208722090774\n",
      "train loss:0.008401238916959632\n",
      "train loss:0.0015494903081532424\n",
      "train loss:0.020613674971848073\n",
      "train loss:0.039946758750679434\n",
      "train loss:0.0031241398484312816\n",
      "train loss:0.01436303912701256\n",
      "train loss:0.0023870761394035847\n",
      "train loss:0.004671510414741676\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.986\n",
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "# 학습하기 : 실제 코드는 다음에 있음. https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch07/train_convnet.py\n",
    "\n",
    "from mnist import load_mnist\n",
    "from trainer import Trainer\n",
    "from simple_convnet import SimpleConvNet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 10\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
